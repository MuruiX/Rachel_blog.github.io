{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5823c581-9f3e-4031-9094-af06b66f2a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install requests beautifulsoup4 edgartools lxml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c577daf1",
   "metadata": {},
   "source": [
    "logic modification 4-modify modification 1 to incorporate p formats together with div- find the level where table is and then check with PART I for the corresponding level to check if it is p or div, and then proceed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d7138bd4-015b-40ed-a3c9-d7d201cdaaa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_html = '../【全部24家公司】10K(01-23年)/SYK.N/SYK.N_10K_2023-12-31'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "54d17859",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/html/body/div[39]/span\n"
     ]
    }
   ],
   "source": [
    "from lxml import etree\n",
    "import re\n",
    "\n",
    "def parse_html_content(html_content):\n",
    "    parser = etree.HTMLParser(recover=True)\n",
    "    return etree.fromstring(html_content, parser)\n",
    "\n",
    "def get_xpath_of_element(element):\n",
    "    \"\"\"Generate XPath of the given element.\"\"\"\n",
    "    components = []\n",
    "    \n",
    "    while element is not None:\n",
    "        tag = element.tag\n",
    "        parent = element.getparent()\n",
    "        \n",
    "        if parent is not None:\n",
    "            siblings = [e for e in parent.iterchildren() if e.tag == tag]\n",
    "            index = siblings.index(element) + 1\n",
    "            if len(siblings) > 1:\n",
    "                components.append(f'{tag}[{index}]')\n",
    "            else:\n",
    "                components.append(tag)\n",
    "        else:\n",
    "            components.append(tag)\n",
    "        \n",
    "        element = parent\n",
    "\n",
    "    components.reverse()\n",
    "    return '/' + '/'.join(components)\n",
    "\n",
    "def transfer_ix_elements(root):\n",
    "    \"\"\"\n",
    "    Continuously check and move non-`ix:` child elements from `ix:` elements into their nearest non-`ix:` parent elements,\n",
    "    starting from the innermost `ix:` elements and moving upwards, ensuring proper handling of nested and parallel `ix:` elements.\n",
    "    \"\"\"\n",
    "    while True:\n",
    "        if_ept = 0  # Flag to check if we moved any content\n",
    "\n",
    "        # Iterate through all elements whose tag starts with \"ix:\", ensuring we process innermost ones first\n",
    "        for elem in reversed(root.xpath('//*[contains(name(), \"ix:\")]')):  # Select all ix elements\n",
    "            parent = elem.getparent()\n",
    "\n",
    "            # Check if we should process the current element\n",
    "            if elem.tag.startswith(\"ix:\"):\n",
    "                # Print the element and its current index for debugging\n",
    "                #print(f\"Processing element: <{elem.tag}> with text '{elem.text.strip() if elem.text else 'None'}'\")\n",
    "\n",
    "                # Move the text content from the ix: element to its parent (if any)\n",
    "                if parent is not None:\n",
    "                    # If there's text in the current ix: element, insert it into the parent's content\n",
    "                    if elem.text and elem.text.strip():\n",
    "                        text_node = etree.Element(\"text\")  # Create a temporary element to hold the text\n",
    "                        text_node.text = elem.text.strip()\n",
    "                        parent.insert(parent.index(elem), text_node)  # Insert the text node at the same index as elem\n",
    "                        #print(f\"  Inserted text: '{elem.text.strip()}' at index {parent.index(elem)} in parent\")\n",
    "\n",
    "                    # Handle the tail content of the current ix: element (if any)\n",
    "                    if elem.tail and elem.tail.strip():\n",
    "                        text_node = etree.Element(\"text\")  # Create another temporary element for tail text\n",
    "                        text_node.text = elem.tail.strip()\n",
    "                        parent.insert(parent.index(elem) + 1, text_node)  # Insert after the ix: element\n",
    "                        #print(f\"  Inserted tail: '{elem.tail.strip()}' at index {parent.index(elem) + 1} in parent\")\n",
    "\n",
    "                    # Move all non-ix child elements to the parent\n",
    "                    for child in list(elem):\n",
    "                        if not child.tag.startswith(\"ix:\"):\n",
    "                            # Insert before the next sibling of the element\n",
    "                            parent.insert(parent.index(elem), child)\n",
    "                            #print(f\"  Moving child element: <{child.tag}> with text '{child.text.strip() if child.text else 'None'}'\")\n",
    "\n",
    "                    # After moving content, remove the ix: element itself\n",
    "                    parent.remove(elem)\n",
    "                    #print(f\"  Removed element: <{elem.tag}>\")\n",
    "                    #print('parent', parent)\n",
    "\n",
    "                # Mark that content was moved\n",
    "                if_ept = 1\n",
    "\n",
    "        # If no more 'ix:' elements were found, break the loop\n",
    "        if if_ept == 0:\n",
    "            #print(\"No more 'ix:' elements to move, breaking loop.\")\n",
    "            break\n",
    "\n",
    "def transfer_page_elements(root):\n",
    "    \"\"\"\n",
    "    Continuously check and move contents from `page` elements to their parent elements until all `page` elements are removed.\n",
    "    \"\"\"\n",
    "    while True:\n",
    "        if_ept = 0\n",
    "        for elem in root.xpath('//page'):\n",
    "            parent = elem.getparent()\n",
    "            if parent is not None:\n",
    "                for child in list(elem):\n",
    "                    parent.insert(parent.index(elem), child)\n",
    "                    if_ept = 1\n",
    "                parent.remove(elem)\n",
    "        if if_ept == 0:\n",
    "            break\n",
    "\n",
    "def process_all_elements(root):\n",
    "    \"\"\"\n",
    "    Main function: move content first, then remove elements.\n",
    "    \"\"\"\n",
    "    transfer_ix_elements(root)  # Move non `ix:` elements\n",
    "    transfer_page_elements(root)\n",
    "\n",
    "def find_toc(root):\n",
    "    # XPath to find potential TOC sections, excluding those inside table elements\n",
    "    toc_section_candidates = root.xpath(\"//span[not(ancestor::table)]|\"\n",
    "                                        \"//p[not(ancestor::table)]|\"\n",
    "                                        \"//font[not(ancestor::table)]|\"\n",
    "                                        \"//b[not(ancestor::table)]\")\n",
    "    \n",
    "    # List of possible TOC text candidates in lowercase\n",
    "    toc_texts = ['table of contents', 'index', 'part i', 'part 1']\n",
    "\n",
    "    # Iterate over all the potential candidates and check text case-insensitively\n",
    "    for candidate in toc_section_candidates:\n",
    "        candidate_text = candidate.xpath(\"string()\").strip().lower()\n",
    "        candidate_text = re.sub(r'\\s+', ' ', candidate_text.replace('\\xa0', ' '))\n",
    "        if any(toc_text == candidate_text for toc_text in toc_texts) and not candidate.xpath(\".//a[@href]\"):\n",
    "            return candidate\n",
    "    \n",
    "    print(\"TOC not found.\")\n",
    "    return None\n",
    "\n",
    "def merge_similar_spans(root):\n",
    "    \"\"\"\n",
    "    Merge consecutive <span> elements with the same style into one <span> element.\n",
    "    \"\"\"\n",
    "    for p in root.xpath('//p'):\n",
    "        children = list(p)\n",
    "        i = 0\n",
    "        while i < len(children) - 1:\n",
    "            current = children[i]\n",
    "            next = children[i + 1]\n",
    "            if current.tag == 'span' and next.tag == 'span' and current.attrib == next.attrib:\n",
    "                current.text = (current.text or '') + (next.text or '')\n",
    "                for child in next:\n",
    "                    current.append(child)\n",
    "                p.remove(next)\n",
    "                children.pop(i + 1)  # Ensure the list is updated correctly\n",
    "            else:\n",
    "                i += 1\n",
    "\n",
    "def process_html_replace_tables(root):\n",
    "    table_xpath = \"//table[tr[count(td) = 2]]\"\n",
    "\n",
    "    if toc_section is None:\n",
    "        return etree.tostring(root, pretty_print=True, encoding=\"unicode\", method=\"html\")  # No TOC, return original\n",
    "\n",
    "    toc_position = toc_section.getparent().index(toc_section)  # Get position of TOC\n",
    "\n",
    "    # Iterate over the found tables\n",
    "    for table in root.xpath(table_xpath):\n",
    "        table_position = table.getparent().index(table)\n",
    "        rows = table.xpath(\".//tr\")\n",
    "        # Ensure we are processing tables after TOC\n",
    "        if table_position <= toc_position or len(rows) >5: #added or statement to rule out miscapture of item 8 in isrg 23\n",
    "            continue\n",
    "        #print(etree.tostring(table, pretty_print=True, encoding=\"unicode\", method=\"html\"))\n",
    "        # Determine the type of new element based on TOC section type\n",
    "        new_element = etree.Element(toc_section.tag) if toc_section.tag in [\"div\", \"p\"] else etree.Element(\"div\")\n",
    "\n",
    "        # Targeting the second row's <td> elements (if they exist)\n",
    "        if len(rows) > 1:\n",
    "            item_td = rows[1].xpath(\"./td[1]\")\n",
    "            title_td = rows[1].xpath(\"./td[2]\")\n",
    "            next_item_td = rows[2].xpath(\"./td[1]\") if len(rows) > 2 else None\n",
    "            next_title_td = rows[2].xpath(\"./td[2]\") if len(rows) > 2 else None\n",
    "        else:\n",
    "            # Fallback to the first row if only one row exists\n",
    "            item_td = rows[0].xpath(\"./td[1]\")\n",
    "            title_td = rows[0].xpath(\"./td[2]\")\n",
    "            next_item_td = None\n",
    "            next_title_td = None\n",
    "\n",
    "        # Function to check if a <td> is effectively empty\n",
    "        def is_td_empty(td):\n",
    "            return not (td.text and td.text.strip()) and len(td.getchildren()) == 0\n",
    "\n",
    "        # Check the content of item_td and title_td\n",
    "        if item_td and title_td:\n",
    "            if not is_td_empty(item_td[0]) and not is_td_empty(title_td[0]):\n",
    "                # Clone the contents of the <td> elements to preserve formatting\n",
    "                for child in item_td[0].getchildren():\n",
    "                    new_element.append(child)  # Append children of item <td>\n",
    "                for child in title_td[0].getchildren():\n",
    "                    new_element.append(child)  # Append children of title <td>\n",
    "            elif next_item_td and next_title_td:  # Check the next row if the current one is empty\n",
    "                if not is_td_empty(next_item_td[0]) and not is_td_empty(next_title_td[0]):\n",
    "                    for child in next_item_td[0].getchildren():\n",
    "                        new_element.append(child)  # Append children of next item <td>\n",
    "                    for child in next_title_td[0].getchildren():\n",
    "                        new_element.append(child)  # Append children of next title <td>\n",
    "            else:\n",
    "                # Print a message if no valid <td> elements were found\n",
    "                table_content = etree.tostring(table, pretty_print=True, encoding=\"unicode\", method=\"html\")\n",
    "                print(f\"No valid <td> elements found in the targeted row of the table. Table content:\\n{table_content}\")\n",
    "                continue\n",
    "\n",
    "        # Replace the table with the new element\n",
    "        table.getparent().replace(table, new_element)\n",
    "\n",
    "    # Convert the tree back to a string, ensuring proper formatting\n",
    "    return etree.tostring(root, pretty_print=True, encoding=\"unicode\", method=\"html\")\n",
    "\n",
    "# Read and parse the HTML file content\n",
    "with open(f'{current_html}.html', 'rb') as f:\n",
    "    html_content = f.read()\n",
    "root = parse_html_content(html_content)\n",
    "toc_section = find_toc(root)\n",
    "toc_xpath = get_xpath_of_element(toc_section)\n",
    "print(toc_xpath)\n",
    "# Process the parsed HTML content\n",
    "process_all_elements(root)\n",
    "merge_similar_spans(root)\n",
    "processed_html_content = process_html_replace_tables(root)\n",
    "\n",
    "# Generate the processed HTML string\n",
    "result_html = etree.tostring(root, pretty_print=True, encoding=\"unicode\", method=\"html\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "933d20d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed HTML has been saved to output.html\n"
     ]
    }
   ],
   "source": [
    "with open('output.html', 'w', encoding='utf-8') as f:\n",
    "    f.write(result_html)\n",
    "\n",
    "print(f\"Processed HTML has been saved to output.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3ab01e9b-5701-4f64-8fc4-9ead22ee84ec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bold items:\n",
      "[{'text': 'UNITED STATES', 'xpath': '/html/body/div[5]/span'}, {'text': 'SECURITIES AND EXCHANGE COMMISSION', 'xpath': '/html/body/div[6]/span'}, {'text': 'FORM 10-K', 'xpath': '/html/body/div[9]/span'}, {'text': 'OR', 'xpath': '/html/body/div[12]/span'}, {'text': 'STRYKER CORPORATION', 'xpath': '/html/body/div[16]/span'}, {'text': 'Securities registered pursuant to Section 12(g) of the Act: None', 'xpath': '/html/body/div[20]/span'}, {'text': 'DOCUMENTS INCORPORATED BY REFERENCE', 'xpath': '/html/body/div[33]/span'}, {'text': 'TABLE OF CONTENTS', 'xpath': '/html/body/div[39]/span'}, {'text': 'Business Segments and Geographic Information', 'xpath': '/html/body/div[44]/div[1]/div[10]/span'}, {'text': 'MedSurg and Neurotechnology', 'xpath': '/html/body/div[44]/div[3]/div[1]/span'}, {'text': 'Orthopaedics and Spine', 'xpath': '/html/body/div[47]/div[1]/div[1]/span'}, {'text': 'Raw Materials and Inventory', 'xpath': '/html/body/div[47]/div[3]/div[1]/span'}, {'text': 'Patents and Trademarks', 'xpath': '/html/body/div[47]/div[3]/div[3]/span'}, {'text': 'Seasonality', 'xpath': '/html/body/div[47]/div[3]/div[5]/span'}, {'text': 'Competition', 'xpath': '/html/body/div[47]/div[3]/div[7]/span'}, {'text': 'Regulation', 'xpath': '/html/body/div[47]/div[3]/div[10]/span'}, {'text': 'Environment', 'xpath': '/html/body/div[50]/div[1]/div[4]/span'}, {'text': 'Employees', 'xpath': '/html/body/div[50]/div[1]/div[6]/span'}, {'text': 'Employee Development', 'xpath': '/html/body/div[50]/div[1]/div[10]/span'}, {'text': 'Employee Engagement', 'xpath': '/html/body/div[50]/div[3]/div[2]/span'}, {'text': 'Diversity, Equity and Inclusion (DE&I)', 'xpath': '/html/body/div[50]/div[3]/div[6]/span'}, {'text': 'Attracting and Hiring', 'xpath': '/html/body/div[50]/div[3]/div[16]/span'}, {'text': 'Health and Safety', 'xpath': '/html/body/div[50]/div[3]/div[18]/span'}, {'text': 'Competitive Pay and Benefits', 'xpath': '/html/body/div[50]/div[3]/div[20]/span'}, {'text': 'Information about our Executive Officers', 'xpath': '/html/body/div[53]/div[1]/div[4]/span'}, {'text': 'Available Information', 'xpath': '/html/body/div[53]/div[1]/div[7]/span'}, {'text': 'BUSINESS AND OPERATIONAL RISKS', 'xpath': '/html/body/div[56]/div[1]/div[3]/span'}, {'text': 'We use a variety of raw materials, components, devices and third-party services in our global supply chains, production and distribution processes; significant shortages, price increases or unavailability of third-party services have in the past increased, and could in the future increase, our operating costs and could require significant capital expenditures or adversely impact the competitive position of our products:', 'xpath': '/html/body/div[56]/div[1]/div[4]/span[1]'}, {'text': 'We are subject to pricing pressures as a result of cost containment measures in the United States and other countries and other factors:', 'xpath': '/html/body/div[56]/div[3]/div[2]/span[1]'}, {'text': 'We operate in a highly competitive industry in which competition in the development and improvement of new and existing products is significant:', 'xpath': '/html/body/div[56]/div[3]/div[3]/span[1]'}, {'text': 'We may be unable to maintain adequate working relationships with healthcare professionals:', 'xpath': '/html/body/div[59]/div[1]/div[2]/span[1]'}, {'text': 'We rely on indirect distribution channels and major distributors that are independent of Stryker:', 'xpath': '/html/body/div[59]/div[1]/div[3]/span[1]'}, {'text': 'We are subject to risks associated with our extensive global operations:', 'xpath': '/html/body/div[59]/div[1]/div[4]/span[1]'}, {'text': 'We may be unable to capitalize on previous or future acquisitions:', 'xpath': '/html/body/div[59]/div[1]/div[5]/span[1]'}, {'text': 'We, our business partners or our third-party vendors could experience a material failure or breach of a key information technology system, network, process or site:', 'xpath': '/html/body/div[59]/div[3]/div[2]/span[1]'}, {'text': 'An inability to successfully manage the implementation of our new commercial global enterprise resource planning (ERP) system could adversely affect our operations and operating results:', 'xpath': '/html/body/div[62]/div[1]/div[2]/span[1]'}, {'text': 'We may be unable to attract, develop and retain executives and key employees:', 'xpath': '/html/body/div[62]/div[1]/div[3]/span[1]'}, {'text': 'Interruption of manufacturing operations could adversely affect our business:', 'xpath': '/html/body/div[62]/div[3]/div[3]/span[1]'}, {'text': 'Our insurance program may not be adequate to cover future losses:', 'xpath': '/html/body/div[62]/div[3]/div[4]/span[1]'}, {'text': 'We have experienced, and may continue to experience, a significant and unpredictable need to adjust our operations as market demand for certain of our products has shifted and continues to shift or as may be mandated by governmental authorities:', 'xpath': '/html/body/div[62]/div[3]/div[5]/span[1]'}, {'text': 'Pandemics and public health emergencies, and the fear thereof, have in the past materially adversely affected and could in the future materially adversely affect, our operations, supply chain, manufacturing, product distribution, customers and other business activities:', 'xpath': '/html/body/div[65]/div[1]/div[3]/span[1]'}, {'text': 'LEGAL AND REGULATORY RISKS', 'xpath': '/html/body/div[65]/div[1]/div[4]/span'}, {'text': 'Current economic and political conditions make tax rules in jurisdictions subject to significant change', 'xpath': '/html/body/div[65]/div[1]/div[5]/span[1]'}, {'text': 'We could be negatively impacted by future changes in the allocation of income to each of the income tax', 'xpath': '/html/body/div[65]/div[3]/div[2]/span[1]'}, {'text': 'jurisdictions in which we operate:', 'xpath': '/html/body/div[65]/div[3]/div[2]/span[3]'}, {'text': 'The impact of healthcare reform legislation on our business remains uncertain:', 'xpath': '/html/body/div[65]/div[3]/div[3]/span[1]'}, {'text': 'We are subject to extensive governmental regulation relating to the classification, manufacturing, sterilization, licensing, labeling, marketing and sale of our products:', 'xpath': '/html/body/div[65]/div[3]/div[4]/span[1]'}, {'text': 'We are subject to federal, state and foreign healthcare regulations, including anti-bribery, anti-corruption, anti-kickback and false claims laws, globally and could face substantial penalties if we fail to comply with such regulations and laws:', 'xpath': '/html/body/div[68]/div[1]/div[1]/span[1]'}, {'text': 'We are subject to privacy, data protection and data security regulations and laws globally, and could face substantial penalties if we fail to comply with such regulations and laws:', 'xpath': '/html/body/div[68]/div[1]/div[2]/span[1]'}, {'text': 'We may be adversely affected by product liability claims, unfavorable court decisions or legal settlements:', 'xpath': '/html/body/div[68]/div[3]/div[2]/span[1]'}, {'text': 'Intellectual property litigation and infringement claims could cause us to incur significant expenses or prevent us from selling certain of our products:', 'xpath': '/html/body/div[68]/div[3]/div[3]/span[1]'}, {'text': 'Dependence on patent and other proprietary rights and failing to protect such rights or to be successful in litigation related to such rights may impact offerings in our product portfolios:', 'xpath': '/html/body/div[68]/div[3]/div[4]/span[1]'}, {'text': 'MARKET RISKS', 'xpath': '/html/body/div[68]/div[3]/div[5]/span'}, {'text': 'We have exposure to exchange rate fluctuations on cross border transactions and translation of local currency results into United States Dollars:', 'xpath': '/html/body/div[68]/div[3]/div[6]/span[1]'}, {'text': 'Additional capital that we may require in the future may not be available to us or may only be available to us on unfavorable terms, which could negatively affect our liquidity:', 'xpath': '/html/body/div[68]/div[3]/div[7]/span[1]'}, {'text': 'ENVIRONMENTAL, SOCIAL AND GOVERNANCE RISKS', 'xpath': '/html/body/div[71]/div[1]/div[2]/span'}, {'text': 'We could be negatively impacted by corporate responsibility and sustainability-related matters:', 'xpath': '/html/body/div[71]/div[1]/div[3]/span[1]'}, {'text': 'Physical effects of climate change or legal, regulatory or market measures intended to address climate change could adversely affect our operations and operating results:', 'xpath': '/html/body/div[71]/div[1]/div[4]/span[1]'}, {'text': 'RISK MANAGEMENT AND STRATEGY', 'xpath': '/html/body/div[71]/div[3]/div[7]/span'}, {'text': \"MANAGEMENT'S ROLE IN MANAGING RISK\", 'xpath': '/html/body/div[71]/div[3]/div[9]/span'}, {'text': 'GOVERNANCE', 'xpath': '/html/body/div[74]/div[1]/div[2]/span'}, {'text': 'RISKS FROM CYBERSECURITY THREATS', 'xpath': '/html/body/div[74]/div[1]/div[11]/span'}, {'text': 'About Stryker', 'xpath': '/html/body/div[88]/div[1]/div[1]/span'}, {'text': 'Macroeconomic Environment', 'xpath': '/html/body/div[88]/div[1]/div[3]/span'}, {'text': 'Overview of 2023', 'xpath': '/html/body/div[88]/div[1]/div[5]/span'}, {'text': '(1)', 'xpath': '/html/body/div[88]/div[3]/div[1]/span[2]'}, {'text': '(1)', 'xpath': '/html/body/div[88]/div[3]/div[1]/span[4]'}, {'text': '(1)', 'xpath': '/html/body/div[89]/span[1]'}, {'text': 'CONSOLIDATED RESULTS OF OPERATIONS', 'xpath': '/html/body/div[91]/span'}, {'text': 'Consolidated Net Sales', 'xpath': '/html/body/div[101]/div[1]/div[1]/span'}, {'text': 'MedSurg and Neurotechnology Net Sales', 'xpath': '/html/body/div[101]/div[1]/div[4]/span'}, {'text': 'Orthopaedics and Spine Net Sales', 'xpath': '/html/body/div[101]/div[3]/div[2]/span'}, {'text': 'Gross Profit', 'xpath': '/html/body/div[101]/div[3]/div[6]/span'}, {'text': 'Research, Development and Engineering Expenses', 'xpath': '/html/body/div[104]/div[1]/div[5]/span'}, {'text': 'Selling, General and Administrative Expenses', 'xpath': '/html/body/div[104]/div[1]/div[8]/span'}, {'text': 'Recall Charges, Net', 'xpath': '/html/body/div[104]/div[3]/div[2]/span'}, {'text': 'Amortization of Intangible Assets', 'xpath': '/html/body/div[104]/div[3]/div[4]/span'}, {'text': 'Goodwill Impairment', 'xpath': '/html/body/div[104]/div[3]/div[6]/span'}, {'text': 'Operating Income', 'xpath': '/html/body/div[104]/div[3]/div[8]/span'}, {'text': 'Other Income (Expense), Net', 'xpath': '/html/body/div[107]/div[1]/div[5]/span'}, {'text': 'Income Taxes', 'xpath': '/html/body/div[107]/div[1]/div[7]/span'}, {'text': 'Net Earnings', 'xpath': '/html/body/div[107]/div[1]/div[9]/span'}, {'text': 'Non-GAAP Financial Measures', 'xpath': '/html/body/div[107]/div[1]/div[12]/span'}, {'text': 'Reconciliation of the Most Directly Comparable GAAP Financial Measure to Non-GAAP Financial Measure', 'xpath': '/html/body/div[112]/span'}, {'text': 'FINANCIAL CONDITION AND LIQUIDITY', 'xpath': '/html/body/div[137]/div[1]/div[1]/span'}, {'text': 'Operating Activities', 'xpath': '/html/body/div[137]/div[1]/div[4]/span'}, {'text': 'Investing Activities', 'xpath': '/html/body/div[137]/div[1]/div[6]/span'}, {'text': 'Financing Activities', 'xpath': '/html/body/div[137]/div[1]/div[8]/span'}, {'text': 'Liquidity', 'xpath': '/html/body/div[137]/div[1]/div[12]/span'}, {'text': 'Guarantees and Other Off-Balance Sheet Arrangements', 'xpath': '/html/body/div[137]/div[3]/div[4]/span'}, {'text': 'CONTRACTUAL OBLIGATIONS AND FORWARD-LOOKING CASH REQUIREMENTS', 'xpath': '/html/body/div[137]/div[3]/div[7]/span'}, {'text': 'CRITICAL ACCOUNTING POLICIES AND ESTIMATES', 'xpath': '/html/body/div[137]/div[3]/div[13]/span'}, {'text': 'Income Taxes', 'xpath': '/html/body/div[140]/div[1]/div[2]/span'}, {'text': 'Acquisitions, Goodwill and Intangibles, and Long-Lived Assets', 'xpath': '/html/body/div[140]/div[3]/div[1]/span'}, {'text': 'Legal and Other Contingencies', 'xpath': '/html/body/div[143]/div[3]/div[3]/span'}, {'text': 'NEW ACCOUNTING PRONOUNCEMENTS', 'xpath': '/html/body/div[143]/div[3]/div[6]/span'}, {'text': 'Report of Independent Registered Public Accounting Firm', 'xpath': '/html/body/div[151]/span'}, {'text': 'Opinion on the Financial Statements', 'xpath': '/html/body/div[153]/span'}, {'text': 'Basis for Opinion', 'xpath': '/html/body/div[156]/span'}, {'text': 'Critical Audit Matter', 'xpath': '/html/body/div[159]/span'}, {'text': 'Stryker Corporation and Subsidiaries', 'xpath': '/html/body/div[170]/span'}, {'text': 'CONSOLIDATED STATEMENTS OF EARNINGS', 'xpath': '/html/body/div[171]/span'}, {'text': 'CONSOLIDATED STATEMENTS OF COMPREHENSIVE INCOME', 'xpath': '/html/body/div[176]/span'}, {'text': 'Stryker Corporation and Subsidiaries', 'xpath': '/html/body/div[182]/span'}, {'text': 'CONSOLIDATED BALANCE SHEETS', 'xpath': '/html/body/div[183]/span'}, {'text': 'Stryker Corporation and Subsidiaries', 'xpath': '/html/body/div[189]/span'}, {'text': 'CONSOLIDATED STATEMENTS OF SHAREHOLDERS’ EQUITY', 'xpath': '/html/body/div[190]/span'}, {'text': 'Stryker Corporation and Subsidiaries', 'xpath': '/html/body/div[196]/span'}, {'text': 'CONSOLIDATED STATEMENTS OF CASH FLOWS', 'xpath': '/html/body/div[197]/span'}, {'text': 'NOTES TO CONSOLIDATED FINANCIAL STATEMENTS', 'xpath': '/html/body/div[205]/div[1]/div[1]/span'}, {'text': 'NOTE 1 - SIGNIFICANT ACCOUNTING POLICIES', 'xpath': '/html/body/div[205]/div[1]/div[3]/span'}, {'text': 'Nature of Operations:', 'xpath': '/html/body/div[205]/div[1]/continuation/div[1]/span'}, {'text': 'Basis of Presentation and Consolidation:', 'xpath': '/html/body/div[205]/div[1]/continuation/div[2]/span'}, {'text': 'Use of Estimates:', 'xpath': '/html/body/div[205]/div[1]/continuation/div[3]/span'}, {'text': 'Revenue Recognition:', 'xpath': '/html/body/div[205]/div[1]/continuation/div[4]/span'}, {'text': 'Cost of Sales:', 'xpath': '/html/body/div[205]/continuation/div/div[1]/span'}, {'text': 'Research, Development and Engineering Expenses:', 'xpath': '/html/body/div[205]/continuation/div/div[2]/span'}, {'text': 'Selling, General and Administrative Expenses:', 'xpath': '/html/body/div[205]/continuation/div/div[3]/span[1]'}, {'text': 'Currency Translation:', 'xpath': '/html/body/div[205]/continuation/div/div[4]/span'}, {'text': 'Cash Equivalents:', 'xpath': '/html/body/div[205]/continuation/div/div[5]/span'}, {'text': 'Marketable Securities:', 'xpath': '/html/body/div[205]/continuation/div/div[6]/span'}, {'text': 'Accounts Receivable:', 'xpath': '/html/body/div[205]/continuation/div/div[7]/span'}, {'text': 'Inventories:', 'xpath': '/html/body/div[205]/continuation/div/div[8]/span'}, {'text': 'Financial Instruments:', 'xpath': '/html/body/continuation[1]/div/div[1]/div[1]/span[1]'}, {'text': 'Derivatives:', 'xpath': '/html/body/continuation[1]/div/div[1]/div[2]/span[1]'}, {'text': 'Property, Plant and Equipment:', 'xpath': '/html/body/continuation[1]/div/div[3]/div[2]/span[1]'}, {'text': 'Goodwill and Other Intangible Assets:', 'xpath': '/html/body/continuation[1]/div/div[3]/div[3]/span[1]'}, {'text': 'Goodwill, Intangibles and Long-Lived Asset Impairment Tests:', 'xpath': '/html/body/continuation[2]/div/div[1]/div[1]/span[1]'}, {'text': 'Share-Based Compensation:', 'xpath': '/html/body/continuation[2]/div/div[1]/div[2]/span[1]'}, {'text': 'Income Taxes:', 'xpath': '/html/body/continuation[2]/div/div[3]/div/span[1]'}, {'text': 'New Accounting Pronouncements Not Yet Adopted', 'xpath': '/html/body/continuation[2]/div/div[3]/nonnumeric/div[1]/span'}, {'text': 'Accounting Pronouncements Recently Adopted', 'xpath': '/html/body/div[212]/div[1]/continuation[1]/continuation/div[1]/span'}, {'text': 'NOTE 2 - REVENUE RECOGNITION', 'xpath': '/html/body/div[212]/div[1]/div[2]/span'}, {'text': 'MedSurg and Neurotechnology', 'xpath': '/html/body/div[212]/continuation/div/div[1]/span'}, {'text': 'Orthopaedics and Spine', 'xpath': '/html/body/div[212]/continuation/div/div[3]/span'}, {'text': 'Contract Assets and Liabilities', 'xpath': '/html/body/div[215]/div[1]/continuation[1]/div[1]/span'}, {'text': 'NOTE 3 - FAIR VALUE MEASUREMENTS', 'xpath': '/html/body/div[215]/div[1]/div[2]/span'}, {'text': 'NOTE 4 - DERIVATIVE INSTRUMENTS', 'xpath': '/html/body/div[219]/div[1]/div/span'}, {'text': 'Foreign Currency Hedges', 'xpath': '/html/body/div[219]/div[1]/continuation/div[2]/span'}, {'text': 'Interest Rate Hedges', 'xpath': '/html/body/div[219]/div[3]/continuation[1]/div[2]/span'}, {'text': 'NOTE 5 - ACCUMULATED OTHER COMPREHENSIVE (LOSS) INCOME (AOCI)', 'xpath': '/html/body/div[219]/div[3]/div[2]/span'}, {'text': 'NOTE 6 - ACQUISITIONS', 'xpath': '/html/body/div[219]/div[3]/div[4]/span'}, {'text': 'NOTE 7 - CONTINGENCIES AND COMMITMENTS', 'xpath': '/html/body/div[222]/div[1]/div[3]/span'}, {'text': 'Leases', 'xpath': '/html/body/div[222]/continuation/div/div[6]/span'}, {'text': 'Future Obligations', 'xpath': '/html/body/div[225]/div[1]/continuation[1]/div[3]/span'}, {'text': 'NOTE 8 - GOODWILL AND OTHER INTANGIBLE ASSETS', 'xpath': '/html/body/div[225]/div[1]/div[2]/span'}, {'text': 'NOTE 9 - CAPITAL STOCK', 'xpath': '/html/body/div[225]/div[3]/div[2]/span'}, {'text': 'Stock Options', 'xpath': '/html/body/div[225]/div[3]/continuation[2]/div[4]/span'}, {'text': 'Employee Stock Purchase Plans (ESPP)', 'xpath': '/html/body/div[228]/div[2]/continuation[1]/div[1]/span'}, {'text': 'NOTE 10 - DEBT AND CREDIT FACILITIES', 'xpath': '/html/body/div[228]/div[2]/div[2]/span'}, {'text': 'NOTE 11 - INCOME TAXES', 'xpath': '/html/body/div[231]/div[1]/div[3]/span'}, {'text': 'NOTE 12 - RETIREMENT PLANS', 'xpath': '/html/body/div[234]/div[2]/div[3]/span'}, {'text': 'Defined Contribution Plans', 'xpath': '/html/body/div[234]/div[2]/continuation/div[1]/span'}, {'text': 'Defined Benefit Plans', 'xpath': '/html/body/div[234]/div[2]/continuation/div[3]/span'}, {'text': 'Discount Rate', 'xpath': '/html/body/div[234]/div[2]/continuation/div[5]/span'}, {'text': 'Expected Return on Plan Assets', 'xpath': '/html/body/div[234]/div[2]/continuation/div[7]/span'}, {'text': 'Investment Strategy', 'xpath': '/html/body/div[234]/div[2]/continuation/div[10]/span'}, {'text': 'NOTE 13 - SUMMARY OF QUARTERLY DATA (UNAUDITED)', 'xpath': '/html/body/div[237]/div[3]/div[2]/span'}, {'text': 'NOTE 14 - SEGMENT AND GEOGRAPHIC DATA', 'xpath': '/html/body/div[237]/div[3]/div[4]/span'}, {'text': 'NOTE 15 - ASSET IMPAIRMENTS', 'xpath': '/html/body/div[240]/div[3]/div[2]/span'}, {'text': 'Evaluation of Disclosure Controls and Procedures', 'xpath': '/html/body/div[244]/div[1]/div[5]/span'}, {'text': 'Changes in Internal Control over Financial Reporting', 'xpath': '/html/body/div[244]/div[1]/div[7]/span'}, {'text': \"Management's Report on Internal Control Over Financial Reporting\", 'xpath': '/html/body/div[244]/div[1]/div[9]/span'}, {'text': 'Report of Independent Registered Public Accounting Firm', 'xpath': '/html/body/div[244]/div[3]/div[1]/span'}, {'text': 'Opinion on Internal Control Over Financial Reporting', 'xpath': '/html/body/div[244]/div[3]/div[3]/span'}, {'text': 'Basis for Opinion', 'xpath': '/html/body/div[244]/div[3]/div[6]/span'}, {'text': 'Definition and Limitations of Internal Control Over Financial Reporting', 'xpath': '/html/body/div[244]/div[3]/div[10]/span'}, {'text': 'FORM 10-K—ITEM 15(a) 3. AND ITEM 15(c)', 'xpath': '/html/body/div[256]/span'}, {'text': 'STRYKER CORPORATION AND SUBSIDIARIES', 'xpath': '/html/body/div[257]/span'}, {'text': 'EXHIBIT INDEX', 'xpath': '/html/body/div[258]/span[1]'}, {'text': 'SIGNATURES', 'xpath': '/html/body/div[281]/span'}]\n",
      "items:\n",
      "[]\n",
      "dict to check\n",
      "[]\n",
      "['Item 1', 'Item 1A', 'Item 7', 'ITEM 1', 'ITEM 1A', 'ITEM 7']\n",
      "{}\n",
      "segment dict:\n",
      "{}\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "from lxml import etree\n",
    "import re\n",
    "import openai\n",
    "import pandas as pd\n",
    "from docx import Document\n",
    "import json\n",
    "import unicodedata\n",
    "\n",
    "def parse_annual_report(html_content):\n",
    "    parser = etree.HTMLParser(recover=True)\n",
    "    tree = etree.fromstring(html_content, parser)\n",
    "\n",
    "    def extract_index_from_xpath(xpath):\n",
    "        indices = re.findall(r'\\[(\\d+)\\]', xpath)\n",
    "        return [int(index) for index in indices]\n",
    "\n",
    "    def compare_xpaths(xpath1, xpath2):\n",
    "        indices1 = extract_index_from_xpath(xpath1)\n",
    "        indices2 = extract_index_from_xpath(xpath2)\n",
    "        return indices1 >= indices2\n",
    "\n",
    "    def is_within_bounds(start_xpath, end_xpath, target_xpath):\n",
    "        start_index = extract_index_from_xpath(start_xpath)\n",
    "        end_index = extract_index_from_xpath(end_xpath)\n",
    "        target_index = extract_index_from_xpath(target_xpath)\n",
    "        return start_index <= target_index <= end_index\n",
    "\n",
    "    def is_after_start(start_xpath, target_xpath):\n",
    "        start_index = extract_index_from_xpath(start_xpath)\n",
    "        target_index = extract_index_from_xpath(target_xpath)\n",
    "        return start_index <= target_index\n",
    "\n",
    "    segment_dict = {}\n",
    "    part_pattern = re.compile(r'^PART\\s+([IVXLCDM]+|\\d+)\\.?', re.IGNORECASE)\n",
    "    item_pattern = re.compile(r'(?i)^Item\\s+\\d+[A-Z]?\\.*')\n",
    "\n",
    "    def is_meaningful_text(text):\n",
    "        \"\"\"\n",
    "        Check if the text is meaningful.\n",
    "        - Must contain more than just punctuation or whitespace\n",
    "        - Length check is still applied\n",
    "        \"\"\"\n",
    "        # Remove leading/trailing whitespace\n",
    "        text = text.strip()        \n",
    "        # Check if text has more than just punctuation and whitespace\n",
    "        return bool(re.search(r'\\S', text))\n",
    "    \n",
    "    # Sample XPath query to get bold items\n",
    "    bold_items = tree.xpath(\"//b | //strong | //span[contains(@style, 'font-weight:700')or contains(@style, 'font-weight:bold')] | //font[contains(@style, 'font-weight:bold')] | //p[contains(@style, 'font-weight:bold')]\")\n",
    "    #print(bold_items)\n",
    "    # Preprocess bold items\n",
    "    filtered_bold_items = []\n",
    "    \n",
    "    for item in bold_items:\n",
    "        text_ori = ''.join(item.xpath('.//text()')).strip()\n",
    "        text = unicodedata.normalize('NFKD', text_ori)\n",
    "        if is_meaningful_text(text) and not item.xpath(\"ancestor::table\"):\n",
    "            filtered_bold_items.append(item)\n",
    "    #print('filter check')\n",
    "    #for item in filtered_bold_items:\n",
    "    #    print(''.join(item.xpath('.//text()')).strip())\n",
    "    #    print(get_xpath_of_element(item))\n",
    "    # Initialize lists for bold items\n",
    "    all_bold_items = []\n",
    "    item_bold_items = []\n",
    "    item_section_paths = {}\n",
    "\n",
    "    last_item_xpath = None\n",
    "    last_item_text = \"\"\n",
    "\n",
    "    for item in filtered_bold_items:\n",
    "        if item.tag in ['b', 'strong', 'span', 'font', 'p']:\n",
    "            text_content = ''.join(item.xpath(\"normalize-space(string(.))\")).strip().replace(u'\\xa0', ' ')\n",
    "            if text_content:\n",
    "                xpath = get_xpath_of_element(item)\n",
    "                # Get grandparent (parent of parent) to check if subtopics should be merged\n",
    "                current_grandparent = item.getparent().getparent()\n",
    "                last_grandparent = last_item_grandparent if 'last_item_grandparent' in locals() else None\n",
    "\n",
    "                # Check if the current item and the last item share the same grandparent\n",
    "                if last_item_text is not None and last_grandparent in ['div','p']  and current_grandparent == last_grandparent:\n",
    "                    # Merge with the previous item (subtopics share the same grandparent)\n",
    "                    last_item_text += f\"{text_content}\"\n",
    "                else:\n",
    "                    # Save the last item (merged or single) before moving to a new grandparent\n",
    "                    if last_item_text:\n",
    "                        if item_pattern.match(last_item_text):\n",
    "                            item_bold_items.append({\n",
    "                                'text': last_item_text,\n",
    "                                'xpath': last_item_xpath\n",
    "                            })\n",
    "                        else:\n",
    "                            if not item.xpath(\"ancestor::table\"):\n",
    "                                all_bold_items.append({\n",
    "                                    'text': last_item_text,\n",
    "                                    'xpath': last_item_xpath\n",
    "                                })\n",
    "                    \n",
    "                    # Update last item info to the current one\n",
    "                    last_item_text = text_content\n",
    "                    last_item_xpath = xpath\n",
    "                    last_item_grandparent = current_grandparent  # Track grandparent for the next iteration\n",
    "\n",
    "                # Check if the current item matches a TOC part or item\n",
    "                if compare_xpaths(xpath, toc_xpath):\n",
    "                    if part_pattern.match(text_content):\n",
    "                        if not item.xpath(\"ancestor::table\"):\n",
    "                            part_number = text_content\n",
    "                            if part_number not in segment_dict:\n",
    "                                segment_dict[part_number] = {'xpath': xpath, 'Items': {}}\n",
    "                    elif item_pattern.match(text_content):\n",
    "                        item_number = text_content\n",
    "                        item_section_paths[item_number] = xpath\n",
    "\n",
    "    # Ensure the last item gets processed after the loop\n",
    "    if last_item_text:\n",
    "        if item_pattern.match(last_item_text):\n",
    "            item_bold_items.append({\n",
    "                'text': last_item_text,\n",
    "                'xpath': last_item_xpath\n",
    "            })\n",
    "        else:\n",
    "            all_bold_items.append({\n",
    "                'text': last_item_text,\n",
    "                'xpath': last_item_xpath\n",
    "            })\n",
    "\n",
    "    print(\"bold items:\")\n",
    "    print(all_bold_items)\n",
    "    print(\"items:\")\n",
    "    print(item_bold_items)\n",
    "\n",
    "    sorted_item_paths = sorted(item_section_paths.items(), key=lambda x: extract_index_from_xpath(x[1]))\n",
    "\n",
    "    parts = list(segment_dict.keys())\n",
    "    parts.sort(key=lambda p: extract_index_from_xpath(segment_dict[p]['xpath']))\n",
    "    print(\"dict to check\")\n",
    "    print(parts)\n",
    "    for i, part_number in enumerate(parts):\n",
    "        part_info = segment_dict[part_number]\n",
    "        part_start_xpath = part_info['xpath']\n",
    "        \n",
    "        if i + 1 < len(parts):\n",
    "            next_part_xpath = segment_dict[parts[i + 1]]['xpath']\n",
    "        else:\n",
    "            next_part_xpath = '/html/body'\n",
    "        \n",
    "        for item in item_bold_items:\n",
    "            item_xpath = item['xpath']\n",
    "            if is_within_bounds(part_start_xpath, next_part_xpath, item_xpath):\n",
    "                segment_dict[part_number]['Items'][item['text']] = {'xpath': item_xpath, 'Subtopics': {}}\n",
    "            last_part_xpath = segment_dict[parts[-1]]['xpath'] if parts else '/html/body'\n",
    "            if is_after_start(last_part_xpath, item_xpath):\n",
    "                segment_dict[parts[-1]]['Items'][item['text']] = {'xpath': item_xpath, 'Subtopics': {}}\n",
    "    \n",
    "    # Update to append subtopics correctly\n",
    "    for item_number, item_info in segment_dict.items():\n",
    "        if 'Items' in item_info:\n",
    "            item_xpath = item_info['xpath']\n",
    "            next_item_xpath = '/html/body'  # Default to the end of the document\n",
    "\n",
    "            # Find the next item xpath\n",
    "            for next_item, next_item_xpath_candidate in sorted_item_paths:\n",
    "                if extract_index_from_xpath(next_item_xpath_candidate) > extract_index_from_xpath(item_xpath):\n",
    "                    next_item_xpath = next_item_xpath_candidate\n",
    "                    break\n",
    "\n",
    "            current_subtopic_text = \"\"\n",
    "            current_subtopic_xpath = None\n",
    "\n",
    "            for subtopic in bold_items:\n",
    "                if not subtopic.xpath(\"ancestor::table\"):\n",
    "                    subtopic_text_ori = ''.join(subtopic.xpath(\".//text()\")).strip()\n",
    "                    subtopic_text = unicodedata.normalize('NFKD', subtopic_text_ori)\n",
    "\n",
    "                    if subtopic_text:\n",
    "                        subtopic_xpath = get_xpath_of_element(subtopic)\n",
    "\n",
    "                        if is_within_bounds(item_xpath, next_item_xpath, subtopic_xpath):\n",
    "                            current_index = extract_index_from_xpath(subtopic_xpath)[0]\n",
    "\n",
    "                            # If current subtopic and the previous subtopic share the same div number, append the text\n",
    "                            if current_subtopic_xpath and extract_index_from_xpath(current_subtopic_xpath)[0] == current_index:\n",
    "                                current_subtopic_text += f\" {subtopic_text}\"\n",
    "                            else:\n",
    "                                # Save the previous subtopic (if exists) before starting a new one\n",
    "                                if current_subtopic_text:\n",
    "                                    for part_number, part_info in segment_dict.items():\n",
    "                                        if item_number in part_info['Items']:\n",
    "                                            item_info = part_info['Items'][item_number]\n",
    "                                            if 'Subtopics' not in item_info:\n",
    "                                                item_info['Subtopics'] = {}\n",
    "                                            if current_subtopic_text not in item_info['Subtopics']:\n",
    "                                                item_info['Subtopics'][current_subtopic_text] = {\n",
    "                                                    'xpath': current_subtopic_xpath,\n",
    "                                                    'content': []\n",
    "                                                }\n",
    "                                            item_info['Subtopics'][current_subtopic_text]['content'].append(current_subtopic_text)\n",
    "\n",
    "                                # Start a new subtopic\n",
    "                                current_subtopic_text = subtopic_text\n",
    "                                current_subtopic_xpath = subtopic_xpath\n",
    "\n",
    "            # Save the last subtopic after finishing the loop\n",
    "            if current_subtopic_text:\n",
    "                for part_number, part_info in segment_dict.items():\n",
    "                    if item_number in part_info['Items']:\n",
    "                        item_info = part_info['Items'][item_number]\n",
    "                        if 'Subtopics' not in item_info:\n",
    "                            item_info['Subtopics'] = {}\n",
    "                        if current_subtopic_text not in item_info['Subtopics']:\n",
    "                            item_info['Subtopics'][current_subtopic_text] = {\n",
    "                                'xpath': current_subtopic_xpath,\n",
    "                                'content': []\n",
    "                            }\n",
    "                        item_info['Subtopics'][current_subtopic_text]['content'].append(current_subtopic_text)\n",
    "\n",
    "    # Ensure each item has a subtopic with the same name as the item\n",
    "    for part_number, part_info in segment_dict.items():\n",
    "        for item_number in part_info['Items']:\n",
    "            item_info = part_info['Items'][item_number]\n",
    "            if 'Subtopics' not in item_info:\n",
    "                item_info['Subtopics'] = {}\n",
    "            if item_number not in item_info['Subtopics']:\n",
    "                item_info['Subtopics'][item_number] = {\n",
    "                    'xpath': item_info['xpath'],\n",
    "                    'content': []\n",
    "                }\n",
    "                \n",
    "    # Special Items handling\n",
    "    special_keywords = ['Item 1', 'Item 1A', 'Item 7', 'ITEM 1', 'ITEM 1A', 'ITEM 7']\n",
    "    print(special_keywords)\n",
    "    print(segment_dict)\n",
    "    for part_number, part_info in segment_dict.items():\n",
    "        for item_number, item_info in part_info['Items'].items():\n",
    "            if any(keyword in item_number for keyword in special_keywords):\n",
    "                item_xpath = item_info['xpath']\n",
    "                next_item_xpath = '/html/body'\n",
    "                \n",
    "                # Find the next item xpath\n",
    "                for next_item, next_item_xpath_candidate in sorted_item_paths:\n",
    "                    if extract_index_from_xpath(next_item_xpath_candidate) > extract_index_from_xpath(item_xpath):\n",
    "                        next_item_xpath = next_item_xpath_candidate\n",
    "                        break\n",
    "\n",
    "                current_subtopic_text = \"\"\n",
    "                current_subtopic_xpath = None\n",
    "\n",
    "                for subtopic in all_bold_items:\n",
    "                    subtopic_text = subtopic['text']\n",
    "                    subtopic_xpath = subtopic['xpath']\n",
    "                    #print(\"subtopic text check\")\n",
    "                    #print(subtopic_text) #fine here\n",
    "\n",
    "                    if is_within_bounds(item_xpath, next_item_xpath, subtopic_xpath):\n",
    "                        #print(item_xpath, next_item_xpath, subtopic_xpath)\n",
    "                        current_index = extract_index_from_xpath(subtopic_xpath)[0]\n",
    "                        # If the current subtopic and the previous subtopic share the same div number, append the text\n",
    "                        if current_subtopic_xpath and extract_index_from_xpath(current_subtopic_xpath)[0] == current_index:\n",
    "                            current_subtopic_text += f\" {subtopic_text}\"\n",
    "                        else:\n",
    "                            if current_subtopic_text:\n",
    "                                #print(current_subtopic_text)\n",
    "                                if 'Subtopics' not in item_info:\n",
    "                                    item_info['Subtopics'] = {}\n",
    "                                if current_subtopic_text not in item_info['Subtopics']:\n",
    "                                    item_info['Subtopics'][current_subtopic_text] = {\n",
    "                                        'xpath': current_subtopic_xpath,\n",
    "                                        'content': [current_subtopic_text]\n",
    "                                    }\n",
    "                                else:\n",
    "                                    item_info['Subtopics'][current_subtopic_text]['content'].append(current_subtopic_text)\n",
    "\n",
    "                            # Start a new subtopic\n",
    "                            current_subtopic_text = subtopic_text\n",
    "                            current_subtopic_xpath = subtopic_xpath\n",
    "                    #else:\n",
    "                        #print(\"not in boud\")\n",
    "                        #print(f\"{subtopic_text}:{item_xpath}, {next_item_xpath}, {subtopic_xpath}\")\n",
    "\n",
    "                # Save the last subtopic after finishing the loop\n",
    "                if current_subtopic_text:\n",
    "                    if 'Subtopics' not in item_info:\n",
    "                        item_info['Subtopics'] = {}\n",
    "                    if current_subtopic_text not in item_info['Subtopics']:\n",
    "                        item_info['Subtopics'][current_subtopic_text] = {\n",
    "                            'xpath': current_subtopic_xpath,\n",
    "                            'content': [current_subtopic_text]\n",
    "                        }\n",
    "                    else:\n",
    "                        item_info['Subtopics'][current_subtopic_text]['content'].append(current_subtopic_text)\n",
    "    print(\"segment dict:\")\n",
    "    print(segment_dict)\n",
    "\n",
    "\n",
    "    def extract_content_between(start_element, end_element):\n",
    "        content = {\n",
    "            \"tables\": [],\n",
    "            \"text\": []\n",
    "        }\n",
    "        def get_parent(element):\n",
    "            \"\"\"Retrieve the correct parent based on tag type.\"\"\"\n",
    "            parent_node = element.getparent()\n",
    "            outer_parent_node = parent_node.getparent() if parent_node is not None else None\n",
    "\n",
    "            # Return outer parent if element is within bold/strong tags\n",
    "            if outer_parent_node is not None and parent_node.tag not in ['p', 'div'] and element.tag in ['b', 'strong']:\n",
    "                return outer_parent_node\n",
    "            # Return parent if it's not a div\n",
    "            elif parent_node is None or parent_node.tag != 'div':\n",
    "                return parent_node\n",
    "            # Return outer div if both parent and outer parent are divs\n",
    "            if outer_parent_node is not None and outer_parent_node.tag == 'div':\n",
    "                return outer_parent_node\n",
    "            # Otherwise, return the immediate parent div\n",
    "            return parent_node\n",
    "    \n",
    "        def find_table(element):\n",
    "            \"\"\"Recursively search for a <table> element within children.\"\"\"\n",
    "            if element.tag == 'table':\n",
    "                return element\n",
    "            for child in element:\n",
    "                result = find_table(child)  # Recursive search in child elements\n",
    "                if result is not None:\n",
    "                    return result\n",
    "            return None  # Return None if no <table> found\n",
    "        \n",
    "        def parse_html_table_lxml(table_element):\n",
    "            \"\"\"Parse a <table> element into a structured format, adjusting for the smallest colspan and removing empty rows.\"\"\"\n",
    "            table_data = []\n",
    "            \n",
    "            # First, find the smallest colspan value in the entire table\n",
    "            min_colspan = float('inf')  # Set to infinity to ensure any colspan is smaller\n",
    "            for row in table_element.xpath(\".//tr\"):\n",
    "                for cell in row.xpath(\".//td | .//th\"):\n",
    "                    colspan = int(cell.get('colspan', 1))  # Default colspan is 1 if not specified\n",
    "                    #print(colspan)\n",
    "                    min_colspan = min(min_colspan, colspan)\n",
    "            \n",
    "            # Now, parse the table and adjust based on the smallest colspan\n",
    "            for row in table_element.xpath(\".//tr\"):\n",
    "                row_data = []\n",
    "                current_column = 0  # Track the current column index in the row\n",
    "                \n",
    "                for cell in row.xpath(\".//td | .//th\"):\n",
    "                    cell_text = ''.join(cell.itertext()).strip()\n",
    "                    colspan = int(cell.get('colspan', 1))  # Default colspan is 1 if not specified\n",
    "                    \n",
    "                    # Calculate the number of empty cells needed to align this cell with the smallest colspan\n",
    "                    gap = (colspan - min_colspan)  # Add this many empty cells before the actual cell\n",
    "                    \n",
    "                    # Add the necessary empty cells before the content\n",
    "                    row_data.extend([''] * gap)\n",
    "                    \n",
    "                    # Add the content of the cell\n",
    "                    row_data.append(cell_text)\n",
    "                    current_column += colspan  # Move the column index by the colspan value\n",
    "                \n",
    "                # Only add rows that are not completely empty\n",
    "                if any(cell != '' for cell in row_data):\n",
    "                    table_data.append(row_data)\n",
    "            \n",
    "            return table_data\n",
    "        # Identify parent nodes for traversal\n",
    "        start_parent = get_parent(start_element)\n",
    "        end_parent = get_parent(end_element)\n",
    "\n",
    "            # Traverse elements between start and end\n",
    "        node = start_parent\n",
    "        while node is not None:\n",
    "            if node.tag in ['p', 'div', 'table']:\n",
    "                # Extract text if <table> is absent in this node\n",
    "                table = find_table(node)\n",
    "                if table is None:\n",
    "                    text_content = etree.tostring(node, encoding='unicode', method='text').strip()\n",
    "                    if text_content:\n",
    "                        content[\"text\"].append(text_content)\n",
    "                else:\n",
    "                    # If a table exists, parse it and add to the content\n",
    "                    structured_table = parse_html_table_lxml(table)\n",
    "                    content[\"tables\"].append(structured_table)\n",
    "            \n",
    "            # Stop traversal once reaching end_element's parent\n",
    "            if node == end_parent:\n",
    "                break\n",
    "\n",
    "            # Move to next node or next parent level if no sibling\n",
    "            next_node = node.getnext()\n",
    "            if next_node is None:\n",
    "                parent_node = node.getparent()\n",
    "                node = parent_node.getnext() if parent_node is not None else None\n",
    "            else:\n",
    "                node = next_node\n",
    "\n",
    "        # Explicitly add the end element's content\n",
    "        if end_element.tag in ['p', 'div', 'table']:\n",
    "            table = find_table(end_element)\n",
    "            if table is None:\n",
    "                text_content = etree.tostring(end_element, encoding='unicode', method='text').strip()\n",
    "                if text_content:\n",
    "                    content[\"text\"].append(text_content)\n",
    "            else:\n",
    "                structured_table = parse_html_table_lxml(table)\n",
    "                content[\"tables\"].append(structured_table)\n",
    "\n",
    "        return content\n",
    "\n",
    "    def extract_subtopics(segment_dict):\n",
    "        subtopics_list = []\n",
    "        \n",
    "        # Add Parts\n",
    "        for part_number, part_info in segment_dict.items():\n",
    "            subtopics_list.append((part_number, part_info['xpath']))\n",
    "            \n",
    "            # Add Items within Parts\n",
    "            for item_number, item_info in part_info['Items'].items():\n",
    "                subtopics_list.append((item_number, item_info['xpath']))\n",
    "                \n",
    "                # Add Subtopics within Items\n",
    "                if 'Subtopics' in item_info:\n",
    "                    for subtopic_text, subtopic_info in item_info['Subtopics'].items():\n",
    "                        subtopics_list.append((subtopic_text, subtopic_info['xpath']))\n",
    "        \n",
    "        return subtopics_list\n",
    "\n",
    "    def get_fallback_xpath(xpath):\n",
    "    # Create a fallback XPath without /span\n",
    "        if '/span' in xpath:\n",
    "            return xpath.rsplit('/span', 1)[0]\n",
    "        return xpath\n",
    "\n",
    "    # Generate the list of subtopics and their XPaths\n",
    "    subtopics_list = extract_subtopics(segment_dict)\n",
    "    print(subtopics_list)\n",
    "    # Iterate through the subtopics_list and extract content\n",
    "    subtopic_content = []\n",
    "    for idx, (subtopic_text, subtopic_xpath) in enumerate(subtopics_list):\n",
    "    # Determine the next subtopic's XPath or end of document\n",
    "        if idx < len(subtopics_list) - 1:\n",
    "            next_subtopic_xpath = subtopics_list[idx + 1][1]\n",
    "        else:\n",
    "            # For the last subtopic, handle the transition to the next part if it exists\n",
    "            next_part_start_xpath = None\n",
    "            for part_number, part_info in segment_dict.items():\n",
    "                if part_info['xpath'] == subtopic_xpath:\n",
    "                    part_index = list(segment_dict.keys()).index(part_number)\n",
    "                    if part_index < len(list(segment_dict.keys())) - 1:\n",
    "                        next_part_number = list(segment_dict.keys())[part_index + 1]\n",
    "                        next_part_start_xpath = segment_dict[next_part_number]['xpath']\n",
    "                        break\n",
    "            next_subtopic_xpath = next_part_start_xpath if next_part_start_xpath else '//div[last()] | //p[last()]'\n",
    "\n",
    "        # Get the start and end elements for extraction\n",
    "        start_elements = tree.xpath(subtopic_xpath)\n",
    "        end_elements = tree.xpath(next_subtopic_xpath)\n",
    "\n",
    "        if not start_elements:\n",
    "            fallback_xpath = get_fallback_xpath(subtopic_xpath)\n",
    "            start_elements = tree.xpath(fallback_xpath)\n",
    "            print(f\"  Fallback XPath: {fallback_xpath}\")\n",
    "\n",
    "        if not end_elements:\n",
    "            fallback_xpath = get_fallback_xpath(next_subtopic_xpath)\n",
    "            end_elements = tree.xpath(fallback_xpath)\n",
    "            print(f\"  Fallback Next XPath: {fallback_xpath}\")\n",
    "\n",
    "        if not start_elements:\n",
    "            print(f\"  Start XPath '{subtopic_text}' not found.\")\n",
    "        if not end_elements:\n",
    "            print(f\"  End XPath '{next_subtopic_xpath}' not found.\")\n",
    "        if start_elements and end_elements:\n",
    "            start_element = start_elements[0]\n",
    "            end_element = end_elements[0]\n",
    "            if start_element != end_element:\n",
    "                # Extract content and tables separately\n",
    "                extracted_content = extract_content_between(start_element, end_element)\n",
    "                text_content = extracted_content.get('text', [])\n",
    "                tables_content = extracted_content.get('tables', [])\n",
    "\n",
    "                if text_content or tables_content:\n",
    "                    # Process text content individually\n",
    "                    formatted_text_content = [\n",
    "                        unicodedata.normalize('NFKD', text.replace('\\n', ' ').replace('  ', ' ').replace('•', '\\n•'))\n",
    "                        for text in text_content\n",
    "                    ]\n",
    "\n",
    "                    for part_number, part_info in segment_dict.items():\n",
    "                        if 'Items' in part_info:\n",
    "                            for item_number, item_info in part_info['Items'].items():\n",
    "                                if 'Subtopics' in item_info and subtopic_text in item_info['Subtopics']:\n",
    "                                    subtopic_info = item_info['Subtopics'][subtopic_text]\n",
    "                                    \n",
    "                                    # Initialize 'content' structure if not already present\n",
    "                                    if 'content' not in subtopic_info or not isinstance(subtopic_info['content'], dict):\n",
    "                                        subtopic_info['content'] = {\n",
    "                                            \"text\": [],\n",
    "                                            \"tables\": []\n",
    "                                        }\n",
    "\n",
    "                                    # Append text and tables content to their respective lists\n",
    "                                    subtopic_info['content']['text'].extend(formatted_text_content)\n",
    "                                    subtopic_info['content']['tables'].extend(tables_content)\n",
    "\n",
    "                                    print(f\"Updated content for subtopic '{subtopic_text}'\")\n",
    "        print(\"Content for each subtopic:\")\n",
    "    for part_number, part_info in segment_dict.items():\n",
    "        #print(f\"Part: {part_number}\")\n",
    "        #print(f\"  XPath: {part_info['xpath']}\")\n",
    "        for item_number, item_info in part_info['Items'].items():\n",
    "            #print(f\"  Item: {item_number}\")\n",
    "            #print(f\"    XPath: {item_info['xpath']}\")\n",
    "            if 'Subtopics' in item_info:\n",
    "                for subtopic_text, subtopic_info in item_info['Subtopics'].items():\n",
    "                    print(f\"Subtopic: {subtopic_text}\")\n",
    "                    print(f\"Subtopic XPath: {subtopic_info.get('xpath', [])}\")\n",
    "                    print(f\"Content: {subtopic_info.get('content', [])}\")\n",
    "    \n",
    "    return segment_dict\n",
    "#print(processed_html_content)\n",
    "segment_dict = parse_annual_report(result_html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2df43277-a4ce-49b7-bd8f-df848fb7fba2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nwith open(f'{current_html}.json', 'w') as json_file:\\n    json.dump(segment_dict, json_file, indent=4)\\n\""
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "with open(f'{current_html}.json', 'w') as json_file:\n",
    "    json.dump(segment_dict, json_file, indent=4)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9734fe74",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('output.json', 'w') as json_file:\n",
    "    json.dump(segment_dict, json_file, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
