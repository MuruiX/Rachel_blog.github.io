# statistical concept çµ±è¨ˆå­¸æ¦‚å¿µ

## PMF and CDF

PMFå®šç¾©çš„å€¼æ˜¯P(X=x)ï¼Œè€ŒCDFå®šç¾©çš„å€¼æ˜¯P(X <= x)ï¼Œxç‚ºæ‰€æœ‰çš„å¯¦æ•¸ç·šä¸Šçš„é»ã€‚

### probability mass function (PMF) æ¦‚ç‡è³ªé‡å‡½æ•¸

$$
pX(x)=P(X=x)
$$

æ˜¯é›¢æ•£éš¨æ©Ÿè®Šæ•¸åœ¨å„å€‹ç‰¹å®šå–å€¼ä¸Šçš„æ¦‚ç‡ã€‚æœ‰æ™‚ä¹Ÿè¢«ç¨±ç‚º**é›¢æ•£å¯†åº¦å‡½æ•¸**ã€‚

æ¦‚ç‡å¯†åº¦å‡½æ•¸é€šå¸¸æ˜¯*å®šç¾©é›¢æ•£éš¨æ©Ÿåˆ†ä½ˆçš„ä¸»è¦æ–¹æ³•*ï¼Œ

ä¸¦ä¸”æ­¤é¡å‡½æ•¸å­˜åœ¨æ–¼å…¶å®šç¾©åŸŸæ˜¯ï¼š

1. é›¢æ•£çš„ç´”é‡è®Šæ•¸
2. å¤šé éš¨æ©Ÿè®Šæ•¸

[ç¶­åŸºç™¾ç§‘](https://zh.wikipedia.org/zh-tw/%E6%A6%82%E7%8E%87%E8%B4%A8%E9%87%8F%E5%87%BD%E6%95%B0)

### Cumulative distribution function(CDF)ç´¯ç©åˆ†ä½ˆå‡½æ•¸

$$
FX(x)=P(X<=x)
$$

ä¹Ÿå«æ¦‚ç‡åˆ†ä½ˆå‡½æ•¸æˆ–åˆ†ä½ˆå‡½æ•¸

æ˜¯æ¦‚ç‡å¯†åº¦å‡½æ•¸çš„**ç©åˆ†**

èƒ½å¤ å®Œæ•´çš„æè¿°ä¸€å€‹å¯¦éš¨æ©Ÿè®Šæ•¸Xçš„æ¦‚ç‡åˆ†ä½ˆ

[ç¶­åŸºç™¾ç§‘](https://zh.wikipedia.org/zh-tw/%E7%B4%AF%E7%A7%AF%E5%88%86%E5%B8%83%E5%87%BD%E6%95%B0)

## probability density functionï¼ˆPDFï¼‰æ¦‚ç‡å¯†åº¦å‡½æ•¸

[PDF](https://www.geeksforgeeks.org/probability-density-function/)

[æ¦‚ç‡å¯†åº¦å‡½æ•¸ï¼ˆProbability Density Functionï¼Œ PDFï¼‰-CSDNåšå®¢](https://blog.csdn.net/YHKKun/article/details/137404224)

## Central Limits ä¸­å¤®ç•Œé™

Support we have a set of independent random variables $X_{i}$ for $i=1,....,n$ with:

$Mean(X_{i})=\mu$        $Var(X_{i})=V$       for all $i$

Then as $n$ becomes large, the sum:

$$
S_{m}=\sum\limits_{i = 1}^n {{X_i} \to {\rm N}(n\mu ,nV)}
$$

**tends to become normally distributed.**

### Absence of Central Limits

**Another case is where the moments are not defined / infinite å¦ä¸€ç¨®æƒ…æ³æ˜¯åŠ›çŸ©ä¸ç¢ºå®šæˆ–ç„¡é™å¤§**

# Randomness éš¨æ©Ÿæ€§

## Motivation å‹•æ©Ÿ

Three main ways that random comes into data science:

1. The data themselves are often best understood as random æ•¸æ“šæœ¬èº«é€šå¸¸æœ€å¥½è¢«ç†è§£ç‚ºéš¨æ©Ÿ
2. **When we want to reason under **subjective uncertainty **(for example in Bayesian approaches) then unknown quantities can be represented as random. Often when we make predictions they will be **probabilitic ç•¶æˆ‘å€‘ä¸»ç®¡ä¸ç¢ºå®šæ€§çš„æƒ…æ³ä¸‹é€²è¡Œæ¨ç†æ™‚ï¼Œå¯ä»¥å°‡æœªçŸ¥é‡è¡¨ç¤ºç‚ºéš¨æ©Ÿé‡ï¼Œç•¶æˆ‘å€‘é€²è¡Œé æ¸¬æ™‚ï¼Œä»–å€‘å°‡æ˜¯æ¦‚ç‡æ€§çš„
3. Many of the most effective / efficient / commonlyâ€‘used algorithms in data scienceâ€”typically called Monte Carlo algorithmsâ€”exploit randomness. è’™ç‰¹å¡æ´›ç®—æ³•

- Unpredictable ä¸å¯é æ¸¬æ€§
- Subjective uncertainty ä¸»ç®¡ä¸ç¢ºå®šæ€§

## The logistic map é‚è¼¯åœ°åœ–ï¼ˆå–®å³°æ˜ è±¡ï¼‰

æ˜¯ä¸€å€‹äºŒæ¬¡å¤šé …å¼æ˜ å°„(éæ­¸é—œä¿‚)ç»å¸¸ä½œä¸ºå…¸å‹èŒƒä¾‹æ¥è¯´æ˜å¤æ‚çš„æ··æ²Œç°è±¡æ˜¯å¦‚ä½•ä»éå¸¸ç®€å•çš„éçº¿æ€§åŠ¨åŠ›å­¦æ–¹ç¨‹ä¸­äº§ç”Ÿçš„ã€‚

is an example of deterministic chaos æ˜¯ç¢ºå®šæ€§æ··æ²Œçš„ä¸€å€‹ä¾‹å­ but whose results are apparently not easy to predict. çµæœä¸å®¹æ˜“è¢«é æ¸¬

å®ƒåœ¨ä¸€å®šç¨‹åº¦ä¸Šæ˜¯ä¸€ä¸ªæ—¶é—´ç¦»æ•£çš„äººå£ç»Ÿè®¡æ¨¡å‹

Logisticæ¨¡å‹å¯ä»¥æè¿°ç”Ÿç‰©ç¨®ç¾¤çš„æ¼”åŒ–ï¼Œå®ƒå¯ä»¥è¡¨ç¤ºæˆä¸€ç¶­éç·šæ€§è¿­ä»£æ–¹ç¨‹ $x_{n+1}=rx_{n}(1-x_{n})$

å…¶ä¸­ $ğ‘¥_{ğ‘›}$  è¡¨ç¤ºç§ç¾¤çš„ä¸ªæ•°ï¼Œç¬¬ä¸€é¡¹è¡¨ç¤ºç¬¬n+1ä»£ä¸ç¬¬nä»£æˆæ­£æ¯”ï¼Œç¬¬äºŒé¡¹è¡¨ç¤ºç¯å¢ƒå› ç´ å¯¹ç§ç¾¤ç¹æ®–æ•°ç›®çš„é™åˆ¶ã€‚å½“ç„¶æˆ‘ä»¬å·²ç»å‡å®š$ğ‘¥$ å¤§äºé›¶ä¸”å°äºä¸€,$ğ‘Ÿ$å¤§äºé›¶ä¸”å°äºå››ï¼ˆæ˜¾ç„¶ï¼‰ã€‚ä»–ä»¬ä¹‹é—´çš„å‡½æ•°å…³ç³»ä¸ºæŠ›ç‰©çº¿å‹ã€‚è¿™æ˜¯ä¸€ç§å…¸å‹çš„è´Ÿåé¦ˆæœºåˆ¶ï¼ˆå•å³°ï¼‰ï¼Œlogisticæ¨¡å‹æ˜¯äºŒæ¬¡å‡½æ•°å…³ç³»ï¼Œå½“ç„¶æˆ‘ä»¬è¿˜å¯ä»¥è®¾å®šä¸ºsinä¹‹ç±»çš„å‡½æ•°ã€‚å› ä¸ºå•å³°æ˜¯ä¸€ç§æœ€ç®€å•ï¼Œä¹Ÿæ˜¯è‡ªç„¶ç•Œæœ€å¸¸è§çš„ä¸€ç§è´Ÿåé¦ˆæœºåˆ¶

Math:

$$
\displaystyle{ x(t+1)=\mu x(t)(1-x(t)) }
$$

å…¶ä¸­ï¼Œtä¸ºè¿­ä»£æ—¶é—´æ­¥ï¼Œå¯¹äºä»»æ„çš„tï¼Œx(t)âˆˆ[0,1]ï¼ŒÎ¼ä¸ºä¸€å¯è°ƒå‚æ•°ï¼Œä¸ºäº†ä¿è¯æ˜ å°„å¾—åˆ°çš„x(t)å§‹ç»ˆä½äº[0,1]å†…ï¼Œåˆ™Î¼âˆˆ[0,4]ã€‚x(t)ä¸ºåœ¨tæ—¶åˆ»ç§ç¾¤å æœ€å¤§å¯èƒ½ç§ç¾¤è§„æ¨¡çš„æ¯”ä¾‹ï¼ˆå³ç°æœ‰äººå£æ•°ä¸æœ€å¤§å¯èƒ½äººå£æ•°çš„æ¯”ç‡ï¼‰ã€‚å½“å˜åŒ–ä¸åŒçš„å‚æ•°Î¼æ—¶ï¼Œè¯¥æ–¹ç¨‹ä¼šå±•ç°å‡ºä¸åŒçš„åŠ¨åŠ›å­¦æé™è¡Œä¸ºï¼ˆå³å½“tè¶‹äºæ— ç©·å¤§ï¼Œx(t)çš„å˜åŒ–æƒ…å†µï¼‰ï¼ŒåŒ…æ‹¬ï¼šç¨³å®šç‚¹ï¼ˆå³æœ€ç»ˆx(t)å§‹ç»ˆä¸ºåŒä¸€ä¸ªæ•°å€¼ï¼‰ã€å‘¨æœŸï¼ˆx(t)ä¼šåœ¨2ä¸ªæˆ–è€…å¤šä¸ªæ•°å€¼ä¹‹é—´è·³è·ƒ)ã€ä»¥åŠæ··æ²Œï¼ˆx(t)çš„ç»ˆæ€ä¸ä¼šé‡å¤ï¼Œè€Œä¼šç­‰æ¦‚ç‡åœ°å–éæŸåŒºé—´ï¼‰ã€‚

å½“Î¼è¶…è¿‡[1,4]æ—¶ï¼Œå°±ä¼šå‘ç”Ÿæ··æ²Œç°è±¡ã€‚è¯¥éçº¿æ€§å·®åˆ†æ–¹ç¨‹æ„åœ¨è§‚å¯Ÿä¸¤ç§æƒ…å½¢:
â€¢ å½“äººå£è§„æ¨¡å¾ˆå°æ—¶ï¼Œäººå£å°†ä»¥ä¸å½“å‰äººå£æˆæ¯”ä¾‹çš„é€Ÿåº¦å¢é•¿è¿›è¡Œç¹æ®–ã€‚
â€¢ é¥¥é¥¿(ä¸å¯†åº¦æœ‰å…³çš„æ­»äº¡ç‡) ï¼Œå…¶å¢é•¿ç‡å°†ä»¥ä¸ç¯å¢ƒçš„â€æ‰¿å—èƒ½åŠ›â€å‡å»å½“å‰äººå£æ‰€å¾—å€¼æˆæ­£æ¯”çš„é€Ÿåº¦ä¸‹é™

ç„¶è€Œï¼ŒLogisticæ˜ å°„ä½œä¸ºä¸€ç§äººå£ç»Ÿè®¡æ¨¡å‹ï¼Œå­˜åœ¨ç€ä¸€äº›åˆå§‹æ¡ä»¶å’Œå‚æ•°å€¼(å¦‚Î¼ >4)ä¸ºæŸå€¼æ—¶æ‰€å¯¼è‡´çš„æ··æ²Œé—®é¢˜ã€‚è¿™ä¸ªé—®é¢˜åœ¨è¾ƒè€çš„ç‘å…‹æ¨¡å‹ä¸­æ²¡æœ‰å‡ºç°ï¼Œè¯¥æ¨¡å‹ä¹Ÿå±•ç¤ºäº†æ··æ²ŒåŠ¨åŠ›å­¦ã€‚

$0<=Î¼<=1$:

![1735432764631](image/note/1735432764631.png)

$1<Î¼<3$:

![1735432855695](image/note/1735432855695.png)

ä»å›¾ä¸­å¾—çŸ¥ï¼š

å½“$Î¼$åœ¨1åˆ°2ä¹‹é—´ï¼Œç§ç¾¤æ•°é‡ä¼šå¾ˆå¿«æ¥è¿‘ $ \frac{Î¼âˆ’1}{Î¼}$ ,ä¸è®ºæœ€åˆç§ç¾¤ä¸ºä½•å€¼
å½“$Î¼$åœ¨2åˆ°3ä¹‹é—´ï¼Œäººå£æ•°åœ¨ç»å†ä¸€æ®µæ—¶é—´çš„æ³¢åŠ¨åä¼šè¶‹äºç¨³å®šå€¼ $\frac{Î¼âˆ’1}{Î¼}$,å…¶æ”¶æ•›é€Ÿåº¦æ»¡è¶³çº¿æ€§å˜åŒ–ã€‚ä½†å½“Î¼=3æ—¶ï¼Œæ¯”çº¿æ€§æ”¶æ•›è¿˜è¦ç¼“æ…¢ã€‚

äº‹å®ä¸Šï¼Œåªè¦Î¼ < 3 Î¼<3Î¼<3ï¼Œç³»ç»Ÿéƒ½ä¼šæ”¶æ•›åˆ°ä¸€ä¸ªä¸åŠ¨ç‚¹ï¼Œè€Œè¿™ä¸ªä¸åŠ¨ç‚¹çš„æ•°å€¼å¯ä»¥é€šè¿‡æ±‚è§£ä¸‹åˆ—æ–¹ç¨‹è€Œå¾—åˆ°ï¼š

$$
X^{*}=\mu x^{*}(1-x^{*})
$$

ä¸åŒå‚æ•° Î¼ \muÎ¼ä¸‹çš„æé™è¡Œä¸º
ä¸è®ºÎ¼å–ä»»æ„å€¼ï¼Œæœ€å¤šæœ‰ä¸€ä¸ªç¨³å®šçš„å‘¨æœŸã€‚ å¦‚æœå­˜åœ¨ä¸€ä¸ªç¨³å®šçš„å‘¨æœŸï¼Œé‚£ä¹ˆå®ƒæ˜¯å…¨å±€ç¨³å®šçš„ï¼Œå¸å¼•äº†å‡ ä¹æ‰€æœ‰çš„ç‚¹ã€‚ä¸€äº›å…·æœ‰å‘¨æœŸç¨³å®šå¾ªç¯çš„Î¼åœ¨æŸäº›æ—¶å€™ä¼šæœ‰æ— ç©·å¤šä¸ªä¸åŒå‘¨æœŸçš„ä¸ç¨³å®šå¾ªç¯ã€‚

ä¸ºäº†æ¦‚æ‹¬ä¸Šè¿°å„ç§æ•°å€¼æ¨¡æ‹Ÿè¯•éªŒçš„ç»“æœï¼Œæˆ‘ä»¬ç”¨ä¸‹é¢çš„ç›¸å›¾æ¥è¡¨ç¤ºä¸åŒÎ¼å€¼å¯¹åº”çš„æé™è¡Œä¸º

### Feigenbaumå›¾

![1735433138747](image/note/1735433138747.png)

### Code operation å¯¦ç¾æ“ä½œ

```python
# Define a function that does one step of the logistic map
def logisticMap(x,r):
    return r*x*(1.0-x)
```

```python
# Construct an orbit of the logistic map
nSteps = 100
t = range(0,(nSteps + 1))
r = 3.9
x = np.zeros(nSteps + 1) # initialise the result as a vector of zeroes å¾0é–‹å§‹
x[0] = 0.1 # Choose the initial condition
for i in range(1,(nSteps + 1)):
    x[i] = logisticMap(x[i-1],r)
```

```python
# Plot the results of the previous step
plt.plot(t, x, '-', linewidth=1,color='black')
plt.scatter( t, x, color="darkviolet" ) # add dots at the points
plt.title('Logistic Map: r=' + str(r))
plt.xlabel('Step, n')
plt.ylabel('x(n)')
plt.show()
```

![1735475233662](image/note/1735475233662.png)

[Logistic Mapping - Encyclopedia - Complex Systems|Artificial Intelligence|Complex Science|Complex Networks|Self-Organizing](https://wiki.swarma.org/index.php?title=Logistic%E6%98%A0%E5%B0%84#.5Bmath.5D.5Cdisplaystyle.7B_0.E2.89.A4.CE.BC.E2.89.A41_.7D.5B.2Fmath.5D)

[ç‰©æµåœ– - ç¶­çªç™¾ç§‘ï¼Œè‡ªç”±çš„ç™¾ç§‘å…¨æ›¸ --- Logistic map - Wikipedia](https://en.wikipedia.org/wiki/Logistic_map)

[Logicsticæ˜ å°„](https://blog.csdn.net/qq_41137110/article/details/115249684)

[Logisticæ˜ å°„ï¼ˆç¼–ç¨‹è®­ç»ƒï¼‰ - çŸ¥ä¹](https://zhuanlan.zhihu.com/p/578152437)

[Logistic.nb.pdf](https://abel.math.harvard.edu/archive/118r_spring_99/doc/proj/2/Logistic.nb.pdf)

## entropy ç†µ

å¦ä¸€ç§æ–¹æ³•æ˜¯åˆ©ç”¨è®¡ç®—æœºå¤–éƒ¨å› ç´ æ¥äº§ç”Ÿéšæœºæ€§ï¼Œ ä¾‹å¦‚é¼ æ ‡ç‚¹å‡»çš„ä½ç½®å’Œæ—¶é—´ã€‚ åœ¨æ­¤ï¼Œ æˆ‘ä»¬å°†è€ƒè™‘æŠŠä»£ç è¿è¡Œçš„æ—¶é—´ä½œä¸ºå¤–éƒ¨å› ç´ 

å³ä½¿ç”¨ç³»ç»Ÿæ—¶é’Ÿå½“å‰æ—¶é—´çš„å°æ•°ç‚¹åå…­ä½æ•°å­—ï¼ˆåˆ†è¾¨ç‡ä¸ºå¾®ç§’ï¼‰

Rå’ŒMatlab ä½¿ç”¨è»Ÿä»¶åŒ…æä¾›éš¨æ©Ÿæ•¸ç”Ÿæˆçš„å‡½æ•¸

## Estimation of $Ï€$ **using Monte Carlo methods**

å‡è¨­æˆ‘å€‘å°‡ $\pi$ å®šç¾©ç‚ºåŠå¾‘ç‚º1çš„åœ“çš„é¢ç©æ ¹æ“šå…¶å®šç¾©ä¾†ä¼°ç®—é€™å€‹æ•¸å­—ï¼Œ

we will pick random values of $x$ and $y$ independently from a uniform distribution between 0 and **1**, then let the random variable $Z$ equal 1 if the point $(x, y)$ falls within the quarter-circle shown and 0 otherwise. This $Z$ allows us to make an estimate of $Ï€$ **in that its expected value, $E[Z] = Ï€/4$**. We can then define a random variable **A**n to be the average of $n$ **independent samples of $Z$**. Formally:

$$
{{\rm{A}}_n} = \frac{1}{n}\sum\limits_{i = 1}^n {{Z_i} = \frac{\pi }{4} + {\varepsilon _n}}
$$

### Code operation

To deal with this, we'll repeat the experiment $m$ times and make a list of all the estimates

we get. We'll then arrange these results in ascending order and throw away a certain fraction $\alpha$ of the largest and smallest results. The remaining values should provide decent upper and lower bounds for an interval containing $\pi$.

```python
m = 100 # Number of estimates taken
n = 80000 # Number of points used in each estimate
```

If we increase $n$ above, we should get a more accurate estimates of $\pi$ each tme we run the experiment, while if we increase $m$, we'll get more accurate estimates of the endpoints of an interval containing $\pi$.

```python
#Generate a set of m estimates of the area of a unit-radius quarter-circle
np.random.seed(42) # Seed the random number generator
A = np.zeros(m) # A will hold our m estimates
for i in range(0,m):
    for j in range(0,n):
        # Generate an (x, y) pair in the unit square
        x = np.random.rand()
        y = np.random.rand()
    
        # Decide whether the point lies in or on
        # the unit circle and set Z accordingly
        r = x**2 + y**2
        if ( r <= 1.0):
            Z = 1.0
        else:
            Z = 0
        
        # Add up the contribution to the current estimate
        A[i] = A[i] + Z
       
    # Convert the sum we've built to an estimate of pi
    A[i] = 4.0 * A[i] / float( n )
```

```python
# Calculate approximate 95% confidence interval for pi based on our Monte Carlo estimates
pi_estimates = np.sort(A)
piLower = np.percentile(pi_estimates,2.5)
piUpper = np.percentile(pi_estimates,97.5)
print(f'We estimate that pi lies between {piLower:.3f} and {piUpper:.3f}.')
```

# standard distribution

## Bernoulli ä¼¯åŠªåˆ©åˆ†ä½ˆ

$$
P(X=x) = p^{x}(1-p)^{1-x}, x = 0, 1; 0 < p < 1
$$

only have two choices(binary situations). åªæœ‰å…©å€‹çµæœ ä¾‹å¦‚æˆåŠŸå¤±æ•— ç¡¬å¹£æ­£åé¢

****Random Variable (X):**** In the context of Bernoulli Distribution, X represents the variable that can take the values 1 or 0, denoting the number of successes occurring.

****Bernoulli Trial:**** An individual experiment or trial with only two possible outcomes.

****Bernoulli Parameter:**** This refers to the probability of success (p) in a Bernoulli Distribution.

Mean:

$$
E[X] = Î¼ = p
$$

Variance:

$$
Var[X] = E[X^{2}] - (E[X])^2
      \\ =Ïƒ2 = p(1 - p) \ or\  pq
$$

###### **Applications of Bernoulli Distribution in Business Statistics**

****1. Quality Control:**** In manufacturing, every product undergoes quality checks. Bernoulli Distribution helps assess whether a product passes (success) or fails (failure) the quality standards. By analysing the probability of success, manufacturers can evaluate the overall quality of their production process and make improvements.

****2. Market Research:**** Bernoulli Distribution is useful in surveys and market research when dealing with yes/no questions. ****For instance,**** when surveying customer satisfaction, responses are often categorised as satisfied (success) or dissatisfied (failure). Analysing these binary outcomes using Bernoulli Distribution helps companies gauge customer sentiment.

****3. Risk Assessment:**** In the context of risk management, the Bernoulli Distribution can be applied to model events with binary outcomes, such as a financial investment succeeding (success) or failing (failure). The probability of success serves as a key parameter for assessing the risk associated with specific investments or decisions.

****4. Marketing Campaigns:**** Businesses use Bernoulli Distribution to measure the effectiveness of marketing campaigns. ****For instance,**** in email marketing, success might represent a recipient opening an email, while failure indicates not opening it. Analysing these binary responses helps refine marketing strategies and improve campaign success rates.

###### Difference between Bernoulli Distribution and Binomial Distribution ä¼¯åŠªåˆ©åˆ†ä½ˆå’ŒäºŒé …åˆ†ä½ˆçš„å€åˆ†

The Bernoulli Distribution and the Binomial Distribution are both used to model random experiments with *binary outcomes*, but they differ in how they handle multiple trials or repetitions of these experiments. åŒæ¨£æ˜¯å°å…·æœ‰äºŒå…ƒçµæœçš„éš¨æ©Ÿå¯¦é©—é€²è¡Œå»ºæ¨¡ï¼Œä½†åœ¨è™•ç†å¤šæ¬¡å¯¦é©—çš„æ–¹å¼ä¸Šæœ‰æ‰€ä¸åŒ

| Basis                                 | Bernoulli Distribution                              | Binomial Distribution                                                                                             |
| ------------------------------------- | --------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------- |
| ****Number of Trials****  | Single trial                                        | Multiple trials                                                                                                   |
| ****Possible Outcomes**** | 2 outcomes (1 for success, 0 for failure)           | Multiple outcomes (e.g., success or failure)                                                                      |
| ****Parameter****         | Probability of success is p                         | Probability of success in each trial is p and the number of trials is n                                           |
| ****Random Variable****   | X can only be 0 or 1                                | X can be any non-negative integer (0, 1, 2, 3, ...)                                                               |
| ****Purpose****           | Describes single trial events with success/failure. | Models the number of successes in multiple trials.                                                                |
| ****Example****           | Coin toss (Heads/Tails), Pass/Fail, Yes/No, etc.    | Counting the number of successful free throws in a series of attempts, number of defective items in a batch, etc. |

PMFs:

![1735310609676](image/note/1735310609676.png)

CDFs:

![1735312750410](image/note/1735312750410.png)

[ä¼¯åŠªåˆ©åˆ†ä½ˆç‰¹æ€§](https://www.geeksforgeeks.org/bernoulli-distribution-in-business-statistics-mean-and-variance/)

## Binomia

represent outcome of a single random 'trial', å®ƒæ˜¯ä¸€ç¨®é›¢æ•£åˆ†ä½ˆ

- $n$ is the number of trials ä¾‹å¦‚æ°‘æ„èª¿æŸ¥æ¨£æœ¬
- $0 \le K \le n$ is the number of successes æˆåŠŸæ¬¡æ•¸ ä¾‹å¦‚æ½›åœ¨é¸æ°‘è¡¨ç¤ºæœƒæŠ•ç¥¨çµ¦æˆ‘å€‘çš„å€™é¸äººçš„æ¬¡æ•¸
- $q$ is the (typically unknown) æ”¯æŒæˆ‘æ–¹å€™é¸äººçš„é¸æ°‘æ¯”ä¾‹

PMF:

$$


$$

![1735314063328](image/note/1735314063328.png)

CDF:

![1735314709953](image/note/1735314709953.png)

## Poison æ³Šé¬†åˆ†ä½ˆ

PMF:

$$
P(X = k) = {p_k} = {P_o}(k|\lambda ) = \frac{{{\lambda ^k}}}{{k!}}{e^{ - \lambda }}
$$

Mean:

$$
Mean(X)= \lambda
$$

Variance:

$$
Var(X)=\lambda
$$

PMF:

![1735311473403](image/note/1735311473403.png)

CMF:

![1735311555474](image/note/1735311555474.png)

## Betaï¼š

**most commonly used to represent uncertainty in probabilities or proportions** å¸¸ç”¨æ–¼è¡¨ç¤ºæ¦‚ç‡æˆ–æ¯”ä¾‹çš„ä¸ç¢ºå®šæ€§

Probability density function:

$$
f(x) = Beta(x|\alpha ,\beta ) = B_{\alpha ,\beta }^{ - 1}{x^{\alpha  - 1}}{(1 - x)^{\beta  - 1}}dx
$$

for  $ x \in [0,1] $ and positive  $\alpha ,\beta $

The factor $B_{\alpha ,\beta }^{ - 1}$  is a normalizing constant

Itâ€™s chosen so that  $P(0 â‰¤ x â‰¤ 1) = 1$ and there is no simple expression for it. Instead, itâ€™s defined as an integral:

$$
{B_{\alpha ,\beta }} = \int_0^1 {{x^{\alpha  - 1}}{{(1 - x)}^{\beta  - 1}}dx}
$$

The mean of the Beta distribution is:

$$
Mean(X)=\frac{\alpha }{{\alpha  + \beta }}
$$

Variance:

$$
Var(X)=\frac{{\alpha \beta }}{{{{(\alpha  + \beta )}^2}(1 + \alpha  + \beta )}}
$$

![1735314804579](image/note/1735314804579.png)

CDF

![1735315209527](image/note/1735315209527.png)

Application

![1735315241672](image/note/1735315241672.png)

## Gamma

is ofen used to model times between events

$$
f(x) = Gamma(x|\kappa ,\theta ) = \frac{1}{{\Gamma (\kappa ){\theta ^\kappa }}}{x^{\kappa  - 1}}{e^{ - x/\theta }}
$$

for positive $x$ and positive $\kappa$, $ \theta$

Mean:

$$
Mean(X)=\kappa \theta
$$

Variance

$$
Var(X)=\kappa \theta^2
$$

PDF

![1735315266727](image/note/1735315266727.png)

CDF

![1735315273039](image/note/1735315273039.png)

## Normal

The Normal **(also called the **Gaussiané«˜æ–¯**) distribution is continuous, with probability density function**

$$
f(x) = {\rm N}(x|\mu ,{\theta ^2}) = \frac{1}{{\sqrt {2\pi {\sigma ^2}} }}{e^{ - \frac{{{{(\chi  - \mu )}^2}}}{{2{\sigma ^2}}}}}
$$

Mean

$$
Mean(X)= \mu
$$

Variance

$$
Var(X)=\sigma^{2}
$$

### **Arithmetic with normally-distributed variables**

Suppose we have two random variables, **X**1 and **X**2 that are independent and are both normally distributed with means **Âµ**1 and **Âµ**2 **and variances **Ïƒ**12 **and **Ïƒ**2 2**, respectively.

#### $W=X_{1}+X_{2}$

will also be normally distributed

mean:

$$
{\mu_{W}}={\mu_{1}} + {\mu_{2}}
$$

variance:

$$
{\sigma^{2}_{W}}={\sigma^{2}_{1}}+{\sigma^{2}_{2}}
$$

#### $Y=aX_{1}+b$

will also be normally distributed

mean:

$$
\mu_{Y}=a\mu_{1}+b
$$

variance:

$$
\sigma^{2}_{Y}=a^{2}\sigma^{2}_{1}
$$

**PDF**

![1735315324363](image/note/1735315324363.png)

CDF

![1735315329536](image/note/1735315329536.png)

## **Cauchy æŸ¯è¥¿åˆ†ä½ˆ**

The Cauchy **distribution has probability density function**

$$
f(x) = \frac{1}{{\pi s(1 + {{((x - t)/s)}^2})}}
$$

$s$ is positive $t$ is parameter can be any parameters

**It has â€œheavy tailsâ€, which means that large values are so common that the Cauchy distribution lacks a well-defined mean and variance!**

But the parameter $t$ **gives the location of the mode and median, which are well-defined.**

The parameter $s$ **determines the â€˜widthâ€™ of the distribution as measured using e.g. the distances between percentiles, which are also well defined.**

PDF

![1735315340154](image/note/1735315340154.png)

CDF

![1735315356626](image/note/1735315356626.png)

# EDA: Exploratory data analysis

motivation:

    EDA is about getting an intuitive understanding of the data, and as such different people will find different techniques useful.

## Data quality

The first thing understand is where the data come from and how accurate they are. æ•¸æ“šçš„ä¾†æºå’Œæº–ç¢ºæ€§

### star rating æ˜Ÿç´šè©•ç´š

This is based on experience rather than any formal theoryï¼š

- 4ï¼š Numbers we can believe. Examples: official statistics(å®˜æ–¹çµ±è¨ˆæ•¸æ“š); well controlled laboratory experiments
- 3ï¼š Numbers that are reasonably accurate. Examples: well conducted surveys / samples; field measurements; less well controlled experiments.
- 2ï¼šNumbers that could be out by quite a long way. Examples: poorly conducted surveys / samples; measurements of very noisy systems
- 1ï¼š Numbers that are unreliable. Examples: highly biased / unrepresentative surveys / samples; measurements using biased / low-quality equipment
- 0ï¼š Numbers that have just been made up. Examples: urban legends / memes; fabricated experimental data

## Univariate Data Vectors

univariate case: one measurement per â€˜thingâ€™ æ¯å€‹è®Šé‡éƒ½å–®ç¨æ¢ç´¢

Mathematically, we represent a univariate dataset as a length-n vector:

$$
x = (x_{1},x_{2},...,x_{n})
$$

The sample mean of a function f (x) is

$$
\left\langle {{\rm{f}}(x)} \right\rangle  = \frac{1}{n}\sum\limits_{i = 1}^n {f({x_i}) = \frac{1}{n}[f({x_1}) + f({x_2}) + .... + f({x_n})]}
$$

## Visualisation and Information

There is an important distinction in visualisations between

- Lossless(ç„¡æ) ones from which, if viewed at sufficiently high resolution, one could recover the original dataset
- Lossy(æœ‰æ) ones, where a given plot would be consistent with many different raw datasets

Typically for complex data, choosing the lossy visualistaion that loses the â€˜rightâ€™ information is key to successful visualisation.

# Summary Statistics

## Measures of Central Tendency é›†ä¸­è¶¨å‹¢æ¸¬åº¦

Often, we are interested in what a typical value of the data;


- The mean of the data is:

$$
Mean(x)=\left\langle {\rm{x}} \right\rangle  = \frac{1}{n}\sum\limits_{i = 1}^n {{x_i}}
$$

- The median of the data is the value that sits in the middle when the data are sorted by value
- A mode in data is a value of $x$ that is â€˜more commonâ€™ than those around it, or a â€˜local maximumâ€™ in the density.
  - **For discrete data[ç¦»æ•£æ•°æ®] this can be *uniquely* determined as the most common value**
  - **For** **continuous data,** modes need to be ***estimated***, one aspect of a major strand in data science, estimating distributions.


### Visualising

For the data, we estimate from the kernel density that there is one mode, and its location and calculate the mean and median directly


> Example: 
>
> The data are **right-skewed(å³åçš„)**, and as a consequence of this the mode is smallest and the mean is largest â€“ we will consider this further; (note that for a normal distribution all would be equal.)


## Variance


|   ç‰¹æ€§   |           æœ‰åå·®æ–¹å·®           |        ç„¡åå·®æ–¹å·®        |
| :------: | :----------------------------: | :----------------------: |
|   åˆ†æ¯   |               n               |           n-1           |
| æ‡‰ç”¨å ´æ™¯ |        æè¿°æ¨£æœ¬çš„é›¢æ•£å‹        |      ä¼°è¨ˆç¸½é«”çš„æ–¹å·®      |
|   åå·®   |  å°ç¸½é«”æ–¹å·®çš„ä¼°è¨ˆå­˜åœ¨ä½ä¼°åå·®  | å°ç¸½é«”æ–¹å·®çš„ä¼°è¨ˆæ˜¯ç„¡åçš„ |
| æ‡‰ç”¨å ´æ™¯ | æ•¸æ“šåˆ†æã€æ©Ÿå™¨å­¸ç¿’ä¸­çš„æ¨£æœ¬å„ªåŒ– |   çµ±è¨ˆå­¸ä¸­ç¸½é«”æ–¹å·®ä¼°è¨ˆ   |

**ä½•æ—¶ä½¿ç”¨ï¼Ÿ**

* **æœ‰åå·®æ–¹å·®** ï¼šåœ¨æœºå™¨å­¦ä¹ ä¸­ï¼Œé€šå¸¸è®¡ç®—æ ·æœ¬çš„æœ‰åå·®æ–¹å·®ï¼ˆåˆ†æ¯ä¸º nn**n**ï¼‰ï¼Œå› ä¸ºé‡ç‚¹åœ¨äºä¼˜åŒ–æ¨¡å‹å¯¹æ ·æœ¬çš„é€‚é…æ€§ï¼Œè€Œéæ¨æ–­æ€»ä½“ã€‚
* **æ— åå·®æ–¹å·®** ï¼šåœ¨ç»Ÿè®¡å­¦å’Œæ¨æ–­ä¸­ï¼Œéœ€è¦ç”¨æ— åå·®æ–¹å·®ï¼ˆåˆ†æ¯ä¸º nâˆ’1n-1**n**âˆ’**1**ï¼‰ï¼Œå› ä¸ºå®ƒæ›´å‡†ç¡®åœ°ä¼°è®¡æ€»ä½“å‚æ•°ã€‚




$$
\begin{array}{ccccc}
Var(x) = \left\langle {{{(x - \left\langle x \right\rangle )}^2}} \right\rangle\\
= \frac{1}{n}\sum\limits_{i = 1}^n {{{({x_i} - \left\langle x \right\rangle )}^2}}\\
=\frac{1}{n}\sum\limits_{i = 1}^n {({x^2}_i - 2{x_i}\left\langle x \right\rangle  + {{\left\langle x \right\rangle }^2})}\\
=\left( {\frac{1}{n}\sum\limits_{i = 1}^n {x_i^2} } \right) + 2\left( {\frac{1}{n}\sum\limits_{i = 1}^n {{x_i}} } \right)\left\langle x \right\rangle + \frac{1}{n}\left( {\sum\limits_{i = 1}^n 1 } \right){\left\langle x \right\rangle ^2}\\
=\frac{1}{n}\left( {\sum\limits_{i = 1}^n {x_i^2} } \right) - {\left( {\frac{1}{n}\sum\limits_{i = 1}^n {{x_i}} } \right)^2}\\
=\left\langle {{x^2}} \right\rangle  - {\left\langle x \right\rangle ^2}
\end{array}
$$


### Unbiased Variance and Computation ç„¡åæ–¹å·®


$$
\begin{array}{ccccc}
\widehat {Var}(x)  = \frac{n}{{n - 1}}Var(x) \\
= \frac{1}{{n - 1}}\sum\limits_{i = 1}^n {{{({x_i} - \left\langle x \right\rangle )}^2}}\\
= \frac{1}{{n - 1}}\left( {\sum\limits_{i = 1}^n {x_i^2 - \frac{1}{n}{{\left( {\sum\limits_{i = 1}^n {{x_i}} } \right)}^2}} } \right)
\end{array}
$$


é»˜èªæƒ…æ³ä¸‹ï¼Œ pythonè¨ˆç®—æœ‰åå·®çš„ï¼ŒRè¨ˆç®—ç„¡åå·®çš„


[ç„¡åå·®æ¨£æœ¬](https://en.wikipedia.org/wiki/Variance#Population_variance_and_sample_variance)


## 'Natural' units

there are two commonly-used quantities that have the same units as the data èˆ‡æ•¸æ“šæœ‰ç›¸åŒå–®ä½

- mean $\mu = Mean(x)$
- standard deviation $\sigma  = \sqrt {Var(x)}$

These two quantities let us define two transformations commonly applied to data ç”¨æ–¼æ•¸æ“šè½‰æ›

- centring ${y_i} = {x_i} - \mu$     |   $Mean(y) = 0$
- standardisation ${z_i} = \frac{{{y_i}}}{\sigma }$  | $Var(z)=1$


## Higher moments

- In general, the $r$-th moment of the data is ç¬¬$r$æ™‚åˆ»çš„æ•¸æ“šæ˜¯ ${m_r} = \left\langle {{x^r}} \right\rangle$
- The $r$-th central momentä¸­å¿ƒè· of the data is   ${\mu _r} = \left\langle {{{(x - \mu )}^r}} \right\rangle  = \left\langle {{y^r}} \right\rangle$

    where the yâ€™s are the centred versions of the data.

- The $r$-th standardised moment of the data is ${\mu _r} = \left\langle {{{(\frac{{x - \mu }}{\sigma })}^r}} \right\rangle  = \left\langle {{z^r}} \right\rangle  = \frac{{\left\langle {{{\left( {x - \mu } \right)}^2}} \right\rangle }}{{{\sigma ^r}}} = \frac{{{\mu _r}}}{{{\sigma ^r}}}$

In theory, all higher moments are informative about the data, but in practice those with r = 3 and r = 4 are most commonly reported


### standardised moment

$$
{M_k} = \frac{{{\mu _k}}}{{{\sigma ^k}}}=\frac{{{åŸå§‹çŸ©}}}{{{æ¨™æº–å·®}}}
$$









# Application

## Linear regression

### **Ordinary Least-Squares (OLS) Regression** æ™®é€šæœ€å°äºŒä¹˜æ³•å›æ­¸

$$
P(y|x) = {\rm N}(x|\mu  = \alpha {\rm{x + }}\beta ,{\sigma ^2})
$$

where $\alpha$ , $\beta $ and $\sigma^{2}$ are parameters to-be-fit

![1735320045922](image/note/1735320045922.png)

an OLS model makes probabilitic predictions: the model syas that $y$ is drawn from a normal distribution whose mean depends on $x$ è©²æ¨¡å‹èªç‚ºyå–è‡ªå‡å€¼å–æ±ºæ–¼xçš„æ­£æ…‹åˆ†ä½ˆ

## Best  estimate æœ€ä½³ä¼°è¨ˆå€¼

$$
{p_A}=k/n
$$

# Packagesï¼š

## input data

### Python

```python
f = pd.read_csv('file_path')
```

### R

```r
f <- read.table("file_path", header=TRUE,sep=",")
```

## Data wrangling:

### Python

Pandas, Numpy

```python
import pandas as pd
import numpy as np
```

```python
x=f.mag.values
```



#### Summary Statistics

```
np.mean(x)
```


#### Variance

```python
#Baised
up.var(x)
#Unbiased
np.var(x,ddof=1)
```






### R

dplyr

```R
library(dplyr)
```

```r
x <- f$mag
```



#### Summary Statistics

```R
mean(x)
```


#### Variance

```R
#Unbiased
var(x)
#biased
library(momnets)
moment(x, order=2,central=TRUE)
```





## Visualisation

### Python

Seaborn(based on MatplotLib)

```python
import matplotlib.pyplot as plt
import seaborn as sns
```

```python
sns.histplot(magnitudes, stat='density')
sns.kdeplot(magnitudes)
sns.rugplot(magnitudes)
```

![1735488472345](image/note/1735488472345.png)









### R

GGPlot

```
library(ggplot2)
```

```R
ggplot(earthquake.df, aes(x=mag))+
geom_histogram(aes(y=..density..))+
geom_density()+ xlab("Magnitude")
```

![1735488562842](image/note/1735488562842.png)
