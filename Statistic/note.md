# statistical concept çµ±è¨ˆå­¸æ¦‚å¿µ

## PMF and CDF

PMFå®šç¾©çš„å€¼æ˜¯P(X=x)ï¼Œè€ŒCDFå®šç¾©çš„å€¼æ˜¯P(X <= x)ï¼Œxç‚ºæ‰€æœ‰çš„å¯¦æ•¸ç·šä¸Šçš„é»ã€‚

### probability mass function (PMF) æ¦‚ç‡è³ªé‡å‡½æ•¸

$$
pX(x)=P(X=x)
$$

æ˜¯é›¢æ•£éš¨æ©Ÿè®Šæ•¸åœ¨å„å€‹ç‰¹å®šå–å€¼ä¸Šçš„æ¦‚ç‡ã€‚æœ‰æ™‚ä¹Ÿè¢«ç¨±ç‚º**é›¢æ•£å¯†åº¦å‡½æ•¸**ã€‚

æ¦‚ç‡å¯†åº¦å‡½æ•¸é€šå¸¸æ˜¯*å®šç¾©é›¢æ•£éš¨æ©Ÿåˆ†ä½ˆçš„ä¸»è¦æ–¹æ³•*ï¼Œ

ä¸¦ä¸”æ­¤é¡å‡½æ•¸å­˜åœ¨æ–¼å…¶å®šç¾©åŸŸæ˜¯ï¼š

1. é›¢æ•£çš„ç´”é‡è®Šæ•¸
2. å¤šé éš¨æ©Ÿè®Šæ•¸

[ç¶­åŸºç™¾ç§‘](https://zh.wikipedia.org/zh-tw/%E6%A6%82%E7%8E%87%E8%B4%A8%E9%87%8F%E5%87%BD%E6%95%B0)

### Cumulative distribution function(CDF)ç´¯ç©åˆ†ä½ˆå‡½æ•¸

$$
FX(x)=P(X<=x)
$$

ä¹Ÿå«æ¦‚ç‡åˆ†ä½ˆå‡½æ•¸æˆ–åˆ†ä½ˆå‡½æ•¸

æ˜¯æ¦‚ç‡å¯†åº¦å‡½æ•¸çš„**ç©åˆ†**

èƒ½å¤ å®Œæ•´çš„æè¿°ä¸€å€‹å¯¦éš¨æ©Ÿè®Šæ•¸Xçš„æ¦‚ç‡åˆ†ä½ˆ

[ç¶­åŸºç™¾ç§‘](https://zh.wikipedia.org/zh-tw/%E7%B4%AF%E7%A7%AF%E5%88%86%E5%B8%83%E5%87%BD%E6%95%B0)

## probability density functionï¼ˆPDFï¼‰æ¦‚ç‡å¯†åº¦å‡½æ•¸

[PDF](https://www.geeksforgeeks.org/probability-density-function/)

[æ¦‚ç‡å¯†åº¦å‡½æ•¸ï¼ˆProbability Density Functionï¼Œ PDFï¼‰-CSDNåšå®¢](https://blog.csdn.net/YHKKun/article/details/137404224)

## Central Limits ä¸­å¤®ç•Œé™

Support we have a set of independent random variables $X_{i}$ for $i=1,....,n$ with:

$Mean(X_{i})=\mu$        $Var(X_{i})=V$       for all $i$

Then as $n$ becomes large, the sum:

$$
S_{m}=\sum\limits_{i = 1}^n {{X_i} \to {\rm N}(n\mu ,nV)}
$$

**tends to become normally distributed.**

### Absence of Central Limits

**Another case is where the moments are not defined / infinite å¦ä¸€ç¨®æƒ…æ³æ˜¯åŠ›çŸ©ä¸ç¢ºå®šæˆ–ç„¡é™å¤§**

# Randomness éš¨æ©Ÿæ€§

## Motivation å‹•æ©Ÿ

Three main ways that random comes into data science:

1. The data themselves are often best understood as random æ•¸æ“šæœ¬èº«é€šå¸¸æœ€å¥½è¢«ç†è§£ç‚ºéš¨æ©Ÿ
2. **When we want to reason under **subjective uncertainty **(for example in Bayesian approaches) then unknown quantities can be represented as random. Often when we make predictions they will be **probabilitic ç•¶æˆ‘å€‘ä¸»ç®¡ä¸ç¢ºå®šæ€§çš„æƒ…æ³ä¸‹é€²è¡Œæ¨ç†æ™‚ï¼Œå¯ä»¥å°‡æœªçŸ¥é‡è¡¨ç¤ºç‚ºéš¨æ©Ÿé‡ï¼Œç•¶æˆ‘å€‘é€²è¡Œé æ¸¬æ™‚ï¼Œä»–å€‘å°‡æ˜¯æ¦‚ç‡æ€§çš„
3. Many of the most effective / efficient / commonlyâ€‘used algorithms in data scienceâ€”typically called Monte Carlo algorithmsâ€”exploit randomness. è’™ç‰¹å¡æ´›ç®—æ³•

- Unpredictable ä¸å¯é æ¸¬æ€§
- Subjective uncertainty ä¸»ç®¡ä¸ç¢ºå®šæ€§

## The logistic map é‚è¼¯åœ°åœ–ï¼ˆå–®å³°æ˜ è±¡ï¼‰

æ˜¯ä¸€å€‹äºŒæ¬¡å¤šé …å¼æ˜ å°„(éæ­¸é—œä¿‚)ç»å¸¸ä½œä¸ºå…¸å‹èŒƒä¾‹æ¥è¯´æ˜å¤æ‚çš„æ··æ²Œç°è±¡æ˜¯å¦‚ä½•ä»éå¸¸ç®€å•çš„éçº¿æ€§åŠ¨åŠ›å­¦æ–¹ç¨‹ä¸­äº§ç”Ÿçš„ã€‚

is an example of deterministic chaos æ˜¯ç¢ºå®šæ€§æ··æ²Œçš„ä¸€å€‹ä¾‹å­ but whose results are apparently not easy to predict. çµæœä¸å®¹æ˜“è¢«é æ¸¬

å®ƒåœ¨ä¸€å®šç¨‹åº¦ä¸Šæ˜¯ä¸€ä¸ªæ—¶é—´ç¦»æ•£çš„äººå£ç»Ÿè®¡æ¨¡å‹

Logisticæ¨¡å‹å¯ä»¥æè¿°ç”Ÿç‰©ç¨®ç¾¤çš„æ¼”åŒ–ï¼Œå®ƒå¯ä»¥è¡¨ç¤ºæˆä¸€ç¶­éç·šæ€§è¿­ä»£æ–¹ç¨‹ $x_{n+1}=rx_{n}(1-x_{n})$

å…¶ä¸­ $ğ‘¥_{ğ‘›}$  è¡¨ç¤ºç§ç¾¤çš„ä¸ªæ•°ï¼Œç¬¬ä¸€é¡¹è¡¨ç¤ºç¬¬n+1ä»£ä¸ç¬¬nä»£æˆæ­£æ¯”ï¼Œç¬¬äºŒé¡¹è¡¨ç¤ºç¯å¢ƒå› ç´ å¯¹ç§ç¾¤ç¹æ®–æ•°ç›®çš„é™åˆ¶ã€‚å½“ç„¶æˆ‘ä»¬å·²ç»å‡å®š$ğ‘¥$ å¤§äºé›¶ä¸”å°äºä¸€,$ğ‘Ÿ$å¤§äºé›¶ä¸”å°äºå››ï¼ˆæ˜¾ç„¶ï¼‰ã€‚ä»–ä»¬ä¹‹é—´çš„å‡½æ•°å…³ç³»ä¸ºæŠ›ç‰©çº¿å‹ã€‚è¿™æ˜¯ä¸€ç§å…¸å‹çš„è´Ÿåé¦ˆæœºåˆ¶ï¼ˆå•å³°ï¼‰ï¼Œlogisticæ¨¡å‹æ˜¯äºŒæ¬¡å‡½æ•°å…³ç³»ï¼Œå½“ç„¶æˆ‘ä»¬è¿˜å¯ä»¥è®¾å®šä¸ºsinä¹‹ç±»çš„å‡½æ•°ã€‚å› ä¸ºå•å³°æ˜¯ä¸€ç§æœ€ç®€å•ï¼Œä¹Ÿæ˜¯è‡ªç„¶ç•Œæœ€å¸¸è§çš„ä¸€ç§è´Ÿåé¦ˆæœºåˆ¶

Math:

$$
\displaystyle{ x(t+1)=\mu x(t)(1-x(t)) }
$$

å…¶ä¸­ï¼Œtä¸ºè¿­ä»£æ—¶é—´æ­¥ï¼Œå¯¹äºä»»æ„çš„tï¼Œx(t)âˆˆ[0,1]ï¼ŒÎ¼ä¸ºä¸€å¯è°ƒå‚æ•°ï¼Œä¸ºäº†ä¿è¯æ˜ å°„å¾—åˆ°çš„x(t)å§‹ç»ˆä½äº[0,1]å†…ï¼Œåˆ™Î¼âˆˆ[0,4]ã€‚x(t)ä¸ºåœ¨tæ—¶åˆ»ç§ç¾¤å æœ€å¤§å¯èƒ½ç§ç¾¤è§„æ¨¡çš„æ¯”ä¾‹ï¼ˆå³ç°æœ‰äººå£æ•°ä¸æœ€å¤§å¯èƒ½äººå£æ•°çš„æ¯”ç‡ï¼‰ã€‚å½“å˜åŒ–ä¸åŒçš„å‚æ•°Î¼æ—¶ï¼Œè¯¥æ–¹ç¨‹ä¼šå±•ç°å‡ºä¸åŒçš„åŠ¨åŠ›å­¦æé™è¡Œä¸ºï¼ˆå³å½“tè¶‹äºæ— ç©·å¤§ï¼Œx(t)çš„å˜åŒ–æƒ…å†µï¼‰ï¼ŒåŒ…æ‹¬ï¼šç¨³å®šç‚¹ï¼ˆå³æœ€ç»ˆx(t)å§‹ç»ˆä¸ºåŒä¸€ä¸ªæ•°å€¼ï¼‰ã€å‘¨æœŸï¼ˆx(t)ä¼šåœ¨2ä¸ªæˆ–è€…å¤šä¸ªæ•°å€¼ä¹‹é—´è·³è·ƒ)ã€ä»¥åŠæ··æ²Œï¼ˆx(t)çš„ç»ˆæ€ä¸ä¼šé‡å¤ï¼Œè€Œä¼šç­‰æ¦‚ç‡åœ°å–éæŸåŒºé—´ï¼‰ã€‚

å½“Î¼è¶…è¿‡[1,4]æ—¶ï¼Œå°±ä¼šå‘ç”Ÿæ··æ²Œç°è±¡ã€‚è¯¥éçº¿æ€§å·®åˆ†æ–¹ç¨‹æ„åœ¨è§‚å¯Ÿä¸¤ç§æƒ…å½¢:
â€¢ å½“äººå£è§„æ¨¡å¾ˆå°æ—¶ï¼Œäººå£å°†ä»¥ä¸å½“å‰äººå£æˆæ¯”ä¾‹çš„é€Ÿåº¦å¢é•¿è¿›è¡Œç¹æ®–ã€‚
â€¢ é¥¥é¥¿(ä¸å¯†åº¦æœ‰å…³çš„æ­»äº¡ç‡) ï¼Œå…¶å¢é•¿ç‡å°†ä»¥ä¸ç¯å¢ƒçš„â€æ‰¿å—èƒ½åŠ›â€å‡å»å½“å‰äººå£æ‰€å¾—å€¼æˆæ­£æ¯”çš„é€Ÿåº¦ä¸‹é™

ç„¶è€Œï¼ŒLogisticæ˜ å°„ä½œä¸ºä¸€ç§äººå£ç»Ÿè®¡æ¨¡å‹ï¼Œå­˜åœ¨ç€ä¸€äº›åˆå§‹æ¡ä»¶å’Œå‚æ•°å€¼(å¦‚Î¼ >4)ä¸ºæŸå€¼æ—¶æ‰€å¯¼è‡´çš„æ··æ²Œé—®é¢˜ã€‚è¿™ä¸ªé—®é¢˜åœ¨è¾ƒè€çš„ç‘å…‹æ¨¡å‹ä¸­æ²¡æœ‰å‡ºç°ï¼Œè¯¥æ¨¡å‹ä¹Ÿå±•ç¤ºäº†æ··æ²ŒåŠ¨åŠ›å­¦ã€‚

$0<=Î¼<=1$:

![1735432764631](image/note/1735432764631.png)

$1<Î¼<3$:

![1735432855695](image/note/1735432855695.png)

ä»å›¾ä¸­å¾—çŸ¥ï¼š

å½“$Î¼$åœ¨1åˆ°2ä¹‹é—´ï¼Œç§ç¾¤æ•°é‡ä¼šå¾ˆå¿«æ¥è¿‘ $ \frac{Î¼âˆ’1}{Î¼}$ ,ä¸è®ºæœ€åˆç§ç¾¤ä¸ºä½•å€¼
å½“$Î¼$åœ¨2åˆ°3ä¹‹é—´ï¼Œäººå£æ•°åœ¨ç»å†ä¸€æ®µæ—¶é—´çš„æ³¢åŠ¨åä¼šè¶‹äºç¨³å®šå€¼ $\frac{Î¼âˆ’1}{Î¼}$,å…¶æ”¶æ•›é€Ÿåº¦æ»¡è¶³çº¿æ€§å˜åŒ–ã€‚ä½†å½“Î¼=3æ—¶ï¼Œæ¯”çº¿æ€§æ”¶æ•›è¿˜è¦ç¼“æ…¢ã€‚

äº‹å®ä¸Šï¼Œåªè¦Î¼ < 3 Î¼<3Î¼<3ï¼Œç³»ç»Ÿéƒ½ä¼šæ”¶æ•›åˆ°ä¸€ä¸ªä¸åŠ¨ç‚¹ï¼Œè€Œè¿™ä¸ªä¸åŠ¨ç‚¹çš„æ•°å€¼å¯ä»¥é€šè¿‡æ±‚è§£ä¸‹åˆ—æ–¹ç¨‹è€Œå¾—åˆ°ï¼š

$$
X^{*}=\mu x^{*}(1-x^{*})
$$

ä¸åŒå‚æ•° Î¼ \muÎ¼ä¸‹çš„æé™è¡Œä¸º
ä¸è®ºÎ¼å–ä»»æ„å€¼ï¼Œæœ€å¤šæœ‰ä¸€ä¸ªç¨³å®šçš„å‘¨æœŸã€‚ å¦‚æœå­˜åœ¨ä¸€ä¸ªç¨³å®šçš„å‘¨æœŸï¼Œé‚£ä¹ˆå®ƒæ˜¯å…¨å±€ç¨³å®šçš„ï¼Œå¸å¼•äº†å‡ ä¹æ‰€æœ‰çš„ç‚¹ã€‚ä¸€äº›å…·æœ‰å‘¨æœŸç¨³å®šå¾ªç¯çš„Î¼åœ¨æŸäº›æ—¶å€™ä¼šæœ‰æ— ç©·å¤šä¸ªä¸åŒå‘¨æœŸçš„ä¸ç¨³å®šå¾ªç¯ã€‚

ä¸ºäº†æ¦‚æ‹¬ä¸Šè¿°å„ç§æ•°å€¼æ¨¡æ‹Ÿè¯•éªŒçš„ç»“æœï¼Œæˆ‘ä»¬ç”¨ä¸‹é¢çš„ç›¸å›¾æ¥è¡¨ç¤ºä¸åŒÎ¼å€¼å¯¹åº”çš„æé™è¡Œä¸º

### Feigenbaumå›¾

![1735433138747](image/note/1735433138747.png)

### Code operation å¯¦ç¾æ“ä½œ

```python
# Define a function that does one step of the logistic map
def logisticMap(x,r):
    return r*x*(1.0-x)
```

```python
# Construct an orbit of the logistic map
nSteps = 100
t = range(0,(nSteps + 1))
r = 3.9
x = np.zeros(nSteps + 1) # initialise the result as a vector of zeroes å¾0é–‹å§‹
x[0] = 0.1 # Choose the initial condition
for i in range(1,(nSteps + 1)):
    x[i] = logisticMap(x[i-1],r)
```

```python
# Plot the results of the previous step
plt.plot(t, x, '-', linewidth=1,color='black')
plt.scatter( t, x, color="darkviolet" ) # add dots at the points
plt.title('Logistic Map: r=' + str(r))
plt.xlabel('Step, n')
plt.ylabel('x(n)')
plt.show()
```

![1735475233662](image/note/1735475233662.png)

[Logistic Mapping - Encyclopedia - Complex Systems|Artificial Intelligence|Complex Science|Complex Networks|Self-Organizing](https://wiki.swarma.org/index.php?title=Logistic%E6%98%A0%E5%B0%84#.5Bmath.5D.5Cdisplaystyle.7B_0.E2.89.A4.CE.BC.E2.89.A41_.7D.5B.2Fmath.5D)

[ç‰©æµåœ– - ç¶­çªç™¾ç§‘ï¼Œè‡ªç”±çš„ç™¾ç§‘å…¨æ›¸ --- Logistic map - Wikipedia](https://en.wikipedia.org/wiki/Logistic_map)

[Logicsticæ˜ å°„](https://blog.csdn.net/qq_41137110/article/details/115249684)

[Logisticæ˜ å°„ï¼ˆç¼–ç¨‹è®­ç»ƒï¼‰ - çŸ¥ä¹](https://zhuanlan.zhihu.com/p/578152437)

[Logistic.nb.pdf](https://abel.math.harvard.edu/archive/118r_spring_99/doc/proj/2/Logistic.nb.pdf)

## entropy ç†µ

å¦ä¸€ç§æ–¹æ³•æ˜¯åˆ©ç”¨è®¡ç®—æœºå¤–éƒ¨å› ç´ æ¥äº§ç”Ÿéšæœºæ€§ï¼Œ ä¾‹å¦‚é¼ æ ‡ç‚¹å‡»çš„ä½ç½®å’Œæ—¶é—´ã€‚ åœ¨æ­¤ï¼Œ æˆ‘ä»¬å°†è€ƒè™‘æŠŠä»£ç è¿è¡Œçš„æ—¶é—´ä½œä¸ºå¤–éƒ¨å› ç´ 

å³ä½¿ç”¨ç³»ç»Ÿæ—¶é’Ÿå½“å‰æ—¶é—´çš„å°æ•°ç‚¹åå…­ä½æ•°å­—ï¼ˆåˆ†è¾¨ç‡ä¸ºå¾®ç§’ï¼‰

Rå’ŒMatlab ä½¿ç”¨è»Ÿä»¶åŒ…æä¾›éš¨æ©Ÿæ•¸ç”Ÿæˆçš„å‡½æ•¸

## Estimation of $Ï€$ **using Monte Carlo methods**

å‡è¨­æˆ‘å€‘å°‡ $\pi$ å®šç¾©ç‚ºåŠå¾‘ç‚º1çš„åœ“çš„é¢ç©æ ¹æ“šå…¶å®šç¾©ä¾†ä¼°ç®—é€™å€‹æ•¸å­—ï¼Œ

we will pick random values of $x$ and $y$ independently from a uniform distribution between 0 and **1**, then let the random variable $Z$ equal 1 if the point $(x, y)$ falls within the quarter-circle shown and 0 otherwise. This $Z$ allows us to make an estimate of $Ï€$ **in that its expected value, $E[Z] = Ï€/4$**. We can then define a random variable **A**n to be the average of $n$ **independent samples of $Z$**. Formally:

$$
{{\rm{A}}_n} = \frac{1}{n}\sum\limits_{i = 1}^n {{Z_i} = \frac{\pi }{4} + {\varepsilon _n}}
$$

### Code operation

To deal with this, we'll repeat the experiment $m$ times and make a list of all the estimates

we get. We'll then arrange these results in ascending order and throw away a certain fraction $\alpha$ of the largest and smallest results. The remaining values should provide decent upper and lower bounds for an interval containing $\pi$.

```python
m = 100 # Number of estimates taken
n = 80000 # Number of points used in each estimate
```

If we increase $n$ above, we should get a more accurate estimates of $\pi$ each tme we run the experiment, while if we increase $m$, we'll get more accurate estimates of the endpoints of an interval containing $\pi$.

```python
#Generate a set of m estimates of the area of a unit-radius quarter-circle
np.random.seed(42) # Seed the random number generator
A = np.zeros(m) # A will hold our m estimates
for i in range(0,m):
    for j in range(0,n):
        # Generate an (x, y) pair in the unit square
        x = np.random.rand()
        y = np.random.rand()
  
        # Decide whether the point lies in or on
        # the unit circle and set Z accordingly
        r = x**2 + y**2
        if ( r <= 1.0):
            Z = 1.0
        else:
            Z = 0
  
        # Add up the contribution to the current estimate
        A[i] = A[i] + Z
   
    # Convert the sum we've built to an estimate of pi
    A[i] = 4.0 * A[i] / float( n )
```

```python
# Calculate approximate 95% confidence interval for pi based on our Monte Carlo estimates
pi_estimates = np.sort(A)
piLower = np.percentile(pi_estimates,2.5)
piUpper = np.percentile(pi_estimates,97.5)
print(f'We estimate that pi lies between {piLower:.3f} and {piUpper:.3f}.')
```

# standard distribution

## Bernoulli ä¼¯åŠªåˆ©åˆ†ä½ˆ

$$
P(X=x) = p^{x}(1-p)^{1-x}, x = 0, 1; 0 < p < 1
$$

only have two choices(binary situations). åªæœ‰å…©å€‹çµæœ ä¾‹å¦‚æˆåŠŸå¤±æ•— ç¡¬å¹£æ­£åé¢

****Random Variable (X):**** In the context of Bernoulli Distribution, X represents the variable that can take the values 1 or 0, denoting the number of successes occurring.

****Bernoulli Trial:**** An individual experiment or trial with only two possible outcomes.

****Bernoulli Parameter:**** This refers to the probability of success (p) in a Bernoulli Distribution.

Mean:

$$
E[X] = Î¼ = p
$$

Variance:

$$
Var[X] = E[X^{2}] - (E[X])^2
      \\ =Ïƒ2 = p(1 - p) \ or\  pq
$$

###### **Applications of Bernoulli Distribution in Business Statistics**

****1. Quality Control:**** In manufacturing, every product undergoes quality checks. Bernoulli Distribution helps assess whether a product passes (success) or fails (failure) the quality standards. By analysing the probability of success, manufacturers can evaluate the overall quality of their production process and make improvements.

****2. Market Research:**** Bernoulli Distribution is useful in surveys and market research when dealing with yes/no questions. ****For instance,**** when surveying customer satisfaction, responses are often categorised as satisfied (success) or dissatisfied (failure). Analysing these binary outcomes using Bernoulli Distribution helps companies gauge customer sentiment.

****3. Risk Assessment:**** In the context of risk management, the Bernoulli Distribution can be applied to model events with binary outcomes, such as a financial investment succeeding (success) or failing (failure). The probability of success serves as a key parameter for assessing the risk associated with specific investments or decisions.

****4. Marketing Campaigns:**** Businesses use Bernoulli Distribution to measure the effectiveness of marketing campaigns. ****For instance,**** in email marketing, success might represent a recipient opening an email, while failure indicates not opening it. Analysing these binary responses helps refine marketing strategies and improve campaign success rates.

###### Difference between Bernoulli Distribution and Binomial Distribution ä¼¯åŠªåˆ©åˆ†ä½ˆå’ŒäºŒé …åˆ†ä½ˆçš„å€åˆ†

The Bernoulli Distribution and the Binomial Distribution are both used to model random experiments with *binary outcomes*, but they differ in how they handle multiple trials or repetitions of these experiments. åŒæ¨£æ˜¯å°å…·æœ‰äºŒå…ƒçµæœçš„éš¨æ©Ÿå¯¦é©—é€²è¡Œå»ºæ¨¡ï¼Œä½†åœ¨è™•ç†å¤šæ¬¡å¯¦é©—çš„æ–¹å¼ä¸Šæœ‰æ‰€ä¸åŒ

| Basis                                 | Bernoulli Distribution                              | Binomial Distribution                                                                                             |
| ------------------------------------- | --------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------- |
| ****Number of Trials****  | Single trial                                        | Multiple trials                                                                                                   |
| ****Possible Outcomes**** | 2 outcomes (1 for success, 0 for failure)           | Multiple outcomes (e.g., success or failure)                                                                      |
| ****Parameter****         | Probability of success is p                         | Probability of success in each trial is p and the number of trials is n                                           |
| ****Random Variable****   | X can only be 0 or 1                                | X can be any non-negative integer (0, 1, 2, 3, ...)                                                               |
| ****Purpose****           | Describes single trial events with success/failure. | Models the number of successes in multiple trials.                                                                |
| ****Example****           | Coin toss (Heads/Tails), Pass/Fail, Yes/No, etc.    | Counting the number of successful free throws in a series of attempts, number of defective items in a batch, etc. |

PMFs:

![1735310609676](image/note/1735310609676.png)

CDFs:

![1735312750410](image/note/1735312750410.png)

[ä¼¯åŠªåˆ©åˆ†ä½ˆç‰¹æ€§](https://www.geeksforgeeks.org/bernoulli-distribution-in-business-statistics-mean-and-variance/)

## Binomia

represent outcome of a single random 'trial', å®ƒæ˜¯ä¸€ç¨®é›¢æ•£åˆ†ä½ˆ

- $n$ is the number of trials ä¾‹å¦‚æ°‘æ„èª¿æŸ¥æ¨£æœ¬
- $0 \le K \le n$ is the number of successes æˆåŠŸæ¬¡æ•¸ ä¾‹å¦‚æ½›åœ¨é¸æ°‘è¡¨ç¤ºæœƒæŠ•ç¥¨çµ¦æˆ‘å€‘çš„å€™é¸äººçš„æ¬¡æ•¸
- $q$ is the (typically unknown) æ”¯æŒæˆ‘æ–¹å€™é¸äººçš„é¸æ°‘æ¯”ä¾‹

PMF:

$$


$$

![1735314063328](image/note/1735314063328.png)

CDF:

![1735314709953](image/note/1735314709953.png)

## Poison æ³Šé¬†åˆ†ä½ˆ

PMF:

$$
P(X = k) = {p_k} = {P_o}(k|\lambda ) = \frac{{{\lambda ^k}}}{{k!}}{e^{ - \lambda }}
$$

Mean:

$$
Mean(X)= \lambda
$$

Variance:

$$
Var(X)=\lambda
$$

PMF:

![1735311473403](image/note/1735311473403.png)

CMF:

![1735311555474](image/note/1735311555474.png)

## Betaï¼š

**most commonly used to represent uncertainty in probabilities or proportions** å¸¸ç”¨æ–¼è¡¨ç¤ºæ¦‚ç‡æˆ–æ¯”ä¾‹çš„ä¸ç¢ºå®šæ€§

Probability density function:

$$
f(x) = Beta(x|\alpha ,\beta ) = B_{\alpha ,\beta }^{ - 1}{x^{\alpha  - 1}}{(1 - x)^{\beta  - 1}}dx
$$

for  $ x \in [0,1] $ and positive  $\alpha ,\beta $

The factor $B_{\alpha ,\beta }^{ - 1}$  is a normalizing constant

Itâ€™s chosen so that  $P(0 â‰¤ x â‰¤ 1) = 1$ and there is no simple expression for it. Instead, itâ€™s defined as an integral:

$$
{B_{\alpha ,\beta }} = \int_0^1 {{x^{\alpha  - 1}}{{(1 - x)}^{\beta  - 1}}dx}
$$

The mean of the Beta distribution is:

$$
Mean(X)=\frac{\alpha }{{\alpha  + \beta }}
$$

Variance:

$$
Var(X)=\frac{{\alpha \beta }}{{{{(\alpha  + \beta )}^2}(1 + \alpha  + \beta )}}
$$

![1735314804579](image/note/1735314804579.png)

CDF

![1735315209527](image/note/1735315209527.png)

Application

![1735315241672](image/note/1735315241672.png)

## Gamma

is ofen used to model times between events

$$
f(x) = Gamma(x|\kappa ,\theta ) = \frac{1}{{\Gamma (\kappa ){\theta ^\kappa }}}{x^{\kappa  - 1}}{e^{ - x/\theta }}
$$

for positive $x$ and positive $\kappa$, $ \theta$

Mean:

$$
Mean(X)=\kappa \theta
$$

Variance

$$
Var(X)=\kappa \theta^2
$$

PDF

![1735315266727](image/note/1735315266727.png)

CDF

![1735315273039](image/note/1735315273039.png)

## Normal

The Normal **(also called the **Gaussiané«˜æ–¯**) distribution is continuous, with probability density function**

$$
f(x) = {\rm N}(x|\mu ,{\theta ^2}) = \frac{1}{{\sqrt {2\pi {\sigma ^2}} }}{e^{ - \frac{{{{(\chi  - \mu )}^2}}}{{2{\sigma ^2}}}}}
$$

Mean

$$
Mean(X)= \mu
$$

Variance

$$
Var(X)=\sigma^{2}
$$

### **Arithmetic with normally-distributed variables**

Suppose we have two random variables, **X**1 and **X**2 that are independent and are both normally distributed with means **Âµ**1 and **Âµ**2 **and variances **Ïƒ**12 **and **Ïƒ**2 2**, respectively.

#### $W=X_{1}+X_{2}$

will also be normally distributed

mean:

$$
{\mu_{W}}={\mu_{1}} + {\mu_{2}}
$$

variance:

$$
{\sigma^{2}_{W}}={\sigma^{2}_{1}}+{\sigma^{2}_{2}}
$$

#### $Y=aX_{1}+b$

will also be normally distributed

mean:

$$
\mu_{Y}=a\mu_{1}+b
$$

variance:

$$
\sigma^{2}_{Y}=a^{2}\sigma^{2}_{1}
$$

**PDF**

![1735315324363](image/note/1735315324363.png)

CDF

![1735315329536](image/note/1735315329536.png)

## **Cauchy æŸ¯è¥¿åˆ†ä½ˆ**

The Cauchy **distribution has probability density function**

$$
f(x) = \frac{1}{{\pi s(1 + {{((x - t)/s)}^2})}}
$$

$s$ is positive $t$ is parameter can be any parameters

**It has â€œheavy tailsâ€, which means that large values are so common that the Cauchy distribution lacks a well-defined mean and variance!**

But the parameter $t$ **gives the location of the mode and median, which are well-defined.**

The parameter $s$ **determines the â€˜widthâ€™ of the distribution as measured using e.g. the distances between percentiles, which are also well defined.**

PDF

![1735315340154](image/note/1735315340154.png)

CDF

![1735315356626](image/note/1735315356626.png)

# EDA: Exploratory data analysis

motivation:

    EDA is about getting an intuitive understanding of the data, and as such different people will find different techniques useful.

## Data quality

The first thing understand is where the data come from and how accurate they are. æ•¸æ“šçš„ä¾†æºå’Œæº–ç¢ºæ€§

### star rating æ˜Ÿç´šè©•ç´š

This is based on experience rather than any formal theoryï¼š

- 4ï¼š Numbers we can believe. Examples: official statistics(å®˜æ–¹çµ±è¨ˆæ•¸æ“š); well controlled laboratory experiments
- 3ï¼š Numbers that are reasonably accurate. Examples: well conducted surveys / samples; field measurements; less well controlled experiments.
- 2ï¼šNumbers that could be out by quite a long way. Examples: poorly conducted surveys / samples; measurements of very noisy systems
- 1ï¼š Numbers that are unreliable. Examples: highly biased / unrepresentative surveys / samples; measurements using biased / low-quality equipment
- 0ï¼š Numbers that have just been made up. Examples: urban legends / memes; fabricated experimental data

## Univariate Data Vectors

univariate case: one measurement per â€˜thingâ€™ æ¯å€‹è®Šé‡éƒ½å–®ç¨æ¢ç´¢

Mathematically, we represent a univariate dataset as a length-n vector:

$$
x = (x_{1},x_{2},...,x_{n})
$$

The sample mean of a function f (x) is

$$
\left\langle {{\rm{f}}(x)} \right\rangle  = \frac{1}{n}\sum\limits_{i = 1}^n {f({x_i}) = \frac{1}{n}[f({x_1}) + f({x_2}) + .... + f({x_n})]}
$$

## Visualisation and Information

There is an important distinction in visualisations between

- Lossless(ç„¡æ) ones from which, if viewed at sufficiently high resolution, one could recover the original dataset
- Lossy(æœ‰æ) ones, where a given plot would be consistent with many different raw datasets

Typically for complex data, choosing the lossy visualistaion that loses the â€˜rightâ€™ information is key to successful visualisation.

## Multivariate Exploratory Data Analysis

- In real applications, we almost almost always have multiple features of different things measured, and are so in a multivariate rather than univariate situation

### Professional Skill

#### Data types

- Nominal or categorical (e.g. colours, car names): not ordered; cannot be added or compared; can be relabelled.
- Ordinal (e.g. small/medium/large): sometimes represented by numbers; can be ordered, but differences or ratios are not meaningful.
- Measurement: meaningful numbers, on which (some) operations make sense. They can be:
  - Discrete (e.g. publication year, number of cylinders): typically integer.
  - Continuous (e.g. height): precision limited only by measurement accuracy.

Measurements can be in an interval scale (e.g. temperature in degrees Celsius), ratio scale (say, weights in kg), or circular scale (time of day on the 24 hr clock), depending on the 0 value and on which operations yield meaningful results

#### Sample Mean

$$
\left\langle x \right\rangle : = \frac{1}{n}\sum\limits_{i = 1}^n {{x_i}}
$$

Note: We are thinking of the n samples as coming from a population of size $N  >> n$, so that each xi is a copy of a random vector of length-p, X, which has its own mean $E[X]$. In particular, $E[X] = E[ã€ˆxã€‰]$, but generally $E[X] \ne <x>$

|   ç‰¹æ€§   |                  Sample Mean æ¨£æœ¬å‡å€¼                  |               Mean ç¸½é«”å‡å€¼               |
| :------: | :-----------------------------------------------------: | :---------------------------------------: |
| æ•¸æ“šä¾†æº |                 å¾ç¸½é«”ä¸­æŠ½å–çš„æ¨£æœ¬æ•¸æ“š                 |        ç¸½é«”æ•¸æ“š(æ‰€æœ‰å¯èƒ½å¾—æ•¸æ“šé»)        |
|   å¤§å°   | æ ·æœ¬å¤§å° nn**n** é€šå¸¸è¿œå°äºæ€»ä½“å¤§å° NN**N** | æ€»ä½“å¤§å° NN**N** é€šå¸¸å¾ˆå¤§ï¼Œç”šè‡³æ— é™ |
| å¯è®¡ç®—æ€§ |                      å¯ä»¥ç›´æ¥è®¡ç®—                      |         é€šå¸¸éœ€è¦é€šè¿‡æ ·æœ¬å‡å€¼ä¼°è®¡         |
|  ç¨³å®šæ€§  |                   ä¼šå› æ ·æœ¬ä¸åŒè€Œæ³¢åŠ¨                   |             ç†è®ºå€¼ï¼Œå›ºå®šä¸å˜             |
|   ç”¨é€”   |               ç”¨äºä¼°è®¡æ€»ä½“å‡å€¼æˆ–ç»Ÿè®¡æ¨æ–­               |          æè¿°æ€»ä½“çš„çœŸå®ä¸­å¿ƒè¶‹åŠ¿          |

#### Sample Covariance Matrix æ¨£æœ¬å”æ–¹å·®çŸ©é™£

$$
S=[S_{ab}],  where \ \ \ \  S_{ab}= \frac{1}{n}\sum\limits_{i = 1}^n {({x_{ia}} - \left\langle {{x_a}} \right\rangle )({x_{ib}} - \left\langle {{x_b}} \right\rangle )}
$$

Where $a = 1,...,p$ and $b=1,...,p$

ç”¨äºæè¿°å¤šç»´æ•°æ®é›†ä¸­æ¯ä¸ªå˜é‡ä¹‹é—´çš„ **åæ–¹å·®** ï¼ˆå³å˜é‡é—´çš„çº¿æ€§å…³ç³»ï¼‰ä»¥åŠæ¯ä¸ªå˜é‡çš„æ–¹å·®ï¼ˆè‡ªç›¸å…³æ€§ï¼‰ã€‚å®ƒæ˜¯å¤šå…ƒç»Ÿè®¡åˆ†æä¸­çš„ä¸€ä¸ªé‡è¦å·¥å…·ã€‚

**ç”¨é€”**

1. **æ•°æ®åˆ†æ** ï¼š

* æ¢ç´¢å˜é‡ä¹‹é—´çš„çº¿æ€§å…³ç³»ï¼ˆæ­£ç›¸å…³ã€è´Ÿç›¸å…³æˆ–æ— å…³ï¼‰ã€‚
* ç¡®å®šå˜é‡çš„ç¦»æ•£ç¨‹åº¦ï¼ˆæ–¹å·®ï¼‰ã€‚

2. **ä¸»æˆåˆ†åˆ†æï¼ˆPCAï¼‰** ï¼š

* æ ·æœ¬åæ–¹å·®çŸ©é˜µç”¨äºè®¡ç®—ä¸»æˆåˆ†çš„æ–¹å‘å’Œå¤§å°ã€‚

3. **æœºå™¨å­¦ä¹ ** ï¼š

* åœ¨é™ç»´ã€ç‰¹å¾é€‰æ‹©å’Œæ•°æ®æ ‡å‡†åŒ–ç­‰ä»»åŠ¡ä¸­èµ·åˆ°é‡è¦ä½œç”¨ã€‚

4. **æ¦‚ç‡å»ºæ¨¡** ï¼š

* åœ¨å¤šå…ƒæ­£æ€åˆ†å¸ƒä¸­ï¼Œåæ–¹å·®çŸ©é˜µç”¨äºæè¿°å˜é‡é—´çš„ç›¸å…³æ€§ã€‚

#### Correlation Matrix

The sample correlation matrix has elements that correspond to the slopes of the (linear) regression lines between variables

$$
R=[R_{ab}], where \ \ \ R_{ab} = \frac{{{S_{ab}}}}{{\sqrt {{S_{aa}}{S_{bb}}} }}
$$

**å…³çŸ©é˜µ**æ˜¯ä¸€ä¸ªçŸ©é˜µï¼Œç”¨æ¥è¡¨ç¤ºå¤šä¸ªå˜é‡ä¹‹é—´çš„ **ç›¸å…³æ€§ç³»æ•°** ï¼ˆé€šå¸¸æ˜¯çš®å°”é€Šç›¸å…³ç³»æ•°ï¼‰ã€‚å®ƒæ˜¯æ ·æœ¬åæ–¹å·®çŸ©é˜µçš„æ ‡å‡†åŒ–å½¢å¼ï¼Œç”¨äºæ¯”è¾ƒå˜é‡ä¹‹é—´çš„çº¿æ€§å…³ç³»ã€‚

| ç‰¹å¾               | Covariance Maztrix                     | Correlation Matrix             |
| ------------------ | -------------------------------------- | ------------------------------ |
| å–®ä½               | ä¿ç•™å˜é‡çš„åŸå§‹å•ä½ï¼ˆå¦‚ç±³ã€ç§’ç­‰)        | æ— å•ä½ï¼ˆæ ‡å‡†åŒ–å½¢å¼ï¼‰           |
| ç¯„åœ               | å€¼çš„èŒƒå›´ä¾èµ–äºå˜é‡çš„å•ä½å’Œå¤§å°         | å–å€¼èŒƒå›´ä¸º [âˆ’1,1]             |
| å¯¹æ¯”ä¸åŒå˜é‡çš„å…³ç³» | å˜é‡ä¹‹é—´çš„å…³ç³»å¯èƒ½å› å°ºåº¦ä¸åŒè€Œä¸æ˜“æ¯”è¾ƒ | æ ‡å‡†åŒ–åæ˜“äºæ¯”è¾ƒå˜é‡ä¹‹é—´çš„å…³ç³» |

**è®¡ç®—ç›¸å…³çŸ©é˜µçš„æ­¥éª¤**

1. **å»ä¸­å¿ƒåŒ–** ï¼šä»æ¯ä¸ªå˜é‡ä¸­å‡å»å…¶å‡å€¼ã€‚
2. **æ ‡å‡†åŒ–** ï¼šå°†æ¯ä¸ªå˜é‡é™¤ä»¥å…¶æ ‡å‡†å·®ã€‚
3. **è®¡ç®—ç›¸å…³æ€§** ï¼šå¯¹äºæ¯ä¸€å¯¹å˜é‡ï¼Œè®¡ç®—ç›¸å…³ç³»æ•° $r_{jk}$

**ç”¨é€”**

1. **æ¢ç´¢å˜é‡å…³ç³»** ï¼š

* åˆ¤æ–­å“ªäº›å˜é‡æ­£ç›¸å…³ã€è´Ÿç›¸å…³æˆ–ä¸ç›¸å…³ã€‚
* ç”¨äºæ•°æ®æ¸…ç†ï¼Œç§»é™¤é«˜åº¦ç›¸å…³çš„å˜é‡ä»¥é¿å…å¤šé‡å…±çº¿æ€§ã€‚

1. **æœºå™¨å­¦ä¹ ** ï¼š

* ç‰¹å¾é€‰æ‹©ï¼šé€šè¿‡æŸ¥çœ‹ç›¸å…³çŸ©é˜µï¼Œå»é™¤å†—ä½™å˜é‡ã€‚
* é™ç»´ï¼šåœ¨ä¸»æˆåˆ†åˆ†æï¼ˆPCAï¼‰ä¸­ï¼Œç›¸å…³çŸ©é˜µå¯ä»¥ç”¨äºæ„é€ ä¸»æˆåˆ†ã€‚

1. **é‡‘èåˆ†æ** ï¼š

* åˆ†æèµ„äº§ä¹‹é—´çš„ç›¸å…³æ€§ï¼Œç”¨äºä¼˜åŒ–æŠ•èµ„ç»„åˆã€‚

1. **ç§‘å­¦ç ”ç©¶** ï¼š

* åœ¨å®éªŒæ•°æ®ä¸­åˆ†æå˜é‡çš„å…³ç³»ã€‚

https://en.wikibooks.org/wiki/Statistics/Different_Types_of_Data


### Visualisation


#### Bivariate visualisation techniques: 



##### Scatter Plots

A scatter plot is a lossless visualisation that involves placing a marker at $(x_{ia}, x_{ib})$ for each $i$ and some $a, b$

```python
plt.scatter(dis, wgt)
```

```R
ggplot(auto.data, aes(x=Displacement.y=Weight))+geom_point()
```

![1735962631926](image/note/1735962631926.png)


##### 2d Histograms

A 2d histogram generalised the univariate in the natural way as the count of data points falling inside a given two-dimensional area.

```python
plt.hist2d(dis,wgt,cmap="Blues")
```

```r
ggplot(auto.data, aes(x=Displacement.y=Weight))+geom_bin2d()
```

![1735962760725](image/note/1735962760725.png)


#### Higher dimensions



##### 3d Scatter

![1735963209105](image/note/1735963209105.png)


##### Scaled Scatter

![1735963250792](image/note/1735963250792.png)

##### Plot Matrices

![1735963281644](image/note/1735963281644.png)


### Pairs of categorical variables

Contingency tables

![1735963333231](image/note/1735963333231.png)

- Can be used for categorical, ordinal and discrete variables, with more than two levels
- Can write values as proportions of each row or each column
- Can write them as proportions of the total
- Can compare them with what the values would be if the two variables were independent (values would be the products of respective marginals)


### Categorical and continuous variables

![1735963429705](image/note/1735963429705.png)


## Multivariate Distributions


### Random vectors

- For two length-p vectors $u$ and $v$, we write $u â‰¤ v$ only if $u_{i} â‰¤ v_{i}$ for each $i$ from 1 to p.
- A random variable can be defined through its cumulative distribution function; we can do the same for a random vector $X$       $F(x) = P(X â‰¤ x)$
- We then define the probability density function for continuous variables through  $F(b)-F(a)= \int\limits_{{x_1} = {a_1}}^{{{\rm{b}}_1}}  \cdots  \int\limits_{{x_p} = {a_p}}^{{b_p}} {f(x)dx{}_1 \cdots d{x_p}}$


### Expectations and the Multivariate Normal


#### Expectations

$$
{\rm E}\left[ {g(X)} \right] = \int\limits_{{x_1} = {a_1}}^{{{\rm{b}}_1}}  \cdots  \int\limits_{{x_p} = {a_p}}^{{b_p}} {g(x)f(x)dx{}_1 \cdots d{x_p}}
$$

- multivariate normal: $f(x) = \frac{1}{(2\pi)^{p/2}(\text{det}(\Sigma))^{1/2}} \exp\left(-\frac{1}{2}(x-\mu)^T \Sigma^{-1} (x-\mu)\right) \equiv \mathcal{N}(x|\mu,\Sigma).$
- Note that the generalisation of variance is a covariance(æ–¹å·®çš„æ³›åŒ–æ˜¯å”æ–¹å·®), with $cov(X)$ being a matrix with $(a, b)$-th element $E[X_{a}X_{b}] âˆ’ E[X_{a}]E[X_{b}]$. For the multivariate normalå¤šå…ƒæ­£æ…‹åˆ†ä½ˆ, the covariance matrix is $Î£$.

#### Multivariate normal contours å¤šå…ƒæ­£æ…‹è¼ªå»“

- The multivariate normal distribution defines a surface that has ellipsoidal contours centred on the mean
- Consider a multivariate normal with $\mu = \begin{pmatrix} 1 \\ 1 \end{pmatrix}, \quad \Sigma = \begin{pmatrix} 1 & 0.75 \\ 0.75 & 2 \end{pmatrix}$


#### Multivariate normal as a surface

![1735965459238](image/note/1735965459238.png)

The probability of a region in the plane is given by the volume of the region beneath a surface, rather than the area beneath a curve.


#### Multivariate normal contours

![1735965495752](image/note/1735965495752.png)

plus samples:

![1735965512125](image/note/1735965512125.png)


#### Centering

- When we centre data we subtract the sample mean from each row
- One natural transform on data is to subtract the mean from each row
- This can be achieved through use of a centering matrix $H = \left( I - \frac{1}{n} \mathbf{1} \mathbf{1}^T \right)$
- Then a transformed data matrix: $Y=HX$  will have mean $<y>=0$

##### Centred auto data

![1735965857460](image/note/1735965857460.png)



#### Standardisation

$$
D_{ab} = \begin{cases} 
S_{aa}^{-1/2} & \text{if } a = b, \\
0 & \text{otherwise.}
\end{cases}
$$

- transformed data matrix $Z=HXD=YD$   

will have mean $ã€ˆzã€‰ = 0 $ and variance-covariance matrix equal to Xâ€™s correlation matrix

![1735965990946](image/note/1735965990946.png)


#### The Mahalanobis Transform for Data

- The matrix version of the square root is called the Cholesky decomposition $S = C^T C.$
- We can use this to remove the correlations in data through the **Mahalanobis transform**: $U=Y(C^{-1})$
- This yields a dataset with no correlations between variables, making comparison with a multivariate normal easier.

axes now show combinations of variables, rather than the original variables.

![1735966284598](image/note/1735966284598.png)


![1735966302670](image/note/1735966302670.png)


#### Spotting Deviations from Normality


- We saw that we can observe deviations from univariate normality with summary statistics such as skewness, kurtosis or through the detection pf multiple modes
- These measures also generalise to the multivariate case.
- Here we show three cases where the multivariate distribution is far from multivariate normal, even if the marginal (i.e. univariate) datasets are very close to (or even indistinguishable from) univariate normal
- We will show these using a visualisation technique known as a joint plot, where a bivariate visualisation is shown in the main panel and univariate visualisations are shown along the axes. These also show the correlation and associated p-value



### Multivariate multimodality

The data may not be centred around a unique mode. This is possible in complex ways in the multivariate case that can be far from obvious from lower dimensional analysis

![1735966536208](image/note/1735966536208.png)


### Outliers

Some datapoints may be very far from the mode. As well as strongly affecting estimates of means and variances, this can affect estimates of correlations between variables in the multivariate case

![1735966651192](image/note/1735966651192.png)


### Restricted Support

The data may only take positive values, or only values in some restricted region of space. For the multivariate case, such restrictions can be more complex than the univariate case

![1735966715880](image/note/1735966715880.png)






# Summary Statistics

## Measures of Central Tendency é›†ä¸­è¶¨å‹¢æ¸¬åº¦

Often, we are interested in what a typical value of the data;

- The mean of the data is:

$$
Mean(x)=\left\langle {\rm{x}} \right\rangle  = \frac{1}{n}\sum\limits_{i = 1}^n {{x_i}}
$$

- The median of the data is the value that sits in the middle when the data are sorted by value
- A mode in data is a value of $x$ that is â€˜more commonâ€™ than those around it, or a â€˜local maximumâ€™ in the density.
  - **For discrete data[ç¦»æ•£æ•°æ®] this can be *uniquely* determined as the most common value**
  - **For** **continuous data,** modes need to be ***estimated***, one aspect of a major strand in data science, estimating distributions.

### Visualising

For the data, we estimate from the kernel density that there is one mode, and its location and calculate the mean and median directly

> Example:
>
> The data are **right-skewed(å³åçš„)**, and as a consequence of this the mode is smallest and the mean is largest â€“ we will consider this further; (note that for a normal distribution all would be equal.)

## Variance

|   ç‰¹æ€§   |           æœ‰åå·®æ–¹å·®           |        ç„¡åå·®æ–¹å·®        |
| :------: | :----------------------------: | :----------------------: |
|   åˆ†æ¯   |               n               |           n-1           |
| æ‡‰ç”¨å ´æ™¯ |        æè¿°æ¨£æœ¬çš„é›¢æ•£å‹        |      ä¼°è¨ˆç¸½é«”çš„æ–¹å·®      |
|   åå·®   |  å°ç¸½é«”æ–¹å·®çš„ä¼°è¨ˆå­˜åœ¨ä½ä¼°åå·®  | å°ç¸½é«”æ–¹å·®çš„ä¼°è¨ˆæ˜¯ç„¡åçš„ |
| æ‡‰ç”¨å ´æ™¯ | æ•¸æ“šåˆ†æã€æ©Ÿå™¨å­¸ç¿’ä¸­çš„æ¨£æœ¬å„ªåŒ– |   çµ±è¨ˆå­¸ä¸­ç¸½é«”æ–¹å·®ä¼°è¨ˆ   |

**ä½•æ—¶ä½¿ç”¨ï¼Ÿ**

* **æœ‰åå·®æ–¹å·®** ï¼šåœ¨æœºå™¨å­¦ä¹ ä¸­ï¼Œé€šå¸¸è®¡ç®—æ ·æœ¬çš„æœ‰åå·®æ–¹å·®ï¼ˆåˆ†æ¯ä¸º nn**n**ï¼‰ï¼Œå› ä¸ºé‡ç‚¹åœ¨äºä¼˜åŒ–æ¨¡å‹å¯¹æ ·æœ¬çš„é€‚é…æ€§ï¼Œè€Œéæ¨æ–­æ€»ä½“ã€‚
* **æ— åå·®æ–¹å·®** ï¼šåœ¨ç»Ÿè®¡å­¦å’Œæ¨æ–­ä¸­ï¼Œéœ€è¦ç”¨æ— åå·®æ–¹å·®ï¼ˆåˆ†æ¯ä¸º nâˆ’1n-1**n**âˆ’**1**ï¼‰ï¼Œå› ä¸ºå®ƒæ›´å‡†ç¡®åœ°ä¼°è®¡æ€»ä½“å‚æ•°ã€‚

$$
\begin{array}{ccccc}
Var(x) = \left\langle {{{(x - \left\langle x \right\rangle )}^2}} \right\rangle\\
= \frac{1}{n}\sum\limits_{i = 1}^n {{{({x_i} - \left\langle x \right\rangle )}^2}}\\
=\frac{1}{n}\sum\limits_{i = 1}^n {({x^2}_i - 2{x_i}\left\langle x \right\rangle  + {{\left\langle x \right\rangle }^2})}\\
=\left( {\frac{1}{n}\sum\limits_{i = 1}^n {x_i^2} } \right) + 2\left( {\frac{1}{n}\sum\limits_{i = 1}^n {{x_i}} } \right)\left\langle x \right\rangle + \frac{1}{n}\left( {\sum\limits_{i = 1}^n 1 } \right){\left\langle x \right\rangle ^2}\\
=\frac{1}{n}\left( {\sum\limits_{i = 1}^n {x_i^2} } \right) - {\left( {\frac{1}{n}\sum\limits_{i = 1}^n {{x_i}} } \right)^2}\\
=\left\langle {{x^2}} \right\rangle  - {\left\langle x \right\rangle ^2}
\end{array}
$$

### Unbiased Variance and Computation ç„¡åæ–¹å·®

$$
\begin{array}{ccccc}
\widehat {Var}(x)  = \frac{n}{{n - 1}}Var(x) \\
= \frac{1}{{n - 1}}\sum\limits_{i = 1}^n {{{({x_i} - \left\langle x \right\rangle )}^2}}\\
= \frac{1}{{n - 1}}\left( {\sum\limits_{i = 1}^n {x_i^2 - \frac{1}{n}{{\left( {\sum\limits_{i = 1}^n {{x_i}} } \right)}^2}} } \right)
\end{array}
$$

é»˜èªæƒ…æ³ä¸‹ï¼Œ pythonè¨ˆç®—æœ‰åå·®çš„ï¼ŒRè¨ˆç®—ç„¡åå·®çš„

[ç„¡åå·®æ¨£æœ¬](https://en.wikipedia.org/wiki/Variance#Population_variance_and_sample_variance)

## 'Natural' units

there are two commonly-used quantities that have the same units as the data èˆ‡æ•¸æ“šæœ‰ç›¸åŒå–®ä½

- mean $\mu = Mean(x)$
- standard deviation $\sigma  = \sqrt {Var(x)}$

These two quantities let us define two transformations commonly applied to data ç”¨æ–¼æ•¸æ“šè½‰æ›

- centring ${y_i} = {x_i} - \mu$     |   $Mean(y) = 0$
- standardisation ${z_i} = \frac{{{y_i}}}{\sigma }$  | $Var(z)=1$

## Higher moments

- In general, the $r$-th moment of the data is ç¬¬$r$æ™‚åˆ»çš„æ•¸æ“šæ˜¯ ${m_r} = \left\langle {{x^r}} \right\rangle$
- The $r$-th central momentä¸­å¿ƒè· of the data is   ${\mu _r} = \left\langle {{{(x - \mu )}^r}} \right\rangle  = \left\langle {{y^r}} \right\rangle$

  where the yâ€™s are the centred versions of the data.
- The $r$-th standardised moment of the data is ${\mu _r} = \left\langle {{{(\frac{{x - \mu }}{\sigma })}^r}} \right\rangle  = \left\langle {{z^r}} \right\rangle  = \frac{{\left\langle {{{\left( {x - \mu } \right)}^2}} \right\rangle }}{{{\sigma ^r}}} = \frac{{{\mu _r}}}{{{\sigma ^r}}}$

In theory, all higher moments are informative about the data, but in practice those with r = 3 and r = 4 are most commonly reported

### standardised moment

$$
{M_k} = \frac{{{\mu _k}}}{{{\sigma ^k}}}=\frac{{{åŸå§‹çŸ©}}}{{{æ¨™æº–å·®}}}
$$

* $M_k$ï¼šç¬¬ $k$é˜¶æ ‡å‡†åŒ–çŸ©ã€‚
* $\mu_k$ï¼šç¬¬ $k$ é˜¶åŸå§‹çŸ©ã€‚
* $\sigma$ï¼šæ ‡å‡†å·®

æ ‡å‡†åŒ–çŸ©é€šè¿‡é™¤ä»¥æ ‡å‡†å·®çš„ $k$ æ¬¡æ–¹ï¼Œä½¿çŸ©çš„é‡çº²æ¶ˆå¤±ï¼Œæ–¹ä¾¿åˆ†å¸ƒçš„æ¯”è¾ƒ

#### ç¬¬ä¸€é˜¶æ ‡å‡†åŒ–çŸ©

$$
{M_1} = \frac{{{\mu _1}}}{{{\sigma ^1}}}
$$

è¡¨ç¤ºåˆ†å¸ƒçš„ä¸­å¿ƒä½ç½®ï¼Œä½†é€šå¸¸ä¸º 0ï¼ˆå¦‚æœä¸­å¿ƒç‚¹é€‰å‡å€¼ï¼‰

#### ç¬¬äºŒé˜¶æ ‡å‡†åŒ–çŸ©

$$
{M_2} = \frac{{{\mu _2}}}{{{\sigma ^2}}}
$$

æ’ç­‰äº 1ï¼Œå› ä¸ºåˆ†å¸ƒå·²ç»ç”¨æ ‡å‡†å·®æ ‡å‡†åŒ–ã€‚

#### ç¬¬ä¸‰é˜¶æ ‡å‡†åŒ–çŸ©ï¼ˆååº¦ï¼ŒSkewnessï¼‰

$$
{M_3} = \frac{{{\mu _3}}}{{{\sigma ^3}}}=\widetilde {{\mu _3}} = Skew(x)
$$

- ç”¨äºæè¿°åˆ†å¸ƒçš„å¯¹ç§°æ€§æˆ–åæ–œç¨‹åº¦

  - ${{\rm{M}}_3} > 0$: åˆ†ä½ˆ åå³(å³å°¾è¼ƒé•·)
  - ${{\rm{M}}_3} < 0$: åˆ†ä½ˆåå·¦(å·¦å°¾è¼ƒé•·)
  - ${{\rm{M}}_3} = 0$: åˆ†ä½ˆå°ç¨±
- A larger (more **positive**) value of this quantity indicates **right-skewness**, meaning that more of the dataâ€™s variability **arises** *from* **values of x larger than the mean**
- Conversely, a smaller (more **negative**) value of this quantity indicates **left-skewness**, meaning that more of the dataâ€™s variability arises from values of x smaller than the mean.
- A value close to **zero** means that the variability of the data is similar either side of the mean (*but does not imply an overall symmetric distribution*).

#### ç¬¬å››é˜¶æ ‡å‡†åŒ–çŸ©ï¼ˆå³°åº¦ï¼ŒKurtosisï¼‰

$$
{M_4} = \frac{{{\mu _4}}}{{{\sigma ^4}}}
$$

- ç”¨äºæè¿°åˆ†å¸ƒçš„å°–å³°æˆ–å¹³å¦ç¨‹åº¦.
  - ${{\rm{M}}_4} > 3$: å°–å³°åˆ†ä½ˆ
  - ${{\rm{M}}_4} < 3$: å¹³å¦åˆ†ä½ˆ

**ç”¨é€”**

* **æè¿°åˆ†å¸ƒå½¢çŠ¶** ï¼šååº¦å’Œå³°åº¦æ˜¯æœ€å¸¸ç”¨çš„æ ‡å‡†åŒ–çŸ©ï¼Œç”¨äºç ”ç©¶æ•°æ®åˆ†å¸ƒçš„å¯¹ç§°æ€§å’Œå°¾éƒ¨ç‰¹æ€§ã€‚
* **æ¨¡å‹å‡è®¾æ£€éªŒ** ï¼šä¾‹å¦‚ï¼Œåˆ¤æ–­æ•°æ®æ˜¯å¦ç¬¦åˆæ­£æ€åˆ†å¸ƒã€‚
* **åˆ†å¸ƒæ¯”è¾ƒ** ï¼šé€šè¿‡æ ‡å‡†åŒ–ï¼Œæ¶ˆé™¤äº†å°ºåº¦å’Œå•ä½çš„å½±å“ï¼Œå¯ä»¥ç›´æ¥æ¯”è¾ƒä¸åŒæ•°æ®é›†çš„å½¢çŠ¶ç‰¹å¾ã€‚

- A value of this quantity larger than 3 means that more of the variance of the data arises from the tails than would be expected if it were normally distributed
- A value of this quantity less than 3 means that less of the variance of the data arises from the tails than would be expected if it were normally distributed.
- A value close to 3 is consistent with, though not strong evidence for, a normal distribution.
- The difference between the kurtosis and 3 is called the excess kurtosis.

# functions

## indicator functions

The indicator function of a logical proposition A

![1735860418049](image/note/1735860418049.png)

![1735860436648](image/note/1735860436648.png)

## empirical cumulative distribution function ï¼ˆECDFï¼‰

![1735860506891](image/note/1735860506891.png)

## Quantiles and Order Statistics

- The z-th percentile, $P_z$ is the value of x for which z% of the data is â‰¤ x
- So the median is median(x) = $P_{50}$
- This is related to the ECDF as illustrated below
- A measure of dispersal of the data is the inter-quartile range $IQR(x) = {P_{75}} - {P_{25}}$

![1735860679914](image/note/1735860679914.png)

# Density Estimation

## Histograms

histogram can be used to make an estimate of the probability density underlying a data set. Given data{ ${ {x_1}, . . . , {x_n} }$} and a collection of q + 1 bin-boundaries,$b = (b_0, b_1, . . . , b_q )$

chosen so that ${b_0} < min(x) \  and \ max(x) < {b_q}$ , we can think of the histogram-based density estimate as a piecewise-constant (that is, constant on intervals) function arranged so that the value of the estimator in the interval $b_{aâˆ’1} â‰¤ x < b_{a}$ is

$$
f(x|b) = \frac{1}{{{b_a} - {b_{a - 1}}}}\left( {\frac{{\left| {\{ {x_j}|{b_{a - 1}} \le {x_j} < {b_a}\} } \right|}}{n}} \right)
$$

where the second factor is the proportion of the ${x_j}$ that fall into the interval and $b_a âˆ’ b_{aâˆ’1}$ is the width of the interval. These choices mean that the bar (of the histogram) above the interval has an area equal to the proportion of the data points $x_j$ that fall in that interval

## Estimating a Density with Kernels

$$
\widehat f(x|w) = \frac{1}{n}\sum\limits_{j = 1}^n {\frac{1}{w}K\left( {\frac{{x - {x_j}}}{w}} \right)}
$$

The main players in this formula are

$K(x)$: the kernel, typically some bump-shaped function such as a Gaussian or a parabolic bump. It should be normalised in the sense that

$$
\int_{ - \infty }^\infty  {K(x)\ dx = 1}
$$

$w$ : the bandwidth, which sets the width of the bumps

### Kernel Density Estimation ï¼ˆKDEï¼‰

æ˜¯ä¸€ç§ **éå‚æ•°æ–¹æ³•** ï¼Œç”¨äºä¼°è®¡éšæœºå˜é‡çš„æ¦‚ç‡å¯†åº¦å‡½æ•°ï¼ˆPDFï¼ŒProbability Density Functionï¼‰ã€‚å®ƒæä¾›äº†ä¸€ç§å¹³æ»‘æ–¹å¼æ¥æè¿°æ•°æ®çš„åˆ†å¸ƒï¼Œä¸ä¾èµ–ç‰¹å®šçš„åˆ†å¸ƒå‡è®¾ï¼ˆå¦‚æ­£æ€åˆ†å¸ƒï¼‰

 **ç›®æ ‡** ï¼š

* KDE çš„ç›®æ ‡æ˜¯ä»æœ‰é™çš„æ ·æœ¬æ•°æ®ä¸­ä¼°è®¡å…¶èƒŒåçš„æ¦‚ç‡å¯†åº¦å‡½æ•°ã€‚
* ä¸ç›´æ–¹å›¾ç±»ä¼¼ï¼ŒKDE æè¿°äº†æ•°æ®çš„åˆ†å¸ƒï¼Œä½†æ¯”ç›´æ–¹å›¾æ›´å¹³æ»‘ä¸”ä¸å—ç‰¹å®šåŒºé—´ï¼ˆbinï¼‰çš„å½±å“ã€‚

 **æ ¸å¿ƒå…¬å¼** ï¼š
ç»™å®š $n$ ä¸ªæ•°æ®ç‚¹$\{x_1, x_2, \dots, x_n\}$ï¼ŒKDE åœ¨ä½ç½® $x$ å¤„çš„ä¼°è®¡å€¼ä¸ºï¼š

$$
f^(x)=1nhâˆ‘i=1nK(xâˆ’xih)\hat{f}(x) = \frac{1}{n h} \sum_{i=1}^{n} K\left(\frac{x - x_i}{h}\right)ï¼šåœ¨ x å¤„çš„å¯†åº¦ä¼°è®¡ã€‚
$$

* $K(\cdot)$ï¼š **æ ¸å‡½æ•°** ï¼ˆKernel Functionï¼‰ï¼Œå®šä¹‰å¦‚ä½•åˆ†å¸ƒå¹³æ»‘æƒé‡ã€‚
* $h$ï¼š **å¸¦å®½å‚æ•°** ï¼ˆBandwidthï¼‰ï¼Œæ§åˆ¶å¹³æ»‘çš„ç¨‹åº¦ã€‚
* $x_i$ï¼šæ•°æ®ç‚¹ã€‚

 **æ ¸å‡½æ•° $K(\cdot)$** ï¼š

* æ ¸å‡½æ•°æ˜¯ä¸€ä¸ªå¯¹ç§°çš„éè´Ÿå‡½æ•°ï¼Œå…¶ç§¯åˆ†ä¸º 1ï¼Œé€šå¸¸ç”¨æ¥ä¸ºæ¯ä¸ªç‚¹åˆ†é…æƒé‡ã€‚
* å¸¸è§æ ¸å‡½æ•°ï¼š
  * é«˜æ–¯æ ¸ï¼ˆGaussian Kernelï¼‰ï¼š$K(u)=12Ï€eâˆ’u22K(u) = \frac{1}{\sqrt{2\pi}} e^{-\frac{u^2}{2}}$
  * å‡åŒ€æ ¸ï¼ˆUniform Kernelï¼‰ï¼š$K(u)=12K(u) = \frac{1}{2}$ï¼ˆå¦‚æœ $âˆ£uâˆ£â‰¤1|u| \leq 1$ï¼Œå¦åˆ™ä¸º 0ï¼‰
  * ä¸‰è§’æ ¸ï¼ˆTriangular Kernelï¼‰ï¼š$K(u)=1âˆ’âˆ£uâˆ£K(u) = 1 - |u|$ï¼ˆå¦‚æœ $âˆ£uâˆ£â‰¤1|u| \leq 1$ï¼Œå¦åˆ™ä¸º 0ï¼‰

![1735898585468](image/note/1735898585468.png)

 **å¸¦å®½** $h$ï¼š

* å¸¦å®½æ§åˆ¶äº†æ ¸çš„æ‰©å±•èŒƒå›´ã€‚
* $h$ çš„é€‰æ‹©éå¸¸é‡è¦ï¼š

  * $h$ å¤ªå°ï¼šä¼°è®¡å‡½æ•°ä¼šè¿‡äºæ³¢åŠ¨ï¼ˆè¿‡æ‹Ÿåˆï¼‰ã€‚
  * $h$ å¤ªå¤§ï¼šä¼°è®¡å‡½æ•°ä¼šè¿‡äºå¹³æ»‘ï¼ˆæ¬ æ‹Ÿåˆï¼‰ã€‚
* KDE çš„æ ¸å¿ƒæ€æƒ³æ˜¯ç”¨æ ¸å‡½æ•° $K(\cdot)$å¹³æ»‘åœ°â€œè¦†ç›–â€æ¯ä¸ªæ•°æ®ç‚¹ã€‚
* é€šè¿‡å°†æ ¸å‡½æ•°ä¸­å¿ƒæ”¾åœ¨æ¯ä¸ªæ•°æ®ç‚¹ä¸Šï¼Œå¹¶æ ¹æ®å¸¦å®½ $h$ è°ƒæ•´å®½åº¦ï¼Œæœ€ç»ˆç”Ÿæˆä¸€ä¸ªè¿ç»­çš„æ¦‚ç‡å¯†åº¦æ›²çº¿

KDEä¸ç›´æ–¹å›¾çš„æ¯”è¾ƒ

|  ç‰¹é»  |              ç›´æ–¹åœ–              |                KDE                |
| :----: | :-------------------------------: | :--------------------------------: |
|  å€é–“  | æ•°æ®è¢«åˆ’åˆ†æˆå›ºå®šå®½åº¦çš„åŒºé—´ï¼ˆbinï¼‰ |           ä¸éœ€è¦å›ºå®šåŒºé—´           |
| å¹³æ»‘æ€§ |      æ›²çº¿å¯èƒ½ä¸è¿ç»­ï¼Œæœ‰æ£±è§’      |           æ›²çº¿è¿ç»­ã€å¹³æ»‘           |
|  åƒæ•¸  |       åŒºé—´å®½åº¦ï¼ˆbin widthï¼‰       | æ ¸å‡½æ•°å’Œå¸¦å®½ï¼ˆkernel + bandwidthï¼‰ |
| éˆæ´»æ€§ |          å¯¹åŒºé—´ä½ç½®æ•æ„Ÿ          |     æ›´çµæ´»ï¼Œé€‚ç”¨äºå¤æ‚æ•°æ®åˆ†å¸ƒ     |

**åº”ç”¨åœºæ™¯**

1. **æ•°æ®åˆ†å¸ƒå¯è§†åŒ–** ï¼šå¦‚è§‚å¯Ÿæ•°æ®çš„é›†ä¸­è¶‹åŠ¿å’Œåˆ†å¸ƒå½¢æ€ã€‚
2. **å¼‚å¸¸æ£€æµ‹** ï¼šè¯†åˆ«ä¸ç¬¦åˆå¯†åº¦åˆ†å¸ƒçš„æ•°æ®ç‚¹ã€‚
3. **æ¦‚ç‡å¯†åº¦ä¼°è®¡** ï¼šç”¨äºæœºå™¨å­¦ä¹ å’Œç»Ÿè®¡å»ºæ¨¡ä¸­çš„ç‰¹å¾åˆ†å¸ƒå»ºæ¨¡ã€‚

#### KDR's stacking pillows

![1735885738016](image/note/1735885738016.png)

#### Convergence æ”¶æ–‚

The bandwidth w is a free parameter and there are various approaches to choosing its value. For parabolic and
Gaussian kernels it is usually chosen so that  $w \propto \frac{1}{{{N^{\frac{1}{5}}}}}$

![1735898559909](image/note/1735898559909.png)



### Multivariate KDE

The kernel density estimate (KDE) approximates the population distribution function (as before) and is defined by


$$
\hat f(x|\theta ) = \frac{1}{n}\sum\limits_{j = 1}^n {K\left( {x|{x_i},\theta } \right)}
$$

Typically the kernel function K will be chosen to be the multivariate normal probability density function

$$
K\left( {x|{x_i},\theta } \right) = {\rm N}(x|{x_i},\sigma )
$$

A 2d kernel density plot shows estimated curves of constant f (x).


#### Bivariate visualisation techniques: 2d KDE

```python
sns.kdeplot(dis,wgt,cmap="Blues")
```

```r
ggplot(auto.data, aes(x=Displacement.y=Weight))+geom_density_2d()
```

![1735963141779](image/note/1735963141779.png)










## Multimodality å¤šæ…‹

- For continuous data, there arenâ€™t typically identical observations (and if there are, they arenâ€™t typical) so we will need to estimate the modes, which we define as local maxima (peaks) of the probability density
  function.
- The location and number of modes is typically the most relevant measure of central tendency and variability for multimodal data
- We will see that different estimation procedures give different modes, and even for simulated data like the below that is â€˜obviouslyâ€™ bimodal, they will give different answers about mode locations.

![1735860845967](image/note/1735860845967.png)

![1735897539968](image/note/1735897539968.png)

But different kernels produce different estimatesâ€”in general, there is no â€˜rightâ€™ answer for this kind of unsupervised learning problem

![1735899540126](image/note/1735899540126.png)

Often we should weight the datapoints for various reasonsâ€”i.e. assign a wi to each datapoint such that


# Estimator

An **estimator** is a **mathematical rule**, function, or formula used to approximate an *unknown population parameter* (such as the mean, variance, or proportion) based on sample data. In statistical analysis, estimators are essential because they provide insights about a population when it is impractical or impossible to measure the entire population directly.



## Key Concepts of Estimators


1. Population vs. Sampleï¼š

- **Population Parameter** ($\theta$): A fixed but unknown characteristic of a population, like the true mean ($\mu$) or variance ($\sigma^2$). ç¸½é«”çš„å›ºå®šä½†æœªçŸ¥çš„ç‰¹å¾
- Esimatorä¼°è¨ˆå™¨($\hat{\theta}$): A statistic (function of sample data) used to estimate the **population parameter**. ç”¨æ–¼ä¼°è¨ˆç¸½é«”åƒæ•¸çš„çµ±è¨ˆé‡

2. **Notation** :ç¬¦å·ï¼š

- The true population parameter is denoted by $\theta$.  çœŸå¯¦ç¸½é«”åƒæ•¸
- The estimator (based on the sample) is denoted by $\hat{\theta}$. åŸºæ–¼æ¨£æœ¬çš„ä¼°è¨ˆé‡
  - Population mean: $\mu$.
  - Sample mean(estimator): $\bar{x}$.  


## Type of Estimators


1. Point Estimator: é»ä¼°è¨ˆå™¨

- Provides a single value estimate of a population parameter. æä¾›æ€»ä½“å‚æ•°çš„**å•å€¼**ä¼°è®¡ã€‚
- Example: The sample mean ($\bar{x}$) is a point estimator of the **population mean** ($\mu$).

2. **Interval Estimator** : å€é–“ä¼°è¨ˆå™¨

- Provides a **range of values** within which the population parameter is likely to lie. æä¾›æ€»ä½“å‚æ•°å¯èƒ½ä½äº**çš„å€¼èŒƒå›´**
  - Example: Confidence intervals for the mean. å¹³å‡å€¼çš„ç½®ä¿¡åŒºé—´


## Properties of a Good Estimator


1. Unbiasednessï¼š

- An estimator is **unbiased** if its expected value equals the true parameter: $E[\hat{Î¸}]=Î¸$  å¦‚æœä¼°è®¡å™¨çš„æœŸæœ›å€¼ç­‰äºçœŸå®å‚æ•°ï¼Œåˆ™è¯¥ä¼°è®¡å™¨æ˜¯**æ— åçš„**
  - Example: The sample mean ($\bar{x}$) is an unbiased estimator of the population mean ($\mu$)

2. Consistency:

- An estimator is **consistent** if it converges to the true parameter value as the sample size increases  $\hat{\theta} \xrightarrow{n \to \infty} \theta$  å¦‚æœéšç€æ ·æœ¬é‡çš„å¢åŠ ï¼Œä¼°è®¡é‡æ”¶æ•›åˆ°çœŸå®å‚æ•°å€¼ï¼Œåˆ™è¯¥ä¼°è®¡é‡æ˜¯**ä¸€è‡´çš„**

3. Efficiency:

- An estimator is **efficient** if it has the smallest possible variance among all unbiased estimators.  å¦‚æœä¸€ä¸ªä¼°è®¡é‡åœ¨æ‰€æœ‰æ— åä¼°è®¡é‡ä¸­å…·æœ‰æœ€å°çš„å¯èƒ½æ–¹å·®ï¼Œåˆ™è¯¥ä¼°è®¡é‡æ˜¯**æœ‰æ•ˆçš„**

4. Sufficiency å……è¶³æ€§

- An estimator is **sufficient** if it captures all the information in the sample relevant to the parameter being estimated. å¦‚æœä¼°è®¡å™¨æ•è·äº†æ ·æœ¬ä¸­ä¸æ‰€ä¼°è®¡çš„å‚æ•°ç›¸å…³çš„æ‰€æœ‰ä¿¡æ¯ï¼Œåˆ™è¯¥ä¼°è®¡å™¨å°±**è¶³å¤Ÿäº†**


## Examples of Estimators


1. Sample Mean:

- Estimator for the population mean ($\mu$) : æ€»ä½“å¹³å‡å€¼çš„ä¼°è®¡é‡
  $$
  \hat{\mu} = \bar{x} = \frac{1}{n} \sum_{i=1}^{n} x_i
  $$

2. Sample Variance

- Biased estimator

  $$
  s^2 = \frac{1}{n} \sum_{i=1}^{n} (x_i - \bar{x})^2
  $$
- Unbiased estimator:

$$
\hat{\sigma}^2 = \frac{1}{n-1} \sum_{i=1}^{n} (x_i - \bar{x})^2
$$

3. Sample Proportion:

- Estimator for the population proportion ($p$): äººå£æ¯”ä¾‹çš„ä¼°è®¡é‡

$$
\hat{p} = \frac{x}{n}
$$

Where $x$ is the number of successes and $n$ is the sample size.


## Evaluation of Estimators


1. **Mean Squared Error (MSE)** :å‡æ–¹è¯¯å·®

- Combines both bias and variance to evaluate an estimator: ç»“åˆåå·®å’Œæ–¹å·®æ¥è¯„ä¼°ä¼°è®¡é‡

$$
\text{MSE}(\hat{\theta}) = \text{Var}(\hat{\theta}) + \left[\text{Bias}(\hat{\theta})\right]^2
$$

- Lower MSE indicates a better estimator  è¾ƒä½çš„ MSE è¡¨ç¤ºæ›´å¥½çš„ä¼°è®¡é‡


2. Bias-Variance Tradeoff åå·®æ–¹å·®æ¬Šè¡¡

- A tradeoff between minimizing bias and variance. In some cases, a slightly biased estimator with lower variance might be preferred (e.g., Ridge Regression).


## Estimators in Statistical Inference ç»Ÿè®¡æ¨æ–­ä¸­çš„ä¼°è®¡å™¨

Estimators are widely used in:ä¼°è®¡å™¨å¹¿æ³›ç”¨äº

1. Parameter Estimation

- Estimating population parameters (e.g., mean, variance, correlation).ä¼°è®¡æ€»ä½“å‚æ•°ï¼ˆä¾‹å¦‚å¹³å‡å€¼ã€æ–¹å·®ã€ç›¸å…³æ€§ï¼‰

2. **Hypothesis Testing** :å‡è®¾æª¢é©—

- Using estimators to calculate test statistics.

3. **Machine Learning** :æœºå™¨å­¦ä¹ ï¼š

- Estimating model parameters to minimize loss functions.ä¼°è®¡æ¨¡å‹å‚æ•°ä»¥æœ€å°åŒ–æŸå¤±å‡½æ•°ã€‚


## summary

An estimator is a tool for inferring population parameters from sample data. Its quality is determined by properties such as unbiasedness, consistency, and efficiency. Choosing or constructing a good estimator is central to statistical inference, enabling accurate and reliable conclusions about the population.


## biased estimator 


For an estimator to be **biased** means that, on average, it systematically **underestimates or overestimates** the true value of the parameter it is trying to estimate. In other words, the **expected value** of the estimator ($E[\hat{\theta}]$) is not equal to the true parameter value ($\theta$). 


The bias of an estimator $\hat{\theta}$ is defined mathematically as

$$
\text{Bias}(\hat{\theta}) = E[\hat{\theta}] - \theta
$$

Where:

- $E[\hat{\theta}]$ : The expected value of the estimator.
- $\theta$ : The true value of the parameter being estimated. è¢«ä¼°è®¡å‚æ•°çš„çœŸå®å€¼
- If $\text{Bias}(\hat{\theta}) = 0$ The estimator is  **unbiased** .
- if $\text{Bias}(\hat{\theta}) \neq 0$: The estimator is biased.



### **Why Does Bias Matter?ä¸ºä»€ä¹ˆåè§å¾ˆé‡è¦ï¼Ÿ**

1. **Implications of Bias** :

* A biased estimator does not consistently provide accurate estimates of the true parameter, even with large sample sizes.å³ä½¿æ ·æœ¬é‡å¾ˆå¤§ï¼Œæœ‰åä¼°è®¡é‡ä¹Ÿä¸èƒ½å§‹ç»ˆå¦‚ä¸€åœ°æä¾›çœŸå®å‚æ•°çš„å‡†ç¡®ä¼°è®¡ã€‚
* This can lead to systematic errors in decision-making or modeling.è¿™å¯èƒ½ä¼šå¯¼è‡´å†³ç­–æˆ–å»ºæ¨¡ä¸­å‡ºç°ç³»ç»Ÿæ€§é”™è¯¯ã€‚

1. **Trade-Off with Variance** :ä¸æ–¹å·®çš„æƒè¡¡ï¼š

* In some cases, a slightly biased estimator may be preferred if it reduces variance (e.g., ridge regression).åœ¨æŸäº›æƒ…å†µä¸‹ï¼Œå¦‚æœå¯ä»¥å‡å°‘æ–¹å·®ï¼ˆä¾‹å¦‚å²­å›å½’ï¼‰ï¼Œåˆ™å¯èƒ½ä¼šé¦–é€‰ç¨å¾®æœ‰åå·®çš„ä¼°è®¡é‡ã€‚

1. **Correcting Bias** :

* Bias can sometimes be removed or corrected through adjustments (e.g., Besselâ€™s correction for sample variance).æœ‰æ—¶å¯ä»¥é€šè¿‡è°ƒæ•´ï¼ˆä¾‹å¦‚ï¼Œæ ·æœ¬æ–¹å·®çš„è´å¡å°”æ ¡æ­£ï¼‰æ¥æ¶ˆé™¤æˆ–çº æ­£åå·®ã€‚

A biased estimator systematically misrepresents the parameter it is estimating, making it less reliable. The goal in most statistical analyses is to use unbiased estimators or account for bias when interpreting results.æœ‰åå·®çš„ä¼°è®¡å™¨ä¼šç³»ç»Ÿæ€§åœ°æ­ªæ›²å…¶ä¼°è®¡çš„å‚æ•°ï¼Œä»è€Œä½¿å…¶å¯é æ€§é™ä½ã€‚å¤§å¤šæ•°ç»Ÿè®¡åˆ†æçš„ç›®æ ‡æ˜¯ä½¿ç”¨æ— åä¼°è®¡é‡æˆ–åœ¨è§£é‡Šç»“æœæ—¶è€ƒè™‘åå·®


# Application

## Linear regression

### **Ordinary Least-Squares (OLS) Regression** æ™®é€šæœ€å°äºŒä¹˜æ³•å›æ­¸

$$
P(y|x) = {\rm N}(x|\mu  = \alpha {\rm{x + }}\beta ,{\sigma ^2})
$$

where $\alpha$ , $\beta $ and $\sigma^{2}$ are parameters to-be-fit

![1735320045922](image/note/1735320045922.png)

an OLS model makes probabilitic predictions: the model syas that $y$ is drawn from a normal distribution whose mean depends on $x$ è©²æ¨¡å‹èªç‚ºyå–è‡ªå‡å€¼å–æ±ºæ–¼xçš„æ­£æ…‹åˆ†ä½ˆ

## Best  estimate æœ€ä½³ä¼°è¨ˆå€¼

$$
{p_A}=k/n
$$

# Packagesï¼š

## input data

### Python

```python
f = pd.read_csv('file_path')
```

### R

```r
f <- read.table("file_path", header=TRUE,sep=",")
```

## Data wrangling:

### Python

Pandas, Numpy

```python
import pandas as pd
import numpy as np
```

```python
x=f.mag.values
```

#### Summary Statistics

```
np.mean(x)
```

##### Variance

```python
#Baised
up.var(x)
#Unbiased
np.var(x,ddof=1)
```

##### Skewness

```python
ss = np.sqrt(np.var(x))
sp.stats.moment(x,3)/(ss**3)
```

##### Kurtosis

```python
(sp.stats.momnet(x,4)/(ss**4))-3
```

#### EDA

```python
import numpy as np
import scipy.stats as st
import matplotlib.pyplot as plt
import pandas as pd

f = open('./auto.txt','r')
mpg, cyl, dis, hp, wgt, acc, yr, org = np.loadtxt(
f, unpack=True, usecols = (0,1,2,3,4,5,6,7))
colNames = [
â€MPGâ€,â€Cylindersâ€,â€Displacementâ€,â€Horsepowerâ€ ,
â€Weightâ€,â€Accelerationâ€,â€ModelYearâ€,â€Originâ€,â€CarNameâ€
]
auto_df = pd.read_table(â€™./auto.txtâ€™,sep=â€™\tâ€™,names=colNames)
auto_df.describe()
```

```python
xbar = np.mean(X,1)
print(xbar)
```

##### Covariance Matrix

```python
S = np.cov(x)
print(S)
```

##### Correlation Matrix

```
R = np.corrcoef(X)
print(R)
```

### R

dplyr

```R
library(dplyr)
```

```r
x <- f$mag
```

#### Summary Statistics

```R
mean(x)
```

##### Variance

```R
#Unbiased
var(x)
#biased
library(momnets)
moment(x, order=2,central=TRUE)
```

##### Skewness

```R
ss = sqrt(momrnt(x, order=2,central=TRUE))
moment(x,order=3,central=TRUE)/(ss^3)
```

#### Kurtosis

```python
(moment(x, order=4,central=TRUE)/(ss^4))-3
```

#### EDA

```r
auto.data <- read.table("auto.txt", header=FALSE, sep="\t")
auto.data
```

```r
library(plyr)
auto . data <âˆ’ rename ( auto . data , c (
â€ V1 â€ = â€MPGâ€ ,
â€ V2 â€ = â€ C y l i n d e r s â€ ,
â€ V3 â€ = â€ D i s p l a c e m e n t â€ ,
â€ V4 â€ = â€ Horsepower â€ ,
â€ V5 â€ = â€ Weight â€ ,
â€ V6 â€ = â€ A c c e l e r a t i o n â€ ,
â€ V7 â€ = â€ ModelYear â€ ,
â€ V8 â€ = â€ O r i g i n â€ ,
â€ V9 â€ = â€ CarName â€
) )
summary(auto.data)
```

```r
colMeans(auto.data[,1:6], na.rm = TRUE)
```

##### Covariance Matrix

```r
cov(auto,data[,c(1,3,5,6)], use = "na.or.complete")
```

##### Correlation Matrix

```r
cor(auto.data[,c(1,3,5,6)], use="na.or.complete")
```

## Visualisation

### Python

Seaborn(based on MatplotLib)

```python
import matplotlib.pyplot as plt
import seaborn as sns
```

```python
sns.histplot(magnitudes, stat='density')
sns.kdeplot(magnitudes)
sns.rugplot(magnitudes)
```

![1735488472345](image/note/1735488472345.png)

#### Measures of central tendency

Plot the KDE curve, then add vertical lines for the mean, median and mode

```python
# Set the dimensions of the plot
plt.figure( figsize=(widthInInches, heightInInches) )

# Plot the kernel density estimate curve
plt.plot(kdeX, kdeY, '-k')

# Add variously dashed vertical lines for the three
# measures fo central tendency
plt.axvline( meanMagnitude, linestyle='--', color='b', label='Mean')
plt.axvline( medianMagnitude, linestyle='-.', color='r', label='Median')
plt.axvline( magnitudeMode,  linestyle=':', color='m', label='KDE-estimated Mode')

# Add labels and set the positions of the tick marks
plt.xlabel('Earthquake Magnitude')
plt.ylabel('Estimated Density')
plt.ylim(kdeYLimits)
plt.xlim([0,5])
plt.xticks((0,1,2,3,4,5))

# Add a legend
plt.legend()
plt.tight_layout()

# Save a copy as a PDF, then show the figure
plt.savefig('Figures/earth_central.pdf')
plt.show()
```

![1735900908099](image/note/1735900908099.png)

#### Empirical Cumulative Density Function (ECDF)

The `statsmodels` package includes routines to compute this easily. The result has two parts, a list of $x$ values and a corresponding list of $y$ values.

```python
# Set the dimensions of the plot
plt.figure( figsize=(widthInInches, heightInInches) )

# Draw the curve
plt.step(ecdf.x, ecdf.y, c='k')

# Add captions, tick marks and the like
plt.xlabel('Earthquake Magnitude')
plt.ylabel('ECDF')
plt.ylim([0,1.02])
plt.xlim([0,5])
plt.xticks((0,1,2,3,4,5))
plt.tight_layout()

# Blap the image out, both to a file and to the screen
plt.savefig('Figures/earth_ecdf.pdf')
plt.show()
```

![1735900971775](image/note/1735900971775.png)

```python
# Set the dimensions of the plot
plt.figure( figsize=(widthInInches, heightInInches) )

# Draw the ECDF as a solid curve
plt.step(ecdf.x, 100*ecdf.y, color='blueviolet' )

# Add dashed lines for the quantiles computed above.
plt.plot([0,p25,p25],[25, 25, 0],'--k')
plt.plot([0,p50,p50],[50, 50, 0],'--k')
plt.plot([0,p75,p75],[75, 75, 0],'--k')
plt.plot([0,p95,p95],[95, 95, 0],'--k')
plt.plot([0,p99,p99],[99, 99, 0],'--k')

# Add axis lables, tick marks and the like
plt.xlabel('Earthquake Magnitude')
plt.ylabel('ECDF x 100%')
plt.ylim([0,102])
plt.xlim([0,5])
plt.xticks((0,p25,p50,p75,p95,p99,5),('0','$P_{25}$','$P_{50}$','$P_{75}$','$P_{95}$','$P_{99}$','5'))
plt.tight_layout()

# Blap the image out, both to a file and to the screen
plt.savefig('Figures/earth_q_ecdf.pdf')
plt.show()
```

![1735900993006](image/note/1735900993006.png)

```python
# Set the dimensions of the plot
plt.figure( figsize=(widthInInches, heightInInches) )

# Add the survival function, which is just 1 - ECDF
plt.step(ecdf.x,1-ecdf.y,c='k')

# Add axis lables, tick marks and the like
plt.xlabel('Earthquake Magnitude')
plt.ylabel('1 - ECDF')
#plt.xscale('log')
plt.yscale('log') # Put y on a log scale
plt.xlim([0,5])
plt.xticks((0,1,2,3,4,5))
plt.ylim((1e-4,1.1))
plt.tight_layout()

# Blap the image out, both to a file and to the screen
plt.savefig('Figures/earth_ll_ecdf.pdf')
plt.show()
```

![1735901018520](image/note/1735901018520.png)

```python
# Set the dimensions of the plot
plt.figure( figsize=(widthInInches, heightInInches) )

# Draw the density estimates
# The middle line draws a thick white line to clear space for the 10-bin histogram's curve
plt.step(xh100,np.concatenate((np.zeros(1),yh100)), color='maroon',label='100-bin Histogram')
plt.step(xh10,np.concatenate((np.zeros(1),yh10)),'w',linewidth=3)
plt.step(xh10,np.concatenate((np.zeros(1),yh10)), color='darkslateblue', linestyle='dashed',label='10-bin Histogram')

# Add axis lables, tick marks and the like
plt.xlabel('Earthquake Magnitude')
plt.ylabel('Estimated Density')
plt.xlim([0,5])
plt.xticks((0,1,2,3,4,5))
plt.legend()
plt.tight_layout()

# Blap the image out, both to a file and to the screen
plt.savefig('Figures/earth_hists.pdf')
plt.show()
```

![1735901040238](image/note/1735901040238.png)

```python
# Set the dimensions of the plot
plt.figure( figsize=(widthInInches, heightInInches) )

# Plot the two estimates
plt.plot(xgau,ygau,label='Gaussian Kernel',color='maroon',linestyle='--')
plt.plot(xuni,yuni,label='Uniform Kernel',color='darkslateblue',linestyle='-')

# Add axis lables, tick marks and the like
plt.xlabel('Earthquake Magnitude')
plt.ylabel('Estimated Density')
plt.ylim(kdeYLimits)
plt.xlim([0,5])
plt.xticks((0,1,2,3,4,5))
plt.legend()
plt.tight_layout()

# Blap the plot out, both to a file and to the screen
plt.savefig('Figures/earth_uni.pdf')
plt.show()
```

![1735901054682](image/note/1735901054682.png)

```python
# Set the dimensions of the plot
plt.figure( figsize=(widthInInches, heightInInches) )

# Plot the two estimates
plt.plot(xgau,ygau,label='Gaussian Kernel',color='maroon',linestyle='--')
plt.plot(xtri,ytri,label='Triangular Kernel',color='darkseagreen',linestyle='-')

# Add axis lables, tick marks and the like
plt.xlabel('Earthquake Magnitude')
plt.ylabel('Estimated Density')
plt.ylim(kdeYLimits)
plt.xlim([0,5])
plt.xticks((0,1,2,3,4,5))
plt.legend()
plt.tight_layout()

# Blap the plot out, both to a file and to the screen
plt.savefig('Figures/earth_tri.pdf')
plt.show()
```

![1735901065377](image/note/1735901065377.png)

#### KDE

```python
import math # for floor()
from scipy.stats import norm # to compute the true density
from statsmodels.nonparametric.kde import KDEUnivariate

# Choose a range of sample sizes to use when building the KDE
sampleSize = [8, 32, 128, 4096]

# Prepare to draw the true distribution
nTicks = 100
zVals = np.linspace( -2.5, 2.5, nTicks+1 )
truePdfVals = np.zeros( len(zVals) )
for j in range(0, len(zVals)):
    truePdfVals[j] = norm.pdf( zVals[j] )

# Set up the plot
kdeFig, kdeAxes = plt.subplots(2, 2, figsize=[10,10])

# Construct the estimates in turn
nSizes = len(sampleSize)
pdfVals = np.zeros( [nTicks, nSizes] )
for j in range(0, nSizes):
    # Generate the sample
    crntSize = sampleSize[j]
    crntSample = np.random.normal( 0, 1, crntSize )
  
    # Fit the KDE
    crnt_kde = KDEUnivariate( crntSample )
    crnt_kde.fit()
  
    # Plot that puppy, along with the data and the true distribution
    row = math.floor( j / 2 )
    col = j % 2 
    kdeAxes[row, col].hist( crntSample, bins="fd", density=True, alpha=0.5 )
    kdeAxes[row, col].plot(crntSample, np.zeros(crntSize), 'b+', ms=20) # rug
    kdeAxes[row, col].plot( zVals, truePdfVals, ':'  )
    kdeAxes[row, col].plot( crnt_kde.support, crnt_kde.density )
    kdeAxes[row, col].set_xlabel('z')
    kdeAxes[row, col].set_ylabel('Density' )
    kdeAxes[row, col].set_title( 'Sample size: ' + str(crntSize) )

plt.tight_layout() # Avoid overlapping labels

# If desired, make a pdf version too
if( makePDF ):
    plt.savefig('Figures/KDE_convergenceExample.pdf')
```

![1735901265107](image/note/1735901265107.png)

```python
# Set up the plot
kdeIdeaFig, kdeAxes = plt.subplots(1, 2, figsize=[10, 5])

# Construct the estimates in turn
sampleSize = 4
crntSample = np.random.normal( 0, 1, sampleSize )
  
# Fit the KDE and make a note of the bandwidth
crnt_kde = KDEUnivariate( crntSample )
crnt_kde.fit()
crntBw = crnt_kde.bw
  
# Plot that puppy, along with the data and the true distribution
kdeAxes[0].hist( crntSample, bins="fd", density=True, alpha=0.5 )
kdeAxes[0].plot( crntSample, np.zeros(sampleSize), 'b+', ms=20) # rug
kdeAxes[0].plot( zVals, truePdfVals, ':'  )
kdeAxes[0].plot( crnt_kde.support, crnt_kde.density )
kdeAxes[0].set_xlabel('z')
kdeAxes[0].set_ylabel('Density' )
kdeAxes[0].set_title( str(sampleSize) + ' samples, KDE and true density'  )

# Also plot the individual bumps
kdeAxes[1].plot( crntSample, np.zeros(sampleSize), 'b+', ms=20) # rug
kdeAxes[1].plot( crnt_kde.support, crnt_kde.density, 'C2' )
kdeAxes[1].set_xlabel('z')
kdeAxes[1].set_ylabel('Density' )
kdeAxes[1].set_title( 'Contributions from individual points'  )
for j in range(0, sampleSize ):
    # Evaluate the contribution due to the kernel on the j-th sample
    bumpVals = np.zeros( len(zVals) ) 
    for k in range(0, len(zVals)):
        bumpVals[k] = norm.pdf( zVals[k], crntSample[j], crntBw ) / sampleSize
  
    # Add it to the plot
    kdeAxes[1].plot( zVals, bumpVals, 'C4' )

plt.tight_layout() # Avoid overlapping labels

# If desired, make a pdf version too
if( makePDF ):
    plt.savefig('Figures/KDE_ideaExample.pdf')
```

![1735901280674](image/note/1735901280674.png)

```python
# Set up the plot
kdeIdeaFig, kdeAxes = plt.subplots(1, 2, figsize=[16, 6])

# Arrange the data in an order that may help highlght
# the way the contributions add up. This is a somewhat 
# arbitrary choice.
sampleSize = 6 # should be even
halfSampleSize = int(sampleSize/2)
sortedSample = sorted( np.random.normal( 0, 1, sampleSize ) )
crntSample = np.zeros(sampleSize)
for j in range(halfSampleSize):
    crntSample[2*j] = sortedSample[j]
    crntSample[2*j+1] = sortedSample[j+halfSampleSize]
  
# Fit the KDE and make a note of the bandwidth
crnt_kde = KDEUnivariate( crntSample )
crnt_kde.fit()
crntBw = crnt_kde.bw

# Get ready to build an array whose columns give bump shapes
nTicks = 200
zVals = np.linspace( -4, 4, nTicks+1 )
bumpShapes = np.zeros( (sampleSize, len(zVals)) )

# Plot the full desnity estimate, along with the bumps for the
# individual data points.
kdeAxes[0].plot( crnt_kde.support, crnt_kde.density, 'black' )
kdeAxes[0].set_xlabel('z')
kdeAxes[0].set_ylabel('Density' )
kdeAxes[0].set_title( 'Contributions from individual points'  )
for j in range(0, sampleSize ):
    # Evaluate the contribution due to the kernel sitting on the j-th sample
    for k in range(0, len(zVals)):
        bumpShapes[j,k] = norm.pdf( zVals[k], crntSample[j], crntBw ) / sampleSize
  
    # Add it to the plot
    colorStr = 'C' + str(j)
    kdeAxes[0].plot( zVals, bumpShapes[j,:], colorStr )
  
# Plot the stacked pillows
kdeAxes[1].set_xlabel('z')
kdeAxes[1].set_ylabel('Density' )
kdeAxes[1].set_title( 'Pillow Plot'  )
kdeAxes[1].stackplot( zVals, bumpShapes )

# If desired, make a pdf version too
if( makePDF ):
    plt.savefig('Figures/KDE_PillowPlot.pdf')
```

![1735901294341](image/note/1735901294341.png)


```python
sns.kdeplot(dis,wgt, cmap="Blues")
```

![1735950197677](image/note/1735950197677.png)







### R

GGPlot

```
library(ggplot2)
```

```R
ggplot(earthquake.df, aes(x=mag))+
geom_histogram(aes(y=..density..))+
geom_density()+ xlab("Magnitude")
```

![1735488562842](image/note/1735488562842.png)



#### KDE

```r
pplot(auto.data, aes(x=Displacement, y=Weight)) + geom_density_2d()
```

![1735950156479](image/note/1735950156479.png)





## EDA

### Density estimates

Here we use `kdeplot` and `histplot` to draw a histogram and a KDE (based on a Gaussian kernel) on the same axes.

```python
# Set the figure's size
plt.figure(figsize=(10,4))

# Draw a histogram and a KDE with Seaborn 
ax = sns.kdeplot( x, color="red", label="Kernel Density")
sns.histplot( x, stat="density", color = "darksalmon", label="Histogram"  )

# Finally, add a rug plot
sns.rugplot( np.array([1]), label="Rug Plot" )

# Add axis labels 
plt.xlabel('Grade of Spondylolisthesis')
plt.ylabel('Estimated Density')

# Set the limits on the axes
plt.xlim([-50,450])
plt.tight_layout()

# Save to a file and display
plt.savefig('Figures/spine_distplot.pdf')
plt.show()
```

![1735901540688](image/note/1735901540688.png)

### Log-transformed data

Transforming the data by taking its log expands the scale for smaller values, which can sometmes be illuminating.

```python
# Set the figure's size
plt.figure(figsize=(10,4))

# Plot the histogram and KDE
y = np.log(x-np.min(x)+1.0) # Log-transform the data to expand the scale for lower values

# Draw a histogram and a KDE
ax = sns.kdeplot( y, color="darkgreen", label="Kernel Density")
sns.histplot( y, stat="density", color = "darkseagreen", label="Histogram"  )

# Add a rug plot
sns.rugplot( y, label="Rug Plot")

# Add axis labels 
plt.xlabel('Log-transformed Grade of Spondylolisthesis')
plt.ylabel('Estimated Density')

# Set the limits on the axes
plt.xlim([-1,7])
plt.tight_layout()

# Save to a file and display
plt.savefig('Figures/spine_log.pdf')
plt.show()
```

![1735901670524](image/note/1735901670524.png)

### Changing the kernel function

Here we compute density estimates using three standard kernels: the uniform, triangular and Gaussian kernels.

```python
# The uniform kernel
mykde = sm.nonparametric.KDEUnivariate(x)
mykde.fit(kernel="uni", fft=False)
xuni = mykde.support
yuni = mykde.density

# The triangular kernel
mykde2 = sm.nonparametric.KDEUnivariate(x)
mykde2.fit(kernel="tri", fft=False)
xtri = mykde2.support
ytri = mykde2.density

# The Gaussian kernel
mykde3 = sm.nonparametric.KDEUnivariate(x)
mykde3.fit(kernel="gau")
xgau = mykde3.support
ygau = mykde3.density
```

#### Comparing KDEs based on the Gaussian and uniform kernels

```python
# Set the figure's size
plt.figure(figsize=(10,4))

# Plot the two KDEs
plt.plot(xgau,ygau,label='Gaussian Kernel',color=[0.5,0.5,0.5],linestyle='--')
plt.plot(xuni,yuni,label='Uniform Kernel',color=[0,0,0.7],linestyle='-')

# Add labels and a legend
plt.xlabel('Grade of Spondylolisthesis')
plt.ylabel('Estimated Density')
plt.xlim([-50,450])
plt.legend()
plt.tight_layout()

# Save to a file and display
plt.savefig('Figures/spine_uni.pdf')
plt.show()
```

![1735901731435](image/note/1735901731435.png)

#### Comparing KDEs based on the Gaussian and triangular kernels

```python
# Set the figure's size
plt.figure(figsize=(10,4))

# Plot the two KDEs
plt.plot(xgau,ygau,label='Gaussian Kernel',color=[0.5,0.5,0.5],linestyle='--')
plt.plot(xtri,ytri,label='Triangular Kernel',color=[0.7, 0, 0],linestyle='-')

# Add labels and a legend
plt.xlabel('Earthquake Severity')
plt.ylabel('Estimated Density')
plt.xlabel('Grade of Spondylolisthesis')
plt.ylabel('Estimated Density')
plt.xlim([-50,450])
plt.legend()
plt.tight_layout()

# Save to a file and display
plt.savefig('Figures/spine_tri.pdf')
plt.show()
```

![1735901778129](image/note/1735901778129.png)


### Scatter Plots 

A scatter plot is a lossless visualisation that involves placing a marker at $(x_{ia}, x_{ib})$ for each $i$ and some $a, b$



```R
ggplot(auto.data, aes(x=Displacement,y=Weight)) + geom_point()
```

![1735948863892](image/note/1735948863892.png)

```python
plt.scatter(dis,wgt)
```

![1735948927349](image/note/1735948927349.png)


#### 2d Histograms

```r
ggplot(auto.data, aes(x=Displacement,y=Weight)) + geom_bin2d()
```

![1735949029386](image/note/1735949029386.png)

```python
plt.hist2d(dis,wgt, cmap='Blues')
```

![1735949090235](image/note/1735949090235.png)
